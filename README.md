# üìö Clustering Papers Collection

Total Papers: 150

This repository collects clustering papers from ICLR, ICML, and NIPS (2021-2024), sorted by relevance.

| Title                                                                                                                                       | Key Points                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | PDF Link                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| Sparse Quantized Spectral Clustering                                                                                                        | - Introduces selective sparsification and quantization methods for spectral clustering.<br>- Uses random matrix theory to predict phase transitions in clustering performance.<br>- Highlights existence of spurious eigenvectors caused by nonlinear transformations.<br>- Provides optimal thresholds for sparsification and quantization to maximize performance while reducing complexity.<br>- Validates findings through experiments on real-world datasets and theoretical analysis.                                                                                                                                                                                                                                                                                                                                    | [PDF](papers/ICLR/2021_clustering/very/525_sparse_quantized_spectral_clus.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Fast Topological Clustering with Wasserstein Distance                                                                                       | - Uses persistent homology to extract 0D and 1D topological features (connected components, cycles) from graphs<br>- Introduces a closed-form, fast computation of Wasserstein distance for 1D persistence barcodes<br>- Defines a topological centroid (average birth and death points) for cluster center updates<br>- Combines geometric and topological distances into a flexible clustering criterion using Œª weighting<br>- Validated on simulated modular networks and real brain connectivity data under anesthesia<br>- Robust against noise and missing direct correspondence between individual edge weights<br>- Clustering framework generalizes to noisy, continuous-valued, large datasets like SNP association statistics                                                                                      | [PDF](papers/ICLR/2022_clustering/very/1709_fast_topological_clustering_wi.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| A Deep Variational Approach to Clustering Survival Data                                                                                     | - Proposes VaDeSC: a semi-supervised probabilistic clustering model<br>- Uses a Gaussian Mixture Variational Autoencoder to jointly model features and outcomes<br>- Survival outcomes are modeled with cluster-specific Weibull distributions<br>- Handles censored data and missing outcomes robustly<br>- Optimizes an ELBO using stochastic gradient variational inference (SGVI)<br>- Demonstrates superior clustering performance on synthetic, semi-synthetic, and real-world datasets<br>- Capable of discovering clusters reflecting true underlying feature-outcome mechanisms<br>- Framework highly adaptable to high-dimensional SNP beta/Z-score data                                                                                                                                                             | [PDF](papers/ICLR/2022_clustering/very/2773_a_deep_variational_approach_to.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Graphon Based Clustering and Testing of Networks: Algorithms and Theory                                                                     | - Introduces graphon-based distance estimation using sorting-and-smoothing<br>- Proposes Distance-based Spectral Clustering (DSC) with consistency guarantees<br>- Develops Similarity-based Semi-Definite Programming (SSDP) with zero error asymptotic guarantees<br>- Applies graph distance to two-sample testing between networks<br>- Outperforms graph kernel and graph matching methods on simulated and real data<br>- Highly scalable to large graphs and effective with small samples<br>- Potential for adaptation to SNP data clustering by treating SNP-trait associations as graphon analogs                                                                                                                                                                                                                    | [PDF](papers/ICLR/2022_clustering/very/3907_graphon_based_clustering_and_t.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Contrastive Clustering to Mine Pseudo-Parallel Data for Unsupervised Translation                                                            | - Introduces LAgSwAV, a language-agnostic variant of SwAV clustering<br>- Uses soft cluster assignments to represent samples, enabling uncertainty modeling<br>- Applies a rigorous multi-step filter suite to detect and remove noise<br>- Develops rank-based cross-entropy loss to prioritize high-confidence pairs<br>- Achieves state-of-the-art results on standard and low-resource translation tasks<br>- Visual and quantitative evaluation of embedding quality and clustering robustness<br>- Soft clustering and rebalancing could inspire methods for SNP data with high noise                                                                                                                                                                                                                                    | [PDF](papers/ICLR/2022_clustering/very/626_contrastive_clustering_to_mine.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| ADA-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space                                                           | - Introduces adaptive neighbor discovery for graph construction<br>- Uses structure space transformation combining cosine and Jaccard similarities<br>- Focuses heavily on detecting and removing noise edges<br>- State-of-the-art clustering results on MS-Celeb-1M and other datasets<br>- Learned heuristic (FŒ≤-score) to balance precision and recall in neighborhood discovery<br>- GCN-based feature refinement with noise-resilient graph input<br>- Robust against changes in neighborhood size parameter k<br>- Methods could be transferred to SNP-level clustering with continuous effect sizes                                                                                                                                                                                                                    | [PDF](papers/ICLR/2022_clustering/very/773_ada_nets_face_clustering_via_a.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Near-Optimal Coresets for Robust Clustering                                                                                                 | - Introduces near-optimal Œµ-coreset construction for robust k-median and k-means clustering with m outliers.<br>- Coreset size is O(m + poly(k, Œµ‚Åª¬π)) with near-linear construction time.<br>- Utilizes a geometric decomposition into rings and groups to sample efficiently.<br>- Empirical experiments show consistent performance gains over uniform and sensitivity sampling.<br>- Enables approximately 100√ó speedup for approximate clustering algorithms while maintaining clustering quality.<br>- Demonstrates scalability and robustness even with large numbers of outliers.<br>- Highlights practical and theoretical advances that are adaptable to high-dimensional biological datasets like SNP beta/Z-score matrices.                                                                                         | [PDF](papers/ICLR/2023_clustering/very/3911_near_optimal_coresets_for_robu.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Most Discriminative Stimuli for Functional Cell Type Clustering                                                                             | - EM-style clustering algorithm alternating between optimization and reassignment.<br>- Softmax-based objective function to differentiate clusters.<br>- Application across species (mouse, marmoset, macaque) and recording techniques.<br>- Improvement over conventional methods with a 20% reduction in experimental time.<br>- Potential applicability to clustering SNPs using beta/Z-scores as signals.                                                                                                                                                                                                                                                                                                                                                                                                                 | [PDF](papers/ICLR/2024_clustering/very/1897_Most_discriminative_stimu.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| Local Graph Clustering with Noisy Labels                                                                                                    | - Introduction of Label-based Flow Diffusion (LFD) method<br>- Use of weighted graphs with label-based edge weights to handle noise<br>- Theoretical analysis providing sufficient conditions for effective clustering<br>- Empirical evaluation showing F1 score improvements up to 13%<br>- Adaptable framework applicable to SNP clustering tasks involving association measures                                                                                                                                                                                                                                                                                                                                                                                                                                            | [PDF](papers/ICLR/2024_clustering/very/2084_Local_Graph_Clustering_wi.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| Progressive Partial Optimal Transport for Deep Imbalanced Clustering                                                                        | - P2OT framework addresses deep clustering on imbalanced datasets using pseudo-labeling.<br>- Formulates pseudo-label generation as a partial optimal transport problem.<br>- Incorporates a virtual cluster to absorb low-confidence samples, enhancing robustness.<br>- Uses a KL divergence constraint to avoid degenerate solutions.<br>- Gradual learning process governed by a progressively increasing parameter œÅ.<br>- Achieves state-of-the-art results on various imbalanced datasets.<br>- Computational efficiency improved by matrix scaling techniques.                                                                                                                                                                                                                                                         | [PDF](papers/ICLR/2024_clustering/very/3053_P_2_OT_Progressive_Partia.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| Effective Pruning of Web-Scale Datasets Based on Complexity of Concept Clusters                                                             | - Uses k-means clustering to group data embeddings.<br>- Introduces Density-Based Pruning (DBP) which retains samples from complex clusters.<br>- Calculates complexity based on inter-cluster and intra-cluster distances.<br>- Uses a quadratic programming solver to manage dataset constraints.<br>- Achieves improved performance with reduced training costs on large-scale datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                    | [PDF](papers/ICLR/2024_clustering/very/5051_Effective_pruning_of_web_.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| A Differentially Private Clustering Algorithm for Well-Clustered Graphs                                                                     | - Addresses cluster recovery in well-clustered graphs under (Œµ, Œ¥)-differential privacy<br>- Employs a specialized SDP with strong-convexity to bound sensitivity for adding minimal Gaussian noise<br>- Achieves near-optimal misclassification ratio compared to non-private cluster recovery<br>- Proves pure (Œµ)-DP is unattainable with small error, justifying (Œµ, Œ¥)-DP relaxation<br>- Experimental results on stochastic block model show better performance than naive randomization approaches                                                                                                                                                                                                                                                                                                                      | [PDF](papers/ICLR/2024_clustering/very/7708_A_Differentially_Private_.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| Explaining Kernel Clustering via Decision Trees                                                                                             | - Generalizes iterative mistake minimization (IMM) to kernel k-means<br>- Introduces surrogate features to preserve axis-aligned interpretability<br>- Offers O(k^2) or dimension-sensitive bounds for the kernel "price of explainability"<br>- Greedy expansions (Kernel ExKMC/Expand) further refine the partition at the cost of deeper trees<br>- Empirically validated on various datasets (e.g. half-moon patterns, Iris), demonstrating near-optimal kernel cluster accuracy                                                                                                                                                                                                                                                                                                                                           | [PDF](papers/ICLR/2024_clustering/very/7781_Explaining_Kernel_Cluster.pdf)                                                                                                                                                                                                                                                                                                                                                                                |
| BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders | - Introduces DeVAE and BasisDeVAE, VAEs with derivative-based decoders.<br>- Performs simultaneous dimensionality reduction and feature-level clustering.<br>- Allows enforcement of monotonicity and transient behaviors in a flexible way.<br>- Demonstrates superior performance in clustering noisy and real-world biological data.<br>- Supports interpretable modeling, critical for pathway analysis in causal inference.<br>- Scalable to datasets with thousands of features (e.g., SNPs or genes).                                                                                                                                                                                                                                                                                                                   | [PDF](papers/ICML/2021_clustering/very/danks21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time                                                                           | - First nearly-linear time exact HAC algorithms for complete-linkage, WPGMA, and average-linkage clustering on graphs.<br>- Dynamic neighbor-heap data structures and graph orientation techniques to maintain clustering efficiency.<br>- Empirical evaluation shows 20x‚Äì70x speedups compared to traditional HAC implementations.<br>- Approximate clustering variants allow tradeoffs between speed and strict merge accuracy.<br>- Demonstrates scalability to millions of points and billions of edges using commodity hardware.<br>- Highly modular framework, allowing adaptation to different graph types and linkage methods.                                                                                                                                                                                         | [PDF](papers/ICML/2021_clustering/very/dhulipala21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures                                                      | - Defines important properties for clustering similarity indices: constant baseline, monotonicity, symmetry, etc.<br>- Demonstrates that popular indices like NMI are often biased and inconsistent.<br>- Introduces the Correlation Distance (based on arccosine of Pearson correlation) as a novel, theoretically sound distance measure.<br>- Provides a methodology to select appropriate validation indices based on application needs.<br>- Highlights the risks of poor validation in production systems with a real-world case study (news aggregator).<br>- Suggests that choosing a good similarity measure is crucial for reliable clustering, especially when no 'true' cluster labels exist.<br>- Gives recommendations on when to prefer metrics like Adjusted Rand, Correlation Coefficient, or Sokal & Sneath. | [PDF](papers/ICML/2021_clustering/very/gosgens21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| A Scalable Deterministic Global Optimization Algorithm for Clustering Problems                                                              | - Clustering formulated as a Mixed Integer Second Order Cone Programming (MISOCP) problem.<br>- Reduced-space branch-and-bound algorithm that only branches on cluster centers, independent of data sample size.<br>- Efficient lower and upper bounding methods using closed-form solutions, scenario-based sample grouping, and Lagrangian decomposition.<br>- Proves global convergence to Œµ-optimal solutions with formal convergence guarantees.<br>- Massively scalable: up to 200,000 samples solvable with parallelism.<br>- Strong practical performance on both synthetic and real-world datasets, compared against k-means and CPLEX.<br>- Open-source implementation available in Julia for reproducibility and future extension.                                                                                  | [PDF](papers/ICML/2021_clustering/very/hua21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                        |
| Local Correlation Clustering with Asymmetric Classification Errors                                                                          | - Models clustering under noisy binary similarity/dissimilarity labels with variable edge weights.<br>- Focuses on minimizing ‚Ñì‚Çö norms (especially ‚Ñì‚ÇÇ and ‚Ñì‚àû) of local disagreement vectors rather than global sums.<br>- Designs a new metric-space partitioning algorithm supporting strong worst-case guarantees.<br>- Provides polynomial-time algorithms with approximation bounds dependent only on the noise parameter Œ±.<br>- Convex relaxation formulation that leads to practical and scalable rounding algorithms.<br>- Handles asymmetric errors naturally, matching real-world imbalance between true associations and false positives/negatives.<br>- Relevant for clustering SNPs based on beta/Z-scores while accounting for different confidence levels across SNPs.                                          | [PDF](papers/ICML/2021_clustering/very/jafarov21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Feature Clustering for Support Identification in Extreme Regions                                                                            | - MEXICO: A sparse optimization-based clustering method focused on extreme data points.<br>- Uses empirical risk minimization with a loss function designed for extreme regions.<br>- Optimization over probability simplex to enforce sparsity and mixture separation.<br>- Projected gradient ascent with regularization to produce disjoint clusters.<br>- Focuses on subspaces where extreme events are concentrated, aligning well with SNPs with large beta/Z-scores.<br>- Provides theoretical convergence guarantees (non-asymptotic excess risk bounds).<br>- Outperforms spectral clustering and spherical k-means in extreme feature clustering tasks.<br>- Demonstrates strong anomaly detection capabilities using the learned extreme structure.                                                                 | [PDF](papers/ICML/2021_clustering/very/jalalzai21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| ACE: Explaining Cluster from an Adversarial Perspective                                                                                     | - Proposes ACE: an integrated deep learning framework for embedding, clustering, and explaining data.<br>- Neuralizes clustering to make it differentiable, enabling attribution of input features to cluster membership.<br>- Uses adversarial perturbations to find minimal sets of important features (genes) for each cluster.<br>- Handles batch effects and technical noise through autoencoder-based embedding (e.g., using SAUCIE).<br>- Outperforms traditional methods (e.g., DESeq2, SHAP, SmoothGrad) in finding compact, non-redundant explanatory panels.<br>- Successfully applied to both simulated and real biological datasets, and generalized to MNIST image data.<br>- Strong candidate method for building interpretable, noise-resistant SNP clustering algorithms.                                     | [PDF](papers/ICML/2021_clustering/very/lu21e.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Local Algorithms for Finding Densely Connected Clusters                                                                                     | - Introduces LocBipartDC (for undirected graphs) and EvoCutDirected (for directed graphs).<br>- Uses a double cover construction to model two-cluster problems as single-cluster problems.<br>- Develops a simplify operator to ensure clean cluster separation.<br>- Uses Personalized PageRank and evolving set processes for local computation.<br>- Provides theoretical bounds on cluster quality (bipartiteness, flow ratio).<br>- Demonstrates success on synthetic stochastic block models and real-world datasets (migration, conflict networks).<br>- Highlights methods for handling noise and large graph sizes efficiently.<br>- Public code available for reproducibility.                                                                                                                                       | [PDF](papers/ICML/2021_clustering/very/macgregor21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Hierarchical Clustering of Data Streams: Scalable Algorithms and Approximation Guarantees                                                   | - Introduces Random Cut Tree (RCT) and Uniform Radial Random Hyperplane (URRH) algorithms for dynamic hierarchical clustering.<br>- Focuses on optimizing HC quality metrics like MW Revenue, CKMM Revenue, Dasgupta Cost, and MW Cost.<br>- Sequential property ensures clustering results are order-invariant ‚Äî critical for dynamic datasets.<br>- Approximation guarantees: 0.9-approximation for CKMM Revenue, 2-approximation for MW Cost, etc.<br>- Experimental validation on real datasets (MNIST, ImageNet, etc.) and synthetic noisy datasets.<br>- RCT/URRH outperform traditional methods in noisy environments.<br>- Provides a general hyperplane-based HC framework adaptable to various metrics.<br>- Potential adaptation for SNP beta/Z-score space clustering.                                             | [PDF](papers/ICML/2021_clustering/very/rajagopalan21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                |
| Individual Preference Stability for Clustering                                                                                              | - Introduces Individual Preference (IP) Stability as a new clustering goal.<br>- Proves NP-hardness for general IP-stable clustering problems.<br>- Efficient algorithms for exact IP-stable clustering on the real line and tree metrics.<br>- Approximation algorithms for finding near IP-stable clusterings in general metric spaces.<br>- Uses tree embeddings and modified single-linkage approaches.<br>- Provides extensive experiments comparing IP-stability of standard clustering algorithms.<br>- Suggests modifications to standard clustering methods to improve stability.<br>- Concepts can be adapted to measure and enforce cluster meaningfulness for SNP data.                                                                                                                                            | [PDF](papers/ICML/2022_clustering/very/ahmadi22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Interactive Correlation Clustering with Existential Cluster Constraints                                                                     | - Introduces existential cluster constraints (ECCs) as a novel, efficient user feedback mechanism for clustering.<br>- Extends the correlation clustering framework to incorporate ECCs.<br>- Develops a scalable inference algorithm combining semidefinite programming (SDP) relaxation and sparse cluster trellis dynamic programming.<br>- Proves NP-hardness of satisfying all ECCs exactly, motivating approximate solutions.<br>- Empirical results show ECCs require roughly half as much feedback to achieve good clustering compared to must-link/cannot-link feedback.<br>- Trellis structure allows efficient optimization over exponentially many possible clusterings.<br>- Ideas can be adapted to clustering SNPs using biological or statistical features to guide noise-resilient clustering.                | [PDF](papers/ICML/2022_clustering/very/angell22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Gradient Based Clustering                                                                                                                   | - Introduces a general clustering framework using gradient steps for center updates.<br>- Supports a wide range of cost functions including robust ones like the Huber loss.<br>- Proves strong convergence guarantees under arbitrary initialization.<br>- Adapts to both traditional and non-standard clustering loss functions (Bregman and non-Bregman).<br>- Demonstrates robustness to noisy data using MNIST and Iris datasets.<br>- Provides a computationally cheaper alternative to exact optimization-based clustering.<br>- Applicable for designing SNP clustering methods that need robustness to noisy association measures.                                                                                                                                                                                    | [PDF](papers/ICML/2022_clustering/very/armacki22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| An iterative clustering algorithm for the Contextual Stochastic Block Model with optimality guarantees                                      | - Proposes an iterative refinement clustering algorithm (IR-LS) for CSBM models.<br>- Incorporates graph structure and covariates naturally without arbitrary weighting.<br>- Achieves exact recovery and optimal convergence under certain noise and separation conditions.<br>- Demonstrates robustness to weak separation in graph or covariate data alone.<br>- Extends theoretical guarantees beyond two-community settings.<br>- Supports clustering signed graphs and heterogeneous information.<br>- Provides a scalable and faster alternative to spectral or SDP-based methods.<br>- The approach is adaptable to clustering SNPs based on association statistics and metadata.                                                                                                                                      | [PDF](papers/ICML/2022_clustering/very/braun22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Understanding Doubly Stochastic Clustering                                                                                                  | - Focuses on projecting affinity matrices onto the set of doubly stochastic matrices under the ‚Ñì2 norm.<br>- Provides necessary and sufficient conditions for eliminating false inter-cluster connections.<br>- Shows how to guarantee both no false connections and full within-cluster connectivity.<br>- Applies analysis to subspace clustering, modeling scenarios similar to grouped SNP effects.<br>- Introduces methods to tune sparsity/noise trade-offs via a scaling parameter (Œ∑).<br>- Demonstrates state-of-the-art empirical performance on noisy clustering tasks.<br>- Offers mathematical tools to adapt normalization strategies for SNP data denoising.                                                                                                                                                    | [PDF](papers/ICML/2022_clustering/very/ding22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| The Complexity of k-Means Clustering when Little is Known                                                                                   | - Study of k-means clustering on incomplete data matrices.<br>- Introduction of incidence graph and primal graph representations for missing data.<br>- Use of treewidth and local feedback edge number as key parameters.<br>- Fixed-parameter tractable (FPT) algorithms for bounded-domain and real-valued data.<br>- Dynamic programming over tree decompositions to merge partial clustering solutions.<br>- Potential inspiration for handling sparse or noisy SNP effect matrices.                                                                                                                                                                                                                                                                                                                                      | [PDF](papers/ICML/2022_clustering/very/ganian22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Simultaneous Graph Signal Clustering and Graph Learning                                                                                     | - Extends spectral clustering with graph signal smoothness regularization.<br>- Simultaneous clustering and learning of cluster-specific graphs (GRASCale).<br>- Efficient optimization using block coordinate descent and prox-linear updates.<br>- Consensus clustering for better initialization and avoiding poor local minima.<br>- Validated on synthetic data and MNIST digit clustering, achieving superior performance.<br>- Applicable for heterogeneous data where different clusters may have different underlying structures.<br>- Open-source code available: https://github.com/SPLab-aviyente/GRASCale                                                                                                                                                                                                         | [PDF](papers/ICML/2022_clustering/very/karaaslanli22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                |
| A Tighter Analysis of Spectral Clustering, and Beyond                                                                                       | - Tighter theoretical guarantees for spectral clustering, requiring weaker assumptions.<br>- Proposal to use fewer than k eigenvectors for clustering, leading to better results under certain conditions.<br>- Introduction of meta-graphs to represent interactions between clusters.<br>- Empirical validation on synthetic and real-world datasets (BSDS, MNIST, USPS).<br>- Could inform dimension reduction, noise handling, and clustering strategies for SNP data.                                                                                                                                                                                                                                                                                                                                                     | [PDF](papers/ICML/2022_clustering/very/macgregor22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Sublinear-Time Clustering Oracle for Signed Graphs                                                                                          | - Introduction of a local clustering oracle for signed graphs with sublinear preprocessing and query times.<br>- Development of a new pseudometric for comparing node embeddings from signed random walks.<br>- Use of lazy signed random walks to explore graph neighborhoods efficiently.<br>- Novel theoretical analysis connecting random walks to spectral properties of signed Laplacians.<br>- Evaluation on synthetic data and real-world Wikipedia-based datasets.<br>- Provision of new datasets and open-source code for reproducibility.<br>- Potentially inspiring for SNP clustering by modeling association signs and detecting noise clusters.                                                                                                                                                                 | [PDF](papers/ICML/2022_clustering/very/neumann22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Practical Nearly-Linear-Time Approximation Algorithms for Hybrid and Overlapping Graph Clustering                                           | - Introduces Œµ-Overlapping Ratio-Cut (Œµ-ORC) and Œª-Hybrid Ratio-Cut (Œª-HCUT) objectives for overlapping clustering.<br>- Designs HybridImprove and OverlapImprove algorithms based on s-t max-flow and cut-matching games.<br>- Provides nearly-linear-time O(log n)-approximation guarantees for overlapping clustering.<br>- Successfully applies the methods to synthetic (Overlapping Stochastic Block Models) and real-world social networks.<br>- Shows robustness to noise and the ability to separate overlapping communities, an important feature for SNP clustering.                                                                                                                                                                                                                                                | [PDF](papers/ICML/2022_clustering/very/orecchia22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| ClusterFuG: Clustering Fully Connected Graphs by Multicut                                                                                   | - Dense multicut formulation using inner products of feature vectors<br>- Greedy Additive Edge Contraction (GAEC) adapted for dense graphs<br>- Efficient scaling using nearest-neighbor sparsification and incremental updates<br>- Affinity strength tuning (Œ±) to favor small or large clusters, useful for noise handling<br>- No need to predefine the number of clusters<br>- Open-source code available at https://github.com/aabbas90/cluster-fug<br>- Empirical validation on ImageNet clustering and Cityscapes panoptic segmentation<br>- Potential for extension to incorporate biological priors into clustering                                                                                                                                                                                                  | [PDF](papers/ICML/2023_clustering/very/abbas23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| K-SHAP: Policy Clustering Algorithm for Anonymous Multi-Agent State-Action Pairs                                                            | - Uses a 'world policy' model to learn from anonymous observations (state-action pairs).<br>- Applies SHAP (Shapley Additive Explanations) to generate local explanations for each data point.<br>- Clusters the SHAP value space, not the raw feature space, leading to better cluster separation.<br>- Shows strong performance improvements over classical clustering methods like k-means and ClusterGAN.<br>- Demonstrates robustness in real-world financial data and synthetic multi-agent simulations.<br>- Relevant for clustering settings where data points are anonymized, noisy, or heterogeneous.<br>- Introduces a flexible three-phase framework (Imitation Learning ‚Üí SHAP Explanation ‚Üí Clustering).<br>- Provides a pathway to adapt feature attribution-based clustering for genomic or SNP data.          | [PDF](papers/ICML/2023_clustering/very/coletta23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Fast Combinatorial Algorithms for Min Max Correlation Clustering                                                                            | - Introduces the 'correlation metric' based on local edge agreement in signed graphs.<br>- Focuses on Min-Max objective: minimizing the maximum disagreement across vertices.<br>- Provides exact and approximate (sampling-based) fast clustering algorithms.<br>- Scales to large graphs (tested on Facebook networks with up to 10,000 nodes).<br>- Clusters produced often recover 'ground truth' communities.<br>- Outperforms LP-based methods in speed without large losses in clustering quality.<br>- Potentially adaptable to SNP clustering based on beta/Z-score concordance.                                                                                                                                                                                                                                      | [PDF](papers/ICML/2023_clustering/very/davies23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Topological Point Cloud Clustering                                                                                                          | - Introduces Topological Point Cloud Clustering (TPCC) synthesizing spectral clustering and TDA.<br>- Clusters based on contributions to global topological features (not just local distances).<br>- Uses Hodge-Laplacians and 0-eigenvectors to capture multi-scale structure.<br>- Robust to noise due to topological invariance.<br>- Shows strong performance on synthetic and real-world noisy datasets.<br>- Highlights theoretical guarantees for correct clustering in synthetic topological structures.<br>- Potential inspiration for noise-resistant feature extraction in non-spatial data like SNP association measures.                                                                                                                                                                                         | [PDF](papers/ICML/2023_clustering/very/grande23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| High-dimensional Clustering onto Hamiltonian Cycle                                                                                          | - Introduces GLDC (Global-Local Deep Clustering) balancing local compactness and global separation.<br>- Learns soft cluster assignment probabilities, suitable for noisy data like SNPs.<br>- Detects and isolates outliers based on low cluster assignment probabilities.<br>- Uses Pearson correlation between clusters to define inter-cluster similarities.<br>- Maps clusters onto a circle via Hamiltonian cycle optimization for visualization.<br>- Outperforms traditional clustering and visualization methods (e.g., k-means, t-SNE, UMAP) on multiple datasets.<br>- Framework and methods are adaptable to association score (beta/Z-score) data for causal inference pipelines.                                                                                                                                 | [PDF](papers/ICML/2023_clustering/very/huang23d.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Dink-Net: Neural Clustering on Large Graphs                                                                                                 | - Proposes Dink-Net, a scalable deep graph clustering framework.<br>- Introduces cluster dilation loss and cluster shrink loss optimized adversarially.<br>- Unifies representation learning and clustering in an end-to-end fashion.<br>- Supports mini-batch training for handling extremely large graphs.<br>- Demonstrates strong performance on ogbn-papers100M (111M nodes).<br>- Could inspire methods for clustering SNPs by association strength while isolating noise clusters.<br>- Official GitHub: https://github.com/yueliu1999/Dink-Net                                                                                                                                                                                                                                                                         | [PDF](papers/ICML/2023_clustering/very/liu23v.pdf)                                                                                                                                                                                                                                                                                                                                                                                                        |
| Deep Clustering with Incomplete Noisy Pairwise Annotations: A Geometric Regularization Approach                                             | - Formal identifiability results for standard and noisy DCC losses.<br>- Introduction of a confusion matrix model to account for noisy pairwise annotations.<br>- Proposed VolMaxDCC method: jointly optimizing data embeddings and a geometric regularizer.<br>- Volume maximization regularization to enforce cluster separability and robustness.<br>- Demonstrated strong generalization to unseen data, crucial for causal inference applications.<br>- Extensive experiments on datasets (STL-10, CIFAR-10, ImageNet-10) with both synthetic and real-world noisy annotations.<br>- Codebase available for reproducibility: github.com/ductri/VolMaxDCC.                                                                                                                                                                 | [PDF](papers/ICML/2023_clustering/very/nguyen23d.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering                                                                    | - Proposes graph reconstruction to build separate homophilic and heterophilic graphs from raw data.<br>- Introduces a mixed graph filter that captures both low- and high-frequency information.<br>- Uses dual subspace embeddings to separate attribute and structure information.<br>- Outperforms state-of-the-art clustering methods across 11 benchmark datasets, especially in heterophilic settings.<br>- Demonstrates robustness in real-world, unsupervised graph clustering scenarios.<br>- Concepts generalize to any data where relationships (homophilic or heterophilic) are unknown ‚Äî highly applicable to SNP clustering with noisy beta/Z-scores.<br>- Code available for reference: github.com/Panern/DGCN.                                                                                                 | [PDF](papers/ICML/2023_clustering/very/pan23b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                        |
| Optimal LP Rounding and Linear-Time Approximation Algorithms for Clustering Edge-Colored Hypergraphs                                        | - Focus on clustering multiway colored relationships (hypergraphs).<br>- Develops optimal LP rounding for clustering with tight integrality gaps.<br>- Introduces a linear-time combinatorial 2-approximation algorithm.<br>- Uses randomized thresholding for LP rounding to handle mistakes.<br>- Proves hardness results through reductions to Vertex Cover.<br>- Results are scalable and practically verified via experiments.                                                                                                                                                                                                                                                                                                                                                                                            | [PDF](papers/ICML/2023_clustering/very/veldt23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| GC-Flow: A Graph-Based Flow Network for Effective Clustering                                                                                | - Proposes GC-Flow, a normalizing flow-based GNN that enables clustering-focused node embeddings.<br>- Models p(x                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | y)p(y) to encourage Gaussian mixture latent spaces.<br>- Develops a determinant lemma to handle graph adjacency during density estimation.<br>- Shows substantial improvements in clustering quality (silhouette scores) across standard graph datasets.<br>- Suggests parameterizations of the adjacency matrix to further boost clustering performance.<br>- Experimental results include comparisons to contrastive learning methods and classic GNNs. | [PDF](papers/ICML/2023_clustering/very/wang23y.pdf) |
| Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data                                                                 | - Introduces likelihood-adjusted SDP (LA-SDP) to handle heterogeneous clusters<br>- Profiles out centroids to avoid sensitivity to their configuration<br>- Alternates between SDP-based assignment estimation and covariance matrix updating<br>- Theoretically guarantees exact recovery under separability conditions<br>- Empirically outperforms K-means, EM, and traditional SDP on real-world datasets<br>- Robust to poor initializations and varying cluster shapes<br>- Suggests extensions for high-dimensional or large-scale data using Burer-Monteiro factorization                                                                                                                                                                                                                                              | [PDF](papers/ICML/2023_clustering/very/zhuang23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering          | - Introduces biharmonic distance as a graph metric sensitive to global structure<br>- Proposes biharmonic k-means and biharmonic Girvan-Newman clustering algorithms<br>- Develops k-harmonic distance for tunable clustering sensitivity<br>- Strong theoretical foundation: Foster-like theorems, sparsity and cuts<br>- Outperforms effective resistance and spectral clustering on multiple datasets<br>- Methods are scalable, noise-robust, and offer low-rank approximation options<br>- Direct relevance to modeling SNP associations as graphs and clustering meaningful groups                                                                                                                                                                                                                                       | [PDF](papers/ICML/2024_clustering/very/black24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering                                                           | - Develops a near-linear time O(1)-approximation algorithm for graph clustering under semi-random models<br>- Uses matrix multiplicative weights and probabilistic oracles instead of heavy SDP solvers<br>- Leverages geometric expansion properties to maintain structure despite perturbations<br>- Handles adversarial edge additions/deletions naturally<br>- Applicable to balanced cut, sparsest cut, and hierarchical clustering objectives<br>- Strong theoretical robustness against semi-random noise<br>- High scalability, appropriate for large datasets like genomic SNP data<br>- Bridges the gap between worst-case theory and practical clustering needs                                                                                                                                                     | [PDF](papers/ICML/2024_clustering/very/cohen-addad24c.pdf)                                                                                                                                                                                                                                                                                                                                                                                                |
| A3S: A General Active Clustering Method with Pairwise Constraints                                                                           | - Proposes A3S: a two-stage adaptive active clustering framework<br>- Uses information-theoretic NMI analysis to guide merging and splitting<br>- Efficient querying strategy based on expected NMI gain<br>- Robust outlier detection and handling through purity testing and splitting<br>- Scalable to massive datasets (up to 100k samples)<br>- Flexible to initial clustering algorithms and noise in the data<br>- Demonstrated superior performance over state-of-the-art active clustering baselines<br>- Directly relevant to SNP clustering based on effect size similarity and noise isolation                                                                                                                                                                                                                     | [PDF](papers/ICML/2024_clustering/very/deng24b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering                                                                 | - Differentiable Structural Information (DSI) minimizes graph uncertainty to reveal clusters without predefining K.<br>- LSEnet models graphs in hyperbolic (Lorentz) space, which captures hierarchical or tree-like structures better.<br>- Integrates node features with structure via manifold-based convolutional layers.<br>- Outperforms strong deep graph clustering baselines even without prior knowledge of cluster number.<br>- Efficient in terms of computation and scalable to large graphs.<br>- Theory-backed connection between structural entropy minimization and cluster conductance.<br>- Visualization shows clear separation of clusters in hyperbolic space.<br>- Potentially transferable methodology to cluster SNPs based on association measures (betas/Z-scores) and detect noise clusters.      | [PDF](papers/ICML/2024_clustering/very/sun24g.pdf)                                                                                                                                                                                                                                                                                                                                                                                                        |
| Interpretable Deep Clustering for Tabular Data                                                                                              | - Introduces self-supervised feature selection using sample-specific gates for sparse reconstruction.<br>- Uses a modified Maximum Coding Rate Reduction loss to enhance cluster compactness and separability.<br>- Provides interpretability both at sample-level and cluster-level by identifying informative features.<br>- Demonstrates strong performance across biomedical and general tabular datasets without relying on domain-specific augmentations.<br>- Proposes metrics for measuring interpretability (diversity, faithfulness, uniqueness, generalizability).<br>- Public code available on GitHub for replication and extension.                                                                                                                                                                              | [PDF](papers/ICML/2024_clustering/very/svirsky24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations                                         | - Model-based clustering using Wasserstein-2 distance between GMRF models.<br>- Generalized semivariogram for multivariate metric autocorrelation.<br>- Clustering loss based on hinge loss from goodness-of-fit tests, avoiding unstable EM.<br>- Demonstrated superior performance on real and synthetic datasets.<br>- Efficient and robust optimization process suitable for large datasets.<br>- Public implementation available (https://github.com/Octopolugal/MC-GTA.git).                                                                                                                                                                                                                                                                                                                                             | [PDF](papers/ICML/2024_clustering/very/wang24av.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age                                             | - Introduces CROCS, a supervised contrastive learning framework for clustering and retrieval.<br>- Learns clinical prototypes representing combinations of patient attributes (disease, sex, age).<br>- Employs hard and soft assignment strategies to map signals to prototypes.<br>- Incorporates a regularization mechanism for semantic arrangement of prototypes.<br>- Demonstrates state-of-the-art clustering and retrieval performance on ECG datasets.<br>- Prototype learning enables both clustering of unlabelled signals and attribute-specific retrieval.<br>- Framework is robust to partial/noisy labels and enhances interpretability.<br>- Techniques can inspire clustering strategies for noisy, partially informative SNP association data.                                                               | [PDF](papers/NIPS/2021_clustering/very/1691_crocs_clustering_and_retrieval.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Solving Soft Clustering Ensemble via k-Sparse Discrete Wasserstein Barycenter                                                               | - Soft clustering ensemble modeled via k-sparse Discrete Wasserstein Barycenter (DWB).<br>- Efficient sampling-based approximation algorithms with provable guarantees.<br>- Use of Wasserstein distance ensures robustness to noise and interpretability.<br>- Consensus analysis showing convergence to ground truth with enough inputs.<br>- Handles soft (fuzzy) clustering outputs rather than hard assignments.<br>- Applicable to high-dimensional and noisy data.<br>- Demonstrated superior performance compared to graph partitioning and matrix-based consensus methods.<br>- Provides a geometric framework useful for understanding and adapting to SNP association data.                                                                                                                                         | [PDF](papers/NIPS/2021_clustering/very/1914_solving_soft_clustering_ensemb.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Nearly-Tight and Oblivious Algorithms for Explainable Clustering                                                                            | - Randomized threshold cuts separate clusters based on centers, not data points<br>- Achieves O(log^2 k) approximation for k-medians and O(k log^2 k) for k-means<br>- Oblivious to the number of data points, fast O(dk log^2 k) runtime<br>- Strong theoretical lower bounds matching the upper bounds up to polylog factors<br>- Potential for robustness against noise due to randomization<br>- No direct mechanism for detecting noise clusters, would need extension                                                                                                                                                                                                                                                                                                                                                    | [PDF](papers/NIPS/2021_clustering/very/3303_nearly_tight_and_oblivious_alg.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Uniform Concentration Bounds toward a Unified Framework for Robust Clustering                                                               | - Introduces robust clustering based on Median-of-Means (MoM) estimation<br>- Achieves O(n^-1/2) convergence under minimal assumptions<br>- Applicable to various dissimilarity measures (Bregman divergences)<br>- Provides strong non-asymptotic statistical guarantees<br>- Empirical validation showing improved robustness over standard methods<br>- Handles outliers without assumptions on their distribution<br>- Directly useful for designing SNP clustering methods capable of isolating noise clusters                                                                                                                                                                                                                                                                                                            | [PDF](papers/NIPS/2021_clustering/very/3728_uniform_concentration_bounds_t.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Fuzzy Clustering with Similarity Queries                                                                                                    | - Introduces fuzzy clustering with limited similarity queries<br>- Efficient two-phase and sequential algorithms with theoretical guarantees<br>- Handles noisy oracle responses through robust averaging techniques<br>- Supports overlapping cluster membership naturally<br>- Relevant to contexts with noisy data and partial cluster assignment, like SNPs<br>- Practical and scalable (sublinear in number of points)<br>- Experimental validation on real-world datasets                                                                                                                                                                                                                                                                                                                                                | [PDF](papers/NIPS/2021_clustering/very/4091_fuzzy_clustering_with_similari.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Coresets for Clustering with Missing Values                                                                                                 | - First coreset construction for clustering with multiple missing values<br>- Near-linear time algorithm based on importance sampling and dynamic Gonzalez‚Äôs algorithm<br>- Avoids imputation; computes distances only over observed dimensions<br>- Robust to missing data without strong assumptions<br>- Significant speedup (5x or more) over full-data clustering<br>- Empirical validation across real and synthetic datasets<br>- Potentially adaptable to SNP-level clustering for causal inference                                                                                                                                                                                                                                                                                                                    | [PDF](papers/NIPS/2021_clustering/very/5096_coresets_for_clustering_with_m.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Deep Conditional Gaussian Mixture Model for Constrained Clustering                                                                          | - Proposes Deep Conditional Gaussian Mixture Model (DC-GMM) for constrained clustering.<br>- Based on Variational Autoencoder (VAE) with Gaussian Mixture Model prior.<br>- Incorporates soft, probabilistic pairwise constraints into the clustering process.<br>- Introduces the Conditional ELBO objective for efficient training.<br>- Demonstrates robustness to noisy or inconsistent constraints.<br>- Achieves state-of-the-art clustering results on diverse benchmark datasets.<br>- Supports clustering control by choosing different constraint sets.<br>- Probabilistic and generative: enables uncertainty quantification and sample generation.                                                                                                                                                                 | [PDF](papers/NIPS/2021_clustering/very/671_deep_conditional_gaussian_mixt.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Active Clustering for Labeling Training Data                                                                                                | - Active clustering framework using pairwise same-class queries<br>- Optimal chordal algorithms minimize query complexity in uniform model<br>- Efficient clique-based strategies for fixed-number-of-classes settings<br>- Handles noisy similarity queries with robust error correction<br>- Query complexity scales nearly linearly with dataset size under reasonable assumptions<br>- Practical analysis for deciding between direct labeling vs pairwise querying<br>- Highly relevant for clustering SNPs based on noisy association signals                                                                                                                                                                                                                                                                            | [PDF](papers/NIPS/2021_clustering/very/8993_active_clustering_for_labeling.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| Multi-Facet Clustering Variational Autoencoders                                                                                             | - Proposes Multi-Facet Clustering VAEs with separate Mixture-of-Gaussians priors for different facets<br>- Fully unsupervised, probabilistic model trained end-to-end<br>- Introduces a progressive training schedule and ladder architecture to disentangle facets<br>- Extends the VaDE trick for optimal ELBO optimization without sampling bias<br>- Achieves strong empirical results in multi-facet clustering, compositionality, and diversity<br>- Highly relevant for clustering SNPs by multiple biological characteristics while handling noise<br>- Robust theoretical foundations and practical empirical validation                                                                                                                                                                                              | [PDF](papers/NIPS/2021_clustering/very/9110_multi_facet_clustering_variati.pdf)                                                                                                                                                                                                                                                                                                                                                                           |
| MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering                                                                      | - Combines contrastive learning with probabilistic clustering under a unified objective.<br>- Employs a mixture of experts framework where each expert specializes in a subset of data.<br>- Uses a scalable EM algorithm for joint optimization of clustering and representation learning.<br>- Handles noise and cluster degeneracy through analytical updates and ELBO maximization.<br>- Shows significant improvement over baseline methods in unsupervised image clustering benchmarks.                                                                                                                                                                                                                                                                                                                                  | [PDF](papers/ICLR/2021_clustering/likely/1026_mice_mixture_of_contrastive_ex.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| Clustering-Friendly Representation Learning via Instance Discrimination and Feature Decorrelation                                           | - Proposes a novel softmax-formulated feature decorrelation loss to promote independent latent features.<br>- Uses instance discrimination to learn similarity-preserving embeddings without labels.<br>- Connects the method theoretically to spectral clustering behavior under certain conditions.<br>- Demonstrates that temperature tuning is critical for achieving clustering-friendly embeddings.<br>- Shows strong empirical performance across multiple deep network architectures and datasets.                                                                                                                                                                                                                                                                                                                     | [PDF](papers/ICLR/2021_clustering/likely/1734_clustering_friendly_representa.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition                                                             | - Development of ORID network to separate order-related and identity-related features.<br>- Use of repulsive term in clustering to increase inter-cluster distance and avoid noisy mixtures.<br>- Application to ordered tasks like age estimation, aesthetic score prediction, and historical image classification.<br>- Proposes a MAP-based decision rule for rank estimation within clusters.<br>- Highlights practical issues around noise, bias, and ethical implications of clustering ordered human data.                                                                                                                                                                                                                                                                                                              | [PDF](papers/ICLR/2021_clustering/likely/516_deep_repulsive_clustering_of_o.pdf)                                                                                                                                                                                                                                                                                                                                                                          |
| Evolutionary Diversity Optimization with Clustering-Based Selection for Reinforcement Learning (EDO-CS)                                     | - Proposes clustering-based selection to ensure behavioral diversity while maximizing quality<br>- Uses evolution strategies (ES) for efficient optimization of policy parameters<br>- Introduces adaptive balancing of quality vs. diversity using multi-armed bandits<br>- Manages an archive to maintain diverse, high-quality solutions<br>- Demonstrates superior performance across a range of RL environments<br>- Addresses trade-offs between exploration and exploitation explicitly and dynamically<br>- Very scalable and efficient even for high-dimensional problems<br>- Behavior-space clustering ideas can transfer to SNP effect clustering                                                                                                                                                                  | [PDF](papers/ICLR/2022_clustering/likely/2401_evolutionary_diversity_optimiz.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| Contrastive Fine-Grained Class Clustering via Generative Adversarial Networks (C3-GAN)                                                      | - Introduces contrastive loss for clustering-friendly embedding learning<br>- Latent codes act as cluster centroids during GAN training<br>- Scene decomposition (foreground vs background) without manual labels<br>- State-of-the-art clustering results on fine-grained image datasets<br>- Combines adversarial and contrastive learning objectives<br>- Improves robustness against mode collapse in GANs<br>- Concepts could inspire contrastive loss designs for SNP beta/Z-score clustering                                                                                                                                                                                                                                                                                                                            | [PDF](papers/ICLR/2022_clustering/likely/2694_contrastive_fine_grained_class.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation                                                         | - Embedding-based clustering improves evaluation quality compared to raw data comparisons.<br>- Cluster-based methods are more sensitive to meaningful (coherence/syntax) than superficial (word/article) changes.<br>- Classical divergences (e.g., KL, JS) work well when applied on clusters.<br>- Perturbation studies help identify robustness and noise susceptibility of clustering methods.<br>- PCA and K-means are effective techniques to cluster high-dimensional embeddings.<br>- Bias-variance trade-offs need careful analysis when switching from fine-grained to clustered distributions.                                                                                                                                                                                                                     | [PDF](papers/ICLR/2023_clustering/likely/2641_on_the_usefulness_of_embedding.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| Statistical Guarantees for Consensus Clustering                                                                                             | - Lifting clustering to association matrices to avoid label-switching issues.<br>- Basic and spectral aggregation algorithms with statistical consistency proofs.<br>- Random Perturbation Model (RPM) to model noisy inputs.<br>- Local refinement algorithm to boost clustering quality toward optimal rates.<br>- Theoretical guarantees: misclassification rates decay exponentially with the number of input clusterings.<br>- Extensive experiments showing improvements over traditional consensus clustering methods.                                                                                                                                                                                                                                                                                                  | [PDF](papers/ICLR/2023_clustering/likely/2988_statistical_guarantees_for_con.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders                                                               | - Proposes CMVAE with a mixture-based latent prior to encode clusters<br>- Introduces a post-hoc method to prune redundant clusters using entropy<br>- Incorporates diffusion models (DDPMs) to sharpen sample quality<br>- Demonstrates improved multimodal generative performance and missing-modality handling<br>- Highlights a weakly-supervised approach, leveraging multiple data views                                                                                                                                                                                                                                                                                                                                                                                                                                 | [PDF](papers/ICLR/2024_clustering/likely/5416_Deep_Generative_Clusterin.pdf)                                                                                                                                                                                                                                                                                                                                                                              |
| Image Clustering via the Principle of Rate Reduction in the Age of Pre-Trained Models                                                       | - Employs a rate-reduction approach (MLC) to build orthogonal low-dimensional subspaces for each cluster.<br>- Uses high-quality, pre-trained image embeddings (CLIP) to enable efficient and scalable clustering.<br>- Provides a post-hoc method to select the optimal number of clusters (avoiding multiple re-trainings).<br>- Automates cluster labeling through vision‚Äìlanguage alignment for generating descriptive captions.<br>- Achieves state-of-the-art performance on standard benchmarks (CIFAR, ImageNet) and more eclectic image collections.                                                                                                                                                                                                                                                                  | [PDF](papers/ICLR/2024_clustering/likely/5846_Image_Clustering_via_the_.pdf)                                                                                                                                                                                                                                                                                                                                                                              |
| Correlation Clustering in Constantly Many Parallel Rounds                                                                                   | - Introduces a constant-round MPC algorithm for correlation clustering based on local agreement trimming.<br>- Defines new measures of weak and strong node agreement to guide clustering decisions.<br>- Uses dense connected components of a trimmed graph to define clusters efficiently.<br>- Theoretical guarantee: constant-factor approximation to optimal correlation clustering cost.<br>- Empirical validation on massive graphs shows faster runtime and better clustering quality than prior methods.<br>- Highlights scalability challenges and opportunities in clustering noisy, large-scale data.                                                                                                                                                                                                              | [PDF](papers/ICML/2021_clustering/likely/cohen-addad21b.pdf)                                                                                                                                                                                                                                                                                                                                                                                              |
| On Variational Inference in Biclustering Models                                                                                             | - Establishes minimax-optimal estimation bounds for variational inference in biclustering models.<br>- Introduces and analyzes the CAVI algorithm with local and partial global convergence guarantees.<br>- Addresses partial observation settings, providing robustness to missing data.<br>- Demonstrates that VI can produce accurate cluster assignments under mild model assumptions.<br>- Highlights sensitivity of convergence to initialization and model misspecification.<br>- Provides detailed empirical validation on Bernoulli and Poisson synthetic datasets.                                                                                                                                                                                                                                                  | [PDF](papers/ICML/2021_clustering/likely/fang21b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| On the Price of Explainability for Some Clustering Problems                                                                                 | - Defines and analyzes the 'price of explainability' for several clustering objectives (k-means, k-medians, k-centers, maximum-spacing).<br>- Proposes Ex-Greedy, an efficient greedy algorithm for explainable k-means clustering.<br>- Provides tighter upper and lower bounds on clustering quality loss when enforcing explainability.<br>- Demonstrates empirically that Ex-Greedy often outperforms previous explainable clustering methods.<br>- Highlights that in low dimensions, the cost of requiring explainability is often quite small.<br>- Explores clustering structures based on decision-tree partitions, leading to interpretable cluster definitions.<br>- Offers a framework that could be adapted to interpretable SNP clustering using association statistics.                                         | [PDF](papers/ICML/2021_clustering/likely/laber21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Learning of Cluster-based Feature Importance for Electronic Health Record Time-series                                                       | - Proposes CAMELOT: a supervised deep learning model for clustering EHR time-series data.<br>- Introduces feature-time attention mechanisms for cluster interpretability.<br>- Develops loss functions to prevent cluster collapse and address class imbalance.<br>- Benchmarked against TSKM, SOM-VAE, and AC-TPC, achieving better cluster separability and prediction accuracy.<br>- Potentially adaptable to static SNP data by modifying the feature attention framework and adjusting loss functions for continuous association scores instead of categorical outcomes.<br>- GitHub repository available: https://github.com/hrna-ox/camelot-icml                                                                                                                                                                        | [PDF](papers/ICML/2022_clustering/likely/aguiar22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| Fair and Fast k-Center Clustering for Data Summarization                                                                                    | - Defines Private Representative k-Center (PRIV-REP-KC) combining demographic fairness and minimum cluster size constraints.<br>- Proposes a fast backbone-based clustering framework scalable to large datasets (O(nk¬≤ + k‚Åµ) runtime).<br>- Introduces flow-based optimization for enforcing private clustering efficiently.<br>- Achieves a 15-approximation factor with strong empirical results.<br>- Highlights benefits of handling multiple fairness aspects simultaneously rather than sequentially.<br>- Shows improved balance and fairness in clustering across multiple datasets (Adult, Diabetes, Electric, Query datasets).<br>- Ideas of backbone stability and minimum cluster size constraints are adaptable to SNP clustering with noise isolation goals.                                                    | [PDF](papers/ICML/2022_clustering/likely/angelidakis22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                              |
| Online and Consistent Correlation Clustering                                                                                                | - Introduces AGREE-ON: a dynamic, stable online clustering algorithm for signed graphs.<br>- Applies lazy cluster reassignments to minimize recourse while maintaining solution quality.<br>- Uses Œµ-agreement to define similarity robustly, even with noisy or evolving data.<br>- Proves logarithmic recourse per node and constant factor approximation bounds.<br>- Demonstrates empirical superiority over baseline methods like Pivot.<br>- Framework is adaptable to clustering SNPs based on noisy association measures.<br>- Highlights trade-offs between cluster quality and stability, highly relevant for causal inference applications.                                                                                                                                                                         | [PDF](papers/ICML/2022_clustering/likely/cohen-addad22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                              |
| Global Optimization of K-Center Clustering                                                                                                  | - Introduces a reduced-space branch-and-bound algorithm for global k-center clustering.<br>- Designs feasibility-based bounds tightening (FBBT) techniques to prune search regions and accelerate convergence.<br>- Proves finite-step convergence to exact global optima.<br>- Demonstrates scalability to datasets with over 14 million samples.<br>- Open-source Julia implementation available.<br>- Highlights the weaknesses of heuristic clustering (e.g., farthest-first) compared to globally optimal solutions.<br>- Methods could inform SNP clustering if adapted to biologically relevant distance metrics.                                                                                                                                                                                                       | [PDF](papers/ICML/2022_clustering/likely/shi22b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Deep Safe Incomplete Multi-view Clustering: Theorem and Algorithm                                                                           | - Proposes Deep Safe Incomplete Multi-View Clustering (DSIMVC) with theoretical safety guarantees.<br>- Dynamic neighbor mining based on learned feature representations.<br>- Automatic weighting of incomplete samples to minimize the risk of performance degradation.<br>- Bi-level optimization framework combining safe learning and semantic imputation.<br>- Empirically validated on multiple datasets like BDGP, MNIST-USPS, CCV, and Multi-Fashion.<br>- Could inspire robust clustering and noise isolation strategies for SNP association studies.                                                                                                                                                                                                                                                                | [PDF](papers/ICML/2022_clustering/likely/tang22c.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Correlation Clustering via Strong Triadic Closure Labeling: Fast Approximation Algorithms and Practical Lower Bounds                        | - Introduces the Match-Flip-Pivot (MFP) algorithm for fast approximate correlation clustering.<br>- Develops strong triadic closure labeling connections to clustering with robustness guarantees.<br>- First purely combinatorial approximation algorithms for cluster deletion with 4-approximation guarantees.<br>- Enables scalable clustering on datasets with millions of nodes.<br>- Practical methods for generating a posteriori quality guarantees for clustering results.<br>- Highlights practical applications including biological network clustering, which could inspire SNP applications.                                                                                                                                                                                                                     | [PDF](papers/ICML/2022_clustering/likely/veldt22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Bregman Power k-Means for Clustering Exponential Family Data                                                                                | - Introduces Bregman Power k-Means, combining Bregman divergences with power mean annealing.<br>- Provides closed-form updates using majorization-minimization (MM), making it efficient and scalable.<br>- Theoretical guarantees for generalization error and convergence without bounded support assumptions.<br>- Demonstrated superior performance across Gaussian, Poisson, Gamma, and Binomial data.<br>- Outperforms standard k-means and Bregman hard clustering in robustness and accuracy.<br>- Applicable to clustering problems where data do not naturally form spherical clusters, such as SNP association data.                                                                                                                                                                                                | [PDF](papers/ICML/2022_clustering/likely/vellal22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| Optimal Clustering with Noisy Queries via Multi-Armed Bandit                                                                                | - Proposes a multi-armed bandit based clustering algorithm under noisy oracle conditions.<br>- Achieves optimal query complexity up to polynomial factors: O(n(k+log n)/Œ¥¬≤).<br>- Proves a new lower bound of ‚Ñ¶(n log n/Œ¥¬≤) on query complexity.<br>- Introduces efficient recursive clustering strategies removing constant fractions of uncertain points per round.<br>- Establishes strong connections between clustering with noisy queries and stochastic block models (SBM).<br>- Could inspire confidence-driven SNP clustering strategies in the presence of noisy association signals.                                                                                                                                                                                                                                | [PDF](papers/ICML/2022_clustering/likely/xia22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Fast Algorithms for Distributed k-Clustering with Outliers                                                                                  | - Proposes Inliers-Recalling Sampling for robust outlier handling without aspect ratio dependence.<br>- Proposes Space-Narrowing Sampling for faster two-round distributed clustering.<br>- Achieves bi-criteria constant-factor approximation guarantees for k-center, k-median, and k-means with outliers.<br>- Focuses on distributed settings, emphasizing communication and computational efficiency.<br>- Experiments show significant gains in speed and communication efficiency over previous methods.<br>- Strategies for adaptive sampling and outlier recovery could inspire robust SNP clustering algorithms.                                                                                                                                                                                                     | [PDF](papers/ICML/2023_clustering/likely/huang23f.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| On Coresets for Clustering in Small Dimensional Euclidean Spaces                                                                            | - Develops small, efficient coresets for k-median clustering in small dimensions.<br>- Provides tight upper and lower bounds for coreset sizes in 1D and low dimensions.<br>- Introduces adaptive cumulative error partitioning for better coreset construction.<br>- Uses discrepancy theory and hierarchical decomposition for error management.<br>- Highlights applications in speeding up clustering algorithms, streaming, and distributed settings.<br>- Coreset construction could be adapted to SNP clustering to reduce computational cost.                                                                                                                                                                                                                                                                          | [PDF](papers/ICML/2023_clustering/likely/huang23h.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| The Catalog Problem: Clustering and Ordering Variable-Sized Sets                                                                            | - Defines the Catalog Problem: joint clustering and ordering of variable-sized sets.<br>- Proposes the Neural Ordered Clusters (NOC) model combining clustering, cardinality prediction, and ordering.<br>- Learns an adaptive, input-dependent number of clusters.<br>- Uses Set Interdependence Transformers for relational set encoding.<br>- Utilizes latent variable models and enhanced pointer networks for ordering clusters.<br>- Outperforms baselines in both clustering quality (V-Measure) and ordering (Kendall‚Äôs tau) on synthetic and real-world datasets (e.g., PROCAT).<br>- Provides freely available code and datasets for reproducibility.                                                                                                                                                                | [PDF](papers/ICML/2023_clustering/likely/jurewicz23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                 |
| Cluster Explanation via Polyhedral Descriptions                                                                                             | - Defines the Polyhedral Description Problem (PDP) for explaining clusters.<br>- Formulates PDP as an integer program, minimizing either complexity or sparsity.<br>- Uses column generation to efficiently search over an exponential number of possible half-spaces.<br>- Introduces a novel grouping strategy to scale explanations to large datasets.<br>- Demonstrates superior cluster description accuracy and interpretability compared to state-of-the-art baselines (CART, IMM, PROTO).<br>- Applicable for explaining black-box clusterings without needing to re-learn clusters.<br>- The approach emphasizes a trade-off between interpretability and explanation accuracy, which could inspire strategies for cluster quality assessment in genomic data.                                                        | [PDF](papers/ICML/2023_clustering/likely/lawless23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| End-to-end Differentiable Clustering with Associative Memories                                                                              | - Clustering framework based on Associative Memories (AMs) with end-to-end differentiability<br>- Maintains discrete hard assignments while using SGD optimization<br>- Introduces a self-supervised loss based on masked input and pattern completion<br>- Demonstrated improvements of up to 60% over k-means in Silhouette Coefficient<br>- Extensible to weighted and spherical clustering settings<br>- Potentially applicable to SNP data by adjusting the energy function and distance measures<br>- Prototypes evolve dynamically and can model interpretable cluster centroids<br>- Open-source code available at https://github.com/bsaha205/clam                                                                                                                                                                    | [PDF](papers/ICML/2023_clustering/likely/saha23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Multi-class Graph Clustering via Approximated Effective p-Resistance                                                                        | - Proposes fast approximated effective p-resistance for graph-based clustering<br>- Theoretical bounds guarantee approximation quality; exact for tree graphs<br>- Parameter p allows tuning between connectivity-based and path-based clusters<br>- Semi-supervised learning interpretation supports clustering quality<br>- Applies k-medoids to resistance distance matrix for clustering<br>- Outperforms classical p-Laplacian spectral clustering and resistance-based clustering in experiments<br>- Scalable and robust to noise in graph structure                                                                                                                                                                                                                                                                    | [PDF](papers/ICML/2023_clustering/likely/saito23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Discrete-Continuous Optimization Framework for Simultaneous Clustering and Training in Mixture Models                                       | - Introduces PRESTO, a discrete-continuous optimization algorithm for clustering and training simultaneously.<br>- No assumptions on data generative model ‚Äî flexible for real-world, noisy datasets.<br>- Optimization formulated using matroid span constraints and Œ±-submodular minimization.<br>- Provides theoretical approximation guarantees even when model estimates are imperfect.<br>- Outperforms K-means++, GMM, Mixture of Experts, and others on classification tasks.<br>- Demonstrates excellent memory-accuracy tradeoffs, relevant for resource-limited settings.<br>- PRESTO could inspire SNP clustering methods with modular fitting and separation of weak/strong signals.                                                                                                                              | [PDF](papers/ICML/2023_clustering/likely/sangani23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Partial Optimality in Cubic Correlation Clustering                                                                                          | - Introduces partial optimality for cubic (triplet-based) correlation clustering<br>- Efficiently identifies subsets of clustering solutions that can be fixed early using min-cut reductions<br>- Focuses on complete graphs with pairwise and triplet costs<br>- Demonstrates practical efficiency (runtime O(n^5.6)) on synthetic and geometric datasets<br>- Highlights that cut-based conditions are more effective than join-based conditions in practice<br>- Methods potentially adaptable to prune noise clusters in SNP clustering scenarios                                                                                                                                                                                                                                                                         | [PDF](papers/ICML/2023_clustering/likely/stein23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond                                             | - Proposes clustering-based data selection using k-means and sensitivity sampling<br>- Works under H√∂lder continuity assumptions, relaxing Lipschitz constraints<br>- Efficiently approximates loss distributions with few model evaluations<br>- Outperforms uniform and classic coreset-based methods empirically<br>- Applies to neural networks, linear regression, and fine-tuning LLMs<br>- Sampling probability balances distance from cluster centers and model loss<br>- Robust against outliers, making it suitable for noisy data scenarios                                                                                                                                                                                                                                                                         | [PDF](papers/ICML/2024_clustering/likely/axiotis24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models                                          | - Introduces Pruned Pivot, a fast, scalable variant of the PIVOT algorithm for correlation clustering<br>- Maintains 3+Œµ approximation with constant expected update time in dynamic graphs<br>- Suitable for massive graphs: runs efficiently in parallel (MPC) and local (LCA) computation models<br>- Natural way to identify and isolate noise clusters (e.g., singleton 'unlucky' nodes)<br>- Experimental results show performance within 1% of traditional methods with much lower computational cost<br>- Concepts of agreement and pruning fit well with SNP grouping based on beta/Z-score similarities<br>- Provides theoretical guarantees and empirical validation                                                                                                                                                | [PDF](papers/ICML/2024_clustering/likely/dalirrooyfard24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                            |
| Dynamic Spectral Clustering with Provable Approximation Guarantee                                                                           | - Introduces dynamic spectral clustering with strong theoretical guarantees.<br>- Maintains cluster-preserving sparsifiers updated in O(1) amortized time.<br>- Uses contracted graphs of supernodes to track cluster structures efficiently.<br>- Detects changes in cluster number via eigen-gap analysis.<br>- Scales to graphs with hundreds of thousands of nodes and millions of edges.<br>- Applicable for clustering noisy, evolving data ‚Äî matches SNP clustering needs.<br>- Can inspire SNP graph construction and dynamic cluster updating frameworks.                                                                                                                                                                                                                                                             | [PDF](papers/ICML/2024_clustering/likely/laenen24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| Image Clustering with External Guidance                                                                                                     | - Introduces externally guided clustering paradigm using external knowledge sources.<br>- Proposes TAC (Text-Aided Clustering) combining image and text modalities.<br>- Retrieves discriminative nouns for each image cluster from WordNet.<br>- Applies cross-modal mutual distillation to enhance clustering quality.<br>- Achieves state-of-the-art results on multiple clustering benchmarks including ImageNet-1K.<br>- Suggests general potential of leveraging external annotations in clustering problems.                                                                                                                                                                                                                                                                                                            | [PDF](papers/ICML/2024_clustering/likely/li24aa.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation                                                         | - Theoretically analyzes kernel matrix expectation under Gaussian data assumptions.<br>- Proposes Scalable Multiple Kernel Clustering (SMKC) using rank-k approximations.<br>- Anchor-based methods for handling large-scale datasets efficiently.<br>- No need for hyper-parameter tuning, making it practical for real-world data.<br>- Provides non-asymptotic theoretical guarantees for approximation errors.<br>- Outperforms several state-of-the-art MKC methods in experiments on large datasets.                                                                                                                                                                                                                                                                                                                     | [PDF](papers/ICML/2024_clustering/likely/liang24g.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Hierarchical Clustering: O(1)-Approximation for Well-Clustered Graphs                                                                       | - O(1)-approximation hierarchical clustering for well-clustered graphs<br>- HCwithDegrees: degree-based fast clustering for high-conductance graphs<br>- PruneMerge: prune-and-merge strategy for handling heterogeneous cluster structures<br>- Introduction of critical nodes for finer control over hierarchical structure<br>- Strong decomposition lemma ensures low cross-cluster noise<br>- Empirical validation: better performance on synthetic graphs with variable cluster densities and sizes<br>- Methodologies readily adaptable to SNP clustering tasks with minor adjustments                                                                                                                                                                                                                                  | [PDF](papers/NIPS/2021_clustering/likely/10489_hierarchical_clustering_o_1_ap.pdf)                                                                                                                                                                                                                                                                                                                                                                        |
| Row-clustering of a Point Process-valued Matrix                                                                                             | - Proposes a mixture model of multi-level marked point processes (MM-MPP) for clustering<br>- Each row modeled with multi-level log-Gaussian Cox processes (LGCPs)<br>- Introduces an efficient Expectation-Solution (ES) algorithm using functional PCA<br>- Demonstrates massive speed-ups and improved clustering stability compared to prior methods<br>- Handles repeated observations and multiple event types (analogous to SNPs affecting multiple traits)<br>- Multi-level decomposition: account-level, day-level, residual-level variability<br>- Potentially adaptable for SNP effect clustering with thoughtful modifications                                                                                                                                                                                     | [PDF](papers/NIPS/2021_clustering/likely/11369_row_clustering_of_a_point_proc.pdf)                                                                                                                                                                                                                                                                                                                                                                        |
| On Margin-Based Cluster Recovery with Oracle Queries                                                                                        | - Introduces convex hull margin and convex hull expansion trick for cluster recovery.<br>- Proposes CHEATREC and MREC algorithms achieving O(log n) query complexity under margin conditions.<br>- Formalizes one-versus-all margin, connecting to clustering stability properties (e.g., perturbation resilience).<br>- Shows theoretical optimality of query bounds in terms of cluster margin and data dimension.<br>- Applies to general pseudometric spaces, not just Euclidean settings.<br>- Highlights connections between active learning, stability, and cluster recoverability.<br>- Potentially adaptable to effect-size-based SNP clustering to isolate meaningful biological pathways.<br>- Provides rigorous proofs of recovery guarantees and lower bounds.                                                    | [PDF](papers/NIPS/2021_clustering/likely/1965_on_margin_based_cluster_recove.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| You Never Cluster Alone                                                                                                                     | - Twin-Contrast Clustering (TCC) combining instance-level and cluster-level contrastive learning.<br>- Soft probabilistic cluster assignments via Gumbel-softmax reparameterization.<br>- Cluster context aggregation using deep sets.<br>- Use of memory queues for maintaining cluster representation consistency across batches.<br>- Extensive experiments showing strong performance over benchmarks like CIFAR-10 and ImageNet-10.<br>- Soft assignments help in dealing with noise and partial cluster memberships.                                                                                                                                                                                                                                                                                                     | [PDF](papers/NIPS/2021_clustering/likely/218_you_never_cluster_alone.pdf)                                                                                                                                                                                                                                                                                                                                                                                 |
| Multi-view Contrastive Graph Clustering (MCGC)                                                                                              | - Proposes Multi-view Contrastive Graph Clustering (MCGC) for clustering noisy, multi-view graph data.<br>- Uses graph filtering to smooth node features and remove high-frequency noise.<br>- Learns a consensus graph by combining multiple views with an adaptive weighting mechanism.<br>- Applies contrastive learning at the graph-level to reinforce clustering structure.<br>- Outperforms both shallow and deep learning methods on ACM, DBLP, IMDB, Amazon datasets.<br>- Graph consensus learning and noise reduction strategies directly applicable to SNP clustering with association measures.<br>- Simple optimization strategy alternating between consensus graph updates and view weight updates.<br>- Public codebase available for reproducibility.                                                        | [PDF](papers/NIPS/2021_clustering/likely/2462_multi_view_contrastive_graph_c.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| A Kernel-based Test of Independence for Cluster-correlated Data                                                                             | - Proposes HSICcl: a kernel-based independence test for cluster-correlated data.<br>- Extends empirical HSIC by deriving asymptotic null distribution under clustered settings.<br>- Efficient approximation method using estimated eigenvalues to perform testing.<br>- Simulation studies show better type I error control and higher power than naive methods.<br>- Real data application to microbiome-metabolite association identifies more significant pathways.<br>- Kernel-based approach allows modeling complex, nonlinear dependence without strong assumptions.<br>- Highlights importance of properly adjusting for correlated samples when assessing structure.<br>- Techniques could inspire robust similarity metrics for SNP clustering tasks.                                                               | [PDF](papers/NIPS/2021_clustering/likely/3015_a_kernel_based_test_of_indepen.pdf)                                                                                                                                                                                                                                                                                                                                                                         |
| A Critique of Self-Expressive Deep Subspace Clustering                                                                                      | - Shows that the self-expressive loss function often leads to degenerate embeddings.<br>- Demonstrates that performance gains in previous deep clustering papers largely stem from post-processing.<br>- Provides theoretical proofs of ill-posedness in SEDSC objective formulations.<br>- Highlights the importance of normalization techniques (instance, dataset, or batch normalization) to stabilize embeddings.<br>- Offers important cautionary advice for embedding-based clustering designs.                                                                                                                                                                                                                                                                                                                         | [PDF](papers/ICLR/2021_clustering/not/1308_a_critique_of_self_expressive_.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Isotropy in the Contextual Embedding Space: Clusters and Manifolds                                                                          | - Introduced clustering (K-Means) to reveal local isotropy within otherwise anisotropic embedding spaces.<br>- Used PCA to analyze effective dimension and cluster separations.<br>- Discovered Swiss Roll manifold structure in GPT/GPT2 embeddings tied to word frequency.<br>- Applied Local Intrinsic Dimension (LID) to measure local geometric properties.<br>- Proposed cluster-wise center-shifting to improve isotropy measurements.<br>- Visualized and validated findings on multiple datasets (PTB, WikiText-2).<br>- Suggested that local manifold structure and clustering account for embedding effectiveness.                                                                                                                                                                                                  | [PDF](papers/ICLR/2021_clustering/not/328_isotropy_in_the_contextual_emb.pdf)                                                                                                                                                                                                                                                                                                                                                                             |
| Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction                                                         | - Introduces AV-HuBERT for self-supervised multimodal speech representation learning<br>- Uses masked cluster prediction instead of traditional supervised loss<br>- Refines cluster assignments over iterations to improve feature quality<br>- Introduces modality dropout to handle imbalance between strong and weak signals<br>- Proposes substitution-based masking to make learning more robust<br>- Outperforms previous lip-reading and speech recognition models with far less labeled data<br>- Concepts like masking, dropout, and iterative clustering could inspire methods for SNP clustering                                                                                                                                                                                                                   | [PDF](papers/ICLR/2022_clustering/not/1757_learning_audio_visual_speech_r.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters                                                                 | - Introduces Differentially Private Local Clustering (DPLC) with strong theoretical privacy guarantees.<br>- Develops a consensus-aware loss to harmonize client updates while avoiding privacy leakage.<br>- Demonstrates large performance improvements in federated face recognition with minimal overhead.<br>- Provides techniques for noise handling and robust clustering that could inform SNP clustering algorithms.<br>- Mathematical proofs for differential privacy and efficiency claims included.                                                                                                                                                                                                                                                                                                                | [PDF](papers/ICLR/2022_clustering/not/1_improving_federated_learning_f.pdf)                                                                                                                                                                                                                                                                                                                                                                               |
| DKM: Differentiable k-Means Clustering Layer for Neural Network Compression                                                                 | - Introduces a differentiable k-means clustering layer (DKM) based on attention mechanisms<br>- Supports soft clustering during training with eventual hard assignment at inference<br>- Handles multi-dimensional clustering robustly<br>- Achieves superior compression results compared to DeepCompression, HAQ, and other methods<br>- Enables direct optimization of cluster assignments with respect to a downstream loss without interfering with the original model loss<br>- No additional parameters or loss terms needed<br>- Could inform strategies for gradual and uncertainty-aware SNP clustering                                                                                                                                                                                                              | [PDF](papers/ICLR/2022_clustering/not/332_dkm_differentiable_k_means_clu.pdf)                                                                                                                                                                                                                                                                                                                                                                             |
| Learning-Augmented k-Means Clustering                                                                                                       | - Learning-augmented clustering framework with noisy predictors<br>- Robust center estimation using coordinate-wise filtering<br>- Optimized algorithm with near-linear runtime using dimension reduction and approximate nearest neighbor search<br>- Theoretical (1+O(Œ±)) approximation guarantees based on predictor error rate<br>- Handles adversarial as well as random noise in predictor labels<br>- Significant empirical improvements on internet graph, KDD, and CIFAR datasets<br>- Insight that predictors must be processed carefully, not blindly trusted                                                                                                                                                                                                                                                       | [PDF](papers/ICLR/2022_clustering/not/4103_learning_augmented_k_means_clu.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Discriminative Similarity for Data Clustering                                                                                               | - Proposes Clustering by Discriminative Similarity (CDS) framework<br>- Introduces CDSK algorithm: kernel-based similarity learning with adaptive weights<br>- Optimizes clustering via minimizing generalization error bounds<br>- Uses Rademacher complexity for bounding classifier error<br>- Strong connection to kernel density classification theory<br>- Efficient coordinate descent optimization for large datasets<br>- Significantly outperforms k-means, spectral clustering, and random forest clustering methods<br>- Scalable and robust to high-dimensional, noisy data                                                                                                                                                                                                                                       | [PDF](papers/ICLR/2022_clustering/not/4489_discriminative_similarity_for_.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Robust Fair Clustering: A Novel Fairness Attack and Defense Framework                                                                       | - Introduced a novel black-box attack that perturbs protected group labels to degrade fairness in clustering.<br>- Proposed Consensus Fair Clustering (CFC), a two-stage defense using consensus clustering and graph-based learning.<br>- Demonstrated robustness of CFC across real-world datasets compared to existing fair clustering models.<br>- Utilized self-supervised contrastive loss and fairness regularization to build resilient cluster embeddings.<br>- Techniques could inspire SNP clustering designs robust to biological noise and unstable association signals.                                                                                                                                                                                                                                          | [PDF](papers/ICLR/2023_clustering/not/1259_robust_fair_clustering_a_novel.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks                                                                  | - Incompatibility clustering: partitions based on self-generalization failure across subsets.<br>- Inverse Self-Paced Learning (ISPL): iterative data refinement minimizing self-expansion error.<br>- Boosting method to identify clean subsets from clusters.<br>- Formal statistical guarantees: convergence and error decay.<br>- Empirical success against a wide range of backdoor poisoning attacks in deep neural networks.<br>- Source code publicly available for reproducibility and adaptation.                                                                                                                                                                                                                                                                                                                    | [PDF](papers/ICLR/2023_clustering/not/3055_incompatibility_clustering_as_.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Machine Unlearning of Federated Clusters                                                                                                    | - Secure Compressed Multiset Aggregation (SCMA) for privacy-preserving data aggregation.<br>- K-means++ initialization at clients, full clustering at the server.<br>- Fast machine unlearning mechanism avoiding full retraining.<br>- Handles imbalanced clusters effectively.<br>- 84√ó average speed-up over retraining for unlearning requests.<br>- Application to real biological datasets (TCGA methylation, microbiome).<br>- Theoretical guarantees for clustering performance and unlearning complexity.                                                                                                                                                                                                                                                                                                             | [PDF](papers/ICLR/2023_clustering/not/3491_machine_unlearning_of_federate.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| The Hidden Uniform Cluster Prior in Self-Supervised Learning                                                                                | - SSL methods (SimCLR, VICReg, SwAV, MSN) implicitly encourage uniform clusters.<br>- Uniform cluster prior harms performance on class-imbalanced datasets.<br>- Proposed Prior Matching for Siamese Networks (PMSN) allows flexible, non-uniform priors.<br>- Toy experiments show semantic features are suppressed by uniform priors but recovered by power-law priors.<br>- Real-world experiments (iNaturalist18) show substantial transfer learning gains by using non-uniform priors.<br>- Concept of controlling feature space structure via priors is introduced, applicable to any high-dimensional clustering task.                                                                                                                                                                                                  | [PDF](papers/ICLR/2023_clustering/not/3748_the_hidden_uniform_cluster_pri.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Improved Learning-Augmented Algorithms for k-Means and k-Medians Clustering                                                                 | - Proposes deterministic learning-augmented algorithms for k-means and k-medians clustering.<br>- Handles label error rates up to 50%, significantly improving prior work (which only handled <14%).<br>- Uses robust subset selection and center construction to minimize the impact of noisy points.<br>- Proves tight theoretical bounds on clustering quality and computational efficiency.<br>- Experiments validate strong empirical performance across various real-world datasets.<br>- Insights on how auxiliary information can be corrected during clustering, relevant for SNP clustering with noisy beta/Z-scores.                                                                                                                                                                                                | [PDF](papers/ICLR/2023_clustering/not/4639_improved_learning_augmented_al.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Task-Customized Masked Autoencoder via Mixture of Cluster-Conditional Experts                                                               | - Identifies negative transfer in standard Masked Autoencoders (MAE) for semantically different tasks.<br>- Proposes Mixture of Cluster-Conditional Experts (MoCE) for task-customized pretraining.<br>- Clusters pretraining data based on semantic similarity using learned embeddings.<br>- Trains different experts on different clusters to avoid negative transfer.<br>- Introduces efficient expert routing and sub-model deployment strategies.<br>- Achieves +2.45% improvement over MAE in transfer accuracy across 11 tasks.<br>- Outperforms baselines in object detection and segmentation (ADE20K, COCO).<br>- Conceptually adaptable to clustering heterogeneous SNPs for Mendelian randomization.                                                                                                              | [PDF](papers/ICLR/2023_clustering/not/4851_task_customized_masked_autoenc.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| KWIKBUCKS: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals                                                              | - Two-oracle model: cheap-weak and expensive-strong signals.<br>- WeakFilterByRanking: ranks nodes based on weak similarity scores.<br>- Budgeted querying strategy using sampled pivots.<br>- Theoretical 3-approximation with controlled additive errors.<br>- Empirical validation across text and graph datasets.<br>- Practical optimizations like post-clustering merges based on weak signals.<br>- General framework easily adaptable to noisy genomic data.                                                                                                                                                                                                                                                                                                                                                           | [PDF](papers/ICLR/2023_clustering/not/5179_kwikbucks_correlation_clusteri.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Differentially-Private Clustering of Easy Instances                                                                                         | - Defines the k-tuple clustering problem with well-separated clusters as a core building block.<br>- Develops two algorithms: PrivatekAverages (theoretically stronger, higher sample needs) and PrivatekNoisyCenters (more practical).<br>- Introduces differentially private tests for checking data 'niceness' (cluster separability).<br>- Applies framework to private k-means clustering and mixture of Gaussians learning.<br>- Highlights a modular approach: test for separation, then cluster ‚Äî relevant for SNP data.<br>- Empirical evaluation shows practical feasibility despite theoretical complexity.                                                                                                                                                                                                         | [PDF](papers/ICML/2021_clustering/not/cohen21c.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| Two-way Kernel Matrix Puncturing: towards resource-efficient PCA and spectral clustering                                                    | - Introduces two-way puncturing: random deletion of entries from both data and kernel matrices.<br>- Theoretical analysis shows eigenvector structure is preserved up to a sparsity threshold.<br>- Identifies phase transitions for when clustering/PCA performance deteriorates.<br>- Demonstrates that sparse matrices can still yield accurate clustering on synthetic and real datasets.<br>- Connects punctured matrix behavior to Marcenko-Pastur and Wigner laws from random matrix theory.<br>- Suggests that puncturing can help decorrelate real-world data, making clustering theoretically easier.                                                                                                                                                                                                                | [PDF](papers/ICML/2021_clustering/not/couillet21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                    |
| Heterogeneity for the Win: One-Shot Federated Clustering                                                                                    | - Introduces k-FED: a one-shot, communication-efficient federated clustering algorithm.<br>- Shows that statistical heterogeneity can help relax cluster separation requirements.<br>- Employs a two-stage clustering: local device clustering and global center aggregation.<br>- Demonstrates robustness to device failures and partial participation.<br>- Validated on FEMNIST, Shakespeare, and synthetic Gaussian mixtures datasets.<br>- Highlights new applications for federated personalized learning and client selection.                                                                                                                                                                                                                                                                                          | [PDF](papers/ICML/2021_clustering/not/dennis21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Efficient Online Learning for Dynamic k-Clustering                                                                                          | - Introduces Dynamic k-Clustering framed as an online learning problem.<br>- Proposes fractional placement of centers, solved efficiently via convex optimization and subgradient methods.<br>- Provides deterministic (Œò(k)) and randomized (Œò(r)) rounding schemes to obtain discrete center sets.<br>- Proves impossibility of constant regret under standard complexity assumptions.<br>- Demonstrates major empirical improvements in clustering dynamic client sets under various realistic scenarios.<br>- Builds a general framework for extending fractional relaxation to other combinatorial online learning problems.                                                                                                                                                                                              | [PDF](papers/ICML/2021_clustering/not/fotakis21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning                                  | - Introduces clustered sampling to improve client selection in federated learning.<br>- Provides theoretical proofs of lower sampling variance and unbiased aggregation.<br>- Develops two algorithms: one based on sample size clustering, one on model similarity clustering.<br>- Demonstrates superior performance in non-iid settings on MNIST and CIFAR-10 datasets.<br>- Maintains compatibility with existing privacy-preserving and communication-efficient FL methods.<br>- Highlights the first theoretical investigation of variance properties in client sampling.                                                                                                                                                                                                                                                | [PDF](papers/ICML/2021_clustering/not/fraboni21a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Approximate Group Fairness for Clustering                                                                                                   | - Introduces core fairness for clustering, ensuring no large group can deviate for better distance costs.<br>- Proposes two relaxation dimensions: distance improvement (Œ≤) and coalition size (Œ±).<br>- Designs approximation algorithms for line, tree, and general metric spaces.<br>- Proves existence and efficiency bounds for approximate core clustering.<br>- Develops two-stage algorithms to refine clustering quality while maintaining fairness.<br>- Shows experimentally that fair clusterings outperform classic algorithms like k-means++ on group fairness.<br>- Fairness frameworks could be adapted to SNP clustering to detect and isolate noise clusters or unfair groupings.                                                                                                                            | [PDF](papers/ICML/2021_clustering/not/li21j.pdf)                                                                                                                                                                                                                                                                                                                                                                                                          |
| Sharper Generalization Bounds for Clustering                                                                                                | - Introduces a general framework for clustering based on pairwise functions and partitions.<br>- Develops clustering Rademacher complexity and local clustering Rademacher complexity.<br>- Obtains excess risk bounds of O(K¬≤/n) for general clustering and O(K/n) for hard clustering under mild assumptions.<br>- Avoids strong margin conditions required in previous work, making results more applicable to real-world data.<br>- Techniques applicable across k-means, kernel k-means, spectral clustering, and neural network-based clustering.<br>- Highlights that controlling model complexity via covering numbers is sufficient for strong generalization guarantees.<br>- Lays a statistical foundation for clustering methods that aim to balance accuracy and robustness to noise.                             | [PDF](papers/ICML/2021_clustering/not/li21k.pdf)                                                                                                                                                                                                                                                                                                                                                                                                          |
| One Pass Late Fusion Multi-View Clustering                                                                                                  | - Proposes OP-LFMVC, a unified one-pass clustering method combining consensus learning and label generation.<br>- Develops a four-step alternate optimization algorithm with theoretical convergence guarantees.<br>- Proves a generalization error bound using Rademacher complexity analysis.<br>- Demonstrates superior performance compared to state-of-the-art multi-view clustering algorithms across several datasets.<br>- Algorithm is parameter-free and scales linearly with data size, suitable for large-scale clustering tasks.<br>- Highlights the benefits of learning cluster labels directly rather than relying on post-processing (e.g., k-means).<br>- Concepts of robust clustering and negotiation between partition and labels could inspire SNP clustering adaptations.                               | [PDF](papers/ICML/2021_clustering/not/liu21l.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering                                                     | - Proposes using random projections to reduce data dimensionality based on the intrinsic doubling dimension.<br>- Proves that clustering quality (facility location cost, MST structure) is preserved after dimensionality reduction.<br>- Demonstrates significant runtime improvements (up to 80x) with small loss in solution quality.<br>- Establishes new theoretical guarantees and lower bounds for facility location and MST after projection.<br>- Experimental validation on MNIST and Faces datasets supports practical benefits.<br>- Highlights that in low doubling dimension settings, high-dimensional clustering can be significantly accelerated.                                                                                                                                                            | [PDF](papers/ICML/2021_clustering/not/narayanan21b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels                                                           | - Replaces anchor-point reliance with a 2-nearest neighbor clusterability assumption.<br>- Introduces High-Order Consensus (HOC) method based on first-, second-, and third-order label agreements.<br>- Provides theoretical guarantees (uniqueness of solution, sample complexity benefits).<br>- Demonstrates effectiveness on synthetic and real noisy datasets (CIFAR-10/100, Clothing1M).<br>- Potentially extendable to local instance-dependent noise settings, suggesting flexibility in different noise scenarios.<br>- Open-source implementation available at https://github.com/UCSC-REAL/HOC.                                                                                                                                                                                                                    | [PDF](papers/ICML/2021_clustering/not/zhu21e.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Massively Parallel k-Means Clustering for Perturbation Resilient Instances                                                                  | - Introduces massively parallel k-means clustering algorithms for perturbation-resilient instances.<br>- Uses Locality Sensitive Hashing (LSH) for fast neighborhood graph construction.<br>- Applies hierarchical clustering trees and dynamic programming for cluster extraction.<br>- Achieves optimal or near-optimal k-means clustering in O(1) parallel rounds.<br>- Demonstrates strong empirical performance even without perfect perturbation resilience.<br>- Techniques are highly adaptable for clustering SNPs based on beta/Z-score similarities.<br>- Focus on bypassing worst-case hardness aligns well with biological data clustering needs.                                                                                                                                                                 | [PDF](papers/ICML/2022_clustering/not/cohen-addad22b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                 |
| A Random Matrix Analysis of Data Stream Clustering: Coping With Limited Memory Resources                                                    | - Introduces a new online spectral clustering method using a banded (Toeplitz-masked) kernel matrix.<br>- Applies random matrix theory to characterize phase transitions and noise effects in high-dimensional clustering.<br>- Demonstrates that only limited memory is needed to achieve near-optimal clustering performance.<br>- Develops a practical online clustering algorithm tested on image data (Fashion-MNIST, BigGAN).<br>- Shows how eigenvector behavior changes with increasing data sparsity, relevant for noise cluster isolation.<br>- Provides an open-source codebase for reproducing simulations.                                                                                                                                                                                                        | [PDF](papers/ICML/2022_clustering/not/lebeau22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering                                                               | - Proposes a distributed clustering approach robust to heterogeneity and communication limits.<br>- Uses local clustering + global centroid aggregation for efficiency and privacy.<br>- Introduces theoretical bounds linking clusterability to generalization error.<br>- Demonstrates that increased heterogeneity can improve performance.<br>- Sinkhorn-Knopp based clustering to enforce equal-sized partitions.<br>- Demonstrates very low computational overhead suitable for resource-constrained environments.                                                                                                                                                                                                                                                                                                       | [PDF](papers/ICML/2022_clustering/not/lubana22a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Convergence and Recovery Guarantees of the K-Subspaces Method for Subspace Clustering                                                       | - Provides superlinear convergence guarantees for the K-Subspaces method under semi-random union of subspaces models.<br>- Introduces TIPS initialization based on thresholded inner-products and spectral clustering.<br>- Demonstrates robustness to subspace overlap and moderate affinity between clusters.<br>- Empirically competitive with sparse subspace clustering (SSC) and other methods.<br>- Algorithmic simplicity and linear computational cost per iteration make it scalable to large datasets.<br>- Could inspire robust, scalable SNP clustering methods with strong noise isolation properties.                                                                                                                                                                                                           | [PDF](papers/ICML/2022_clustering/not/wang22r.pdf)                                                                                                                                                                                                                                                                                                                                                                                                        |
| Parallel Online Clustering of Bandits via Hedonic Game                                                                                      | - Novel use of hedonic games for self-organizing clustering<br>- Parallel online updating, which may inspire SNP clustering pipelines with dynamic re-analysis<br>- Noise robustness through cost-based user incentives<br>- Theoretical guarantees on convergence and regret<br>- No need for predefining cluster numbers<br>- Empirical validation on large real-world datasets (Netflix, MovieLens)                                                                                                                                                                                                                                                                                                                                                                                                                         | [PDF](papers/ICML/2023_clustering/not/cheng23d.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| Approximation Algorithms for Fair Range Clustering                                                                                          | - Proposes fair range clustering with group representation intervals rather than strict quotas.<br>- Provides constant-factor approximation algorithms for k-means, k-median, and k-center under fairness constraints.<br>- Uses LP relaxation (FAIRRANGELP and STRUCTUREDLP) with rounding strategies based on network flow.<br>- Introduces efficient rounding from half-integral solutions exploiting total unimodularity.<br>- Demonstrates that relaxed fairness constraints significantly improve clustering quality.<br>- Focus is on fairness and group balance, not noise resistance or association score clustering.                                                                                                                                                                                                 | [PDF](papers/ICML/2023_clustering/not/hotegni23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost                                                  | - Introduces explainable tree operators for modifying hierarchical clusterings.<br>- Achieves near-exponential improvements in fairness-cost tradeoffs.<br>- Handles fairness for multiple protected classes and varying class proportions.<br>- Provides algorithms for both deterministic and stochastic fairness.<br>- Applies and validates algorithms on real-world datasets (Census and Bank).<br>- Focuses on hierarchical clustering cost minimization using Dasgupta‚Äôs cost.                                                                                                                                                                                                                                                                                                                                          | [PDF](papers/ICML/2023_clustering/not/knittel23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs                                                                            | - Proposes SpecWRSC: a nearly-linear time spectral clustering + bucketing method.<br>- Achieves O(1)-approximate hierarchical clustering cost under Dasgupta‚Äôs metric.<br>- Introduces degree-based bucketing to simplify hierarchical tree construction.<br>- Avoids reliance on high inner-cluster conductance (a limitation of prior work).<br>- Extensive experiments on synthetic (SBM, HSBM) and real-world datasets.<br>- Algorithm is scalable: handles graphs with tens of thousands of nodes.<br>- Potentially useful for large-scale clustering of SNPs with well-separated signals.                                                                                                                                                                                                                                | [PDF](papers/ICML/2023_clustering/not/laenen23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Consistency of Multiple Kernel Clustering                                                                                                   | - First theoretical proof of kernel weight consistency in multiple kernel clustering (MKC).<br>- Non-asymptotic bound of OÃÉ(k/‚àön) for kernel weight convergence.<br>- Excess clustering risk bound for SimpleMKKM.<br>- Nystrom-based scalable MKC algorithm with learning guarantees.<br>- Validation on large datasets (up to 580,000 samples) showing scalability and effectiveness.<br>- Highlights the importance of eigenvalue gaps for clustering stability.<br>- Suggests that kernel-based methods can be extended to very large biological datasets if needed.                                                                                                                                                                                                                                                        | [PDF](papers/ICML/2023_clustering/not/liang23b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| CLUSTSEG: Clustering for Universal Segmentation                                                                                             | - Unifies multiple segmentation tasks through clustering.<br>- Introduces 'Dreamy-Start': task-specific initialization of cluster centers.<br>- Uses Recurrent Cross-Attention: mimicking Expectation-Maximization (EM) clustering.<br>- Demonstrates state-of-the-art results on COCO Panoptic, ADE20K, and BSDS500 datasets.<br>- Framework handles heterogeneity between tasks (semantic vs instance vs superpixel).<br>- No extra parameters are needed for iterative clustering.<br>- Focuses on robust, informative cluster center selection to mitigate noise.<br>- Could inspire SNP clustering designs by adapting cluster initialization and EM-like iterative refinement.                                                                                                                                           | [PDF](papers/ICML/2023_clustering/not/liang23h.pdf)                                                                                                                                                                                                                                                                                                                                                                                                       |
| Spurious Valleys and Clustering Behavior of Neural Networks                                                                                 | - Explicit bounds on hidden layer sizes to avoid spurious valleys (Theorems 3.7, 3.10)<br>- Clustering of singular error points as indicators of model representability (Theorem 4.4)<br>- Use of algebraic geometry techniques to study neural network loss landscapes<br>- Potentially inspiring ideas for defining and isolating 'noise clusters' in complex spaces<br>- Related work connections to landscape geometry, algebraic approaches, and robustness studies in deep learning                                                                                                                                                                                                                                                                                                                                      | [PDF](papers/ICML/2023_clustering/not/pollaci23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Effective Neural Topic Modeling with Embedding Clustering Regularization                                                                    | - Problem: Topic collapsing (semantic redundancy) in neural topic models<br>- Proposed method: Embedding Clustering Regularization (ECR) using optimal transport<br>- New model: ECRTM (Embedding Clustering Regularization Topic Model)<br>- Avoids trivial solutions by presetting cluster size constraints<br>- Joint optimization with standard VAE-based topic modeling<br>- Extensive evaluation on benchmark datasets shows improved topic coherence, diversity, and document clustering<br>- Potentially adaptable to SNP clustering by treating SNPs' association statistics as embeddings                                                                                                                                                                                                                            | [PDF](papers/ICML/2023_clustering/not/wu23c.pdf)                                                                                                                                                                                                                                                                                                                                                                                                          |
| Near-Optimal Quantum Coreset Construction Algorithms for Clustering                                                                         | - Quantum coreset construction reduces n-point dataset to size polynomial in k, d, and 1/epsilon<br>- Introduces a novel subroutine: multidimensional quantum approximate summation<br>- Achieves OÃÉ(‚àönkd^{3/2}) query complexity for clustering tasks<br>- Proves matching quantum lower bounds for clustering complexity<br>- Highlights quantum speedup potential for clustering without requiring strong assumptions like sparsity<br>- Coreset approach generalizable across k-median, k-means, and (k,z)-clustering problems                                                                                                                                                                                                                                                                                              | [PDF](papers/ICML/2023_clustering/not/xue23a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| CHAI: Clustered Head Attention for Efficient LLM Inference                                                                                  | - Proposes dynamic clustering of attention heads based on high correlation in outputs<br>- Reduces inference memory (K,V cache) by up to 21.4% and speeds up inference by 1.73√ó<br>- Maintains high model accuracy with minimal loss (within 3.2%)<br>- Uses elbow plots for offline determination of number of clusters<br>- Online cluster membership determination after processing five tokens<br>- Ideas about redundancy, dynamic grouping, and lightweight clustering could inspire SNP clustering methods                                                                                                                                                                                                                                                                                                              | [PDF](papers/ICML/2024_clustering/not/agarwal24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                     |
| Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better                                                              | - Focus on Cluster Deletion: partitioning graphs into disjoint cliques by minimal edge deletion<br>- Improved approximation guarantees: 3-approximation algorithms developed<br>- Deterministic, degree-based pivoting strategy for clustering<br>- Combinatorial solution replacing expensive LP solvers<br>- Efficient algorithms scaling to graphs with millions of nodes and billions of edges<br>- Robustness to noisy edges through careful edge deletion strategies<br>- Applicable to biological network clustering, including gene expression data                                                                                                                                                                                                                                                                    | [PDF](papers/ICML/2024_clustering/not/balmaseda24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                   |
| Dynamic Correlation Clustering in Sublinear Update Time                                                                                     | - Proposes the Dynamic Agreement Algorithm for correlation clustering<br>- Uses sparse graph maintenance via anchor nodes and probabilistic agreement checking<br>- Achieves O(1)-approximation to optimal clustering cost<br>- Maintains sublinear polylog(n) update time per node addition/deletion<br>- Highly scalable, robust against noisy graph perturbations<br>- Experimentally validated on large real-world graph datasets<br>- Framework easily adaptable to SNP clustering with appropriate graph construction                                                                                                                                                                                                                                                                                                    | [PDF](papers/ICML/2024_clustering/not/cohen-addad24d.pdf)                                                                                                                                                                                                                                                                                                                                                                                                 |
| EDISON: Enhanced Dictionary-Induced Tensorized Incomplete Multi-View Clustering with Gaussian Error Rank Minimization                       | - Enhanced Dictionary Representation (EDR) for missing data and subspace recovery<br>- Gaussian Error Rank (GER) for noise-robust tensor decomposition<br>- Hyper-anchor graph Laplacian Regularization (HLR) for preserving local manifold structure<br>- Optimization with guaranteed convergence (ADMM-based)<br>- Superior performance over 8 strong baselines on incomplete multi-view datasets<br>- Scalable to large datasets with low computational overhead<br>- Strong ablation studies and robustness analyses                                                                                                                                                                                                                                                                                                      | [PDF](papers/ICML/2024_clustering/not/gu24b.pdf)                                                                                                                                                                                                                                                                                                                                                                                                          |
| FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering                                            | - Introduces a novel clustering principle separating concept shifts from feature and label shifts<br>- Develops FedRC: a soft-clustering bi-level optimization framework<br>- Objective function encourages consistent decision boundaries<br>- Demonstrated robust performance across major datasets (FashionMNIST, CIFAR10, Tiny-ImageNet)<br>- Handles imbalanced, noisy, and shifting data distributions<br>- Potential applicability to SNP clustering where pleiotropy and pathway overlap exist<br>- Soft clustering assignments could mirror SNPs' partial membership to causal groups                                                                                                                                                                                                                                 | [PDF](papers/ICML/2024_clustering/not/guo24f.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework                                                       | - First adversarial attack framework specifically for DMVC<br>- GAN-based attack targeting multi-view complementarity and consistency<br>- Proposed AR-DMVC and AR-DMVC-AM models for adversarially robust clustering<br>- Information-theoretic Attack Mitigator minimizes mutual information to boost robustness<br>- Comprehensive experiments on RegDB, NoisyFashion, NoisyMNIST, and PatchedMNIST datasets<br>- Highlights vulnerability of deep clustering models to subtle perturbations<br>- Potential inspiration for designing robust SNP clustering algorithms under noisy data conditions                                                                                                                                                                                                                          | [PDF](papers/ICML/2024_clustering/not/huang24ai.pdf)                                                                                                                                                                                                                                                                                                                                                                                                      |
| Clustered Federated Learning via Gradient-based Partitioning                                                                                | - Proposes CFL-GP: Clustered Federated Learning via Gradient-based Partitioning<br>- Uses moving averages of gradients as clustering features<br>- Applies spectral clustering with dimensionality reduction for scalability<br>- Provides theoretical guarantees for convergence in clustering and model optimization<br>- Achieves rapid convergence and robustness to noise and heterogeneity<br>- Empirically validated on synthetic regression, rotated MNIST, CIFAR10, and industrial tasks<br>- Potentially adaptable to SNP clustering by redefining gradient features as SNP association profiles                                                                                                                                                                                                                     | [PDF](papers/ICML/2024_clustering/not/kim24p.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Cluster-Aware Similarity Diffusion for Instance Retrieval                                                                                   | - Introduces Cluster-Aware Similarity Diffusion restricting propagation to local clusters.<br>- Proposes Bidirectional Similarity Diffusion (BSD) for smoother and symmetric similarity matrices.<br>- Introduces Neighbor-guided Similarity Smooth (NSS) to maintain local neighbor consistency.<br>- Employs Jensen-Shannon divergence to enhance distance measures post-diffusion.<br>- Demonstrates superior retrieval performance over state-of-the-art re-ranking methods.<br>- Highlights the importance of avoiding global noise contamination in graph-based similarity propagation.                                                                                                                                                                                                                                  | [PDF](papers/ICML/2024_clustering/not/luo24i.pdf)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Expand-and-Cluster: Parameter Recovery of Neural Networks                                                                                   | - Introduction of Expand-and-Cluster method for neural network parameter recovery<br>- Key role of overparameterization to ease optimization landscapes<br>- Novel clustering approach to separate true neurons from noise<br>- Handling of activation function symmetries (even, odd, scaling effects)<br>- Application to both synthetic data (XOR-type problems) and real-world datasets (MNIST, CIFAR-10)<br>- Comparison to pruning methods; Expand-and-Cluster outperforms traditional pruning for parameter recovery<br>- Potential relevance to model reverse engineering and neuroscience connectivity inference<br>- Discussion of theoretical limitations and future directions, including scaling to larger networks                                                                                               | [PDF](papers/ICML/2024_clustering/not/martinelli24a.pdf)                                                                                                                                                                                                                                                                                                                                                                                                  |
| Robust Online Correlation Clustering                                                                                                        | - Robust online clustering under adversarial noise<br>- Dynamic coreset maintenance to trust certain points more<br>- Noise filtering techniques for similarity links<br>- Theoretical guarantees for clustering accuracy under noise<br>- Useful inspiration for isolating signal vs noise in SNP clustering<br>- Focus on pairwise similarity rather than continuous feature data                                                                                                                                                                                                                                                                                                                                                                                                                                            | [PDF](papers/NIPS/2021_clustering/not/4647_robust_online_correlation_clus.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Clustering Effect of (Linearized) Adversarial Robust Models                                                                                 | - Linearization of neural networks by removing non-linear layers to analyze weight structure.<br>- Discovery of a hierarchical clustering effect aligned with class labels in robust models.<br>- Introduction of a clustering regularization penalty to enforce semantic groupings.<br>- Demonstrated improvements in both adversarial robustness and domain adaptation tasks.<br>- Use of weight correlation matrices to measure inter-class relationships.<br>- Findings suggest that better hierarchical representation leads to more robust and transferable models.                                                                                                                                                                                                                                                      | [PDF](papers/NIPS/2021_clustering/not/482_clustering_effect_of_adversari.pdf)                                                                                                                                                                                                                                                                                                                                                                             |
| Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem                                                     | - First invex relaxation for a combinatorial clustering and regression problem<br>- Simultaneous recovery of sparse regression vector and hidden binary clustering<br>- Logarithmic sample complexity with feature size<br>- Fairness constraints incorporated without access to sensitive attributes<br>- Strong theoretical guarantees and primal-dual construction proofs<br>- Empirical validation on synthetic and real-world datasets<br>- Framework could inspire SNP clustering models that handle hidden biological subgrouping                                                                                                                                                                                                                                                                                       | [PDF](papers/NIPS/2021_clustering/not/7805_fair_sparse_regression_with_cl.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Parallel and Efficient Hierarchical k-Median Clustering                                                                                     | - First distributed algorithm for hierarchical Euclidean k-median clustering with guarantees<br>- Uses randomized quadtree embedding into 2-RHST for tree-based approximation<br>- Greedy, quasi-linear time algorithm for clustering on the tree structure<br>- Handles all k-values simultaneously (nested clusters)<br>- Near-linear total work and logarithmic parallel rounds<br>- Empirical validation shows substantial speed gains over previous methods<br>- Highly scalable to datasets with billions of points<br>- Adaptations needed for SNP noise detection and non-Euclidean association structures                                                                                                                                                                                                             | [PDF](papers/NIPS/2021_clustering/not/8652_parallel_and_efficient_hierarc.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Coresets for Time Series Clustering                                                                                                         | - First coreset method for clustering time series with Gaussian mixtures and autocorrelations<br>- Reduces full clustering problem to a k-means problem on averaged entity observations<br>- Two-stage importance sampling to select small, representative subsets<br>- Provides theoretical approximation guarantees on clustering quality<br>- Coreset size independent of number of entities or time points, only depends on k, d, and 1/Œµ<br>- Empirically validated: 14x-171x faster than full data clustering<br>- Highly promising for scaling up SNP clustering tasks with necessary adaptations                                                                                                                                                                                                                       | [PDF](papers/NIPS/2021_clustering/not/9329_coresets_for_time_series_clust.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| Better Algorithms for Individually Fair k-Clustering                                                                                        | - Formulates fair k-clustering as a linear program and rounds solutions efficiently<br>- Develops Fair-Round algorithm achieving strong fairness and cost guarantees<br>- Sparsification method greatly improves LP solving speed for large datasets<br>- Empirical results show minimal fairness violations and near-optimal clustering cost<br>- Fine-grained fairness analysis with histograms of individual fairness violations<br>- Framework is adaptable to SNP clustering where local fairness (signal strength consistency) is critical<br>- Scalable to very large datasets via LP sparsification and filtering techniques                                                                                                                                                                                           | [PDF](papers/NIPS/2021_clustering/not/9679_better_algorithms_for_individu.pdf)                                                                                                                                                                                                                                                                                                                                                                            |
| A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space                                                               | - Introduces a cluster-based method using K-means and PCA to improve isotropy.<br>- Shows local isotropy enhancement often outperforms global methods for embeddings.<br>- Deals explicitly with noise and structured redundancy in data representations.<br>- Reproducibility challenges indicate care is needed with hyperparameters and clustering size.<br>- Open-source code available at https://github.com/Sara-Rajaee/clusterbased_isotropy_enhancement.<br>- Findings could inspire strategies for clustering numeric feature vectors like SNP effects.                                                                                                                                                                                                                                                               | [PDF](papers/NIPS/2022_clustering/not/A Cluster-based Approach for Improving Isotropy in.pdf)                                                                                                                                                                                                                                                                                                                                                             |
