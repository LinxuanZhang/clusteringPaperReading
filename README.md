# üìö Clustering Papers Collection

Total Papers: 150

This repository collects clustering papers from ICLR, ICML, and NIPS (2021-2024), sorted by relevance.

| Title | Summary | Key Points | PDF Link |
|------|---------|------------|----------|
| Sparse Quantized Spectral Clustering | This paper explores how entry-wise nonlinear operations like sparsification, quantization, and binarization affect the performance of spectral clustering. Using random matrix theory, the authors show that while aggressive sparsification or quantization drastically reduces computation and storage costs, it minimally impacts clustering quality. They characterize phase transitions for clustering success, propose optimal transformation strategies, and highlight the potential for spurious (non-informative) eigenvectors resulting from non-linear transformations. | - Introduces selective sparsification and quantization methods for spectral clustering.<br>- Uses random matrix theory to predict phase transitions in clustering performance.<br>- Highlights existence of spurious eigenvectors caused by nonlinear transformations.<br>- Provides optimal thresholds for sparsification and quantization to maximize performance while reducing complexity.<br>- Validates findings through experiments on real-world datasets and theoretical analysis. | [PDF](papers/ICLR/2021_clustering/very/525_sparse_quantized_spectral_clus.pdf) |
| Fast Topological Clustering with Wasserstein Distance | This paper presents a novel and efficient method for clustering complex networks based on topological similarity. Using persistent homology to capture connected components and cycles, it computes Wasserstein distances between persistence barcodes and clusters networks based on a joint topological and geometric metric. The method achieves high accuracy and scalability on both simulated modular networks and real-world brain connectivity data, demonstrating superior ability to detect subtle structural variations in noisy data. | - Uses persistent homology to extract 0D and 1D topological features (connected components, cycles) from graphs<br>- Introduces a closed-form, fast computation of Wasserstein distance for 1D persistence barcodes<br>- Defines a topological centroid (average birth and death points) for cluster center updates<br>- Combines geometric and topological distances into a flexible clustering criterion using Œª weighting<br>- Validated on simulated modular networks and real brain connectivity data under anesthesia<br>- Robust against noise and missing direct correspondence between individual edge weights<br>- Clustering framework generalizes to noisy, continuous-valued, large datasets like SNP association statistics | [PDF](papers/ICLR/2022_clustering/very/1709_fast_topological_clustering_wi.pdf) |
| A Deep Variational Approach to Clustering Survival Data | This paper introduces VaDeSC, a semi-supervised probabilistic model that clusters survival data by jointly modeling explanatory variables and censored survival outcomes using a deep variational autoencoder with a Gaussian mixture prior. Survival times are modeled with cluster-specific Weibull distributions. The model optimizes an evidence lower bound (ELBO) to efficiently learn both feature and outcome distributions. VaDeSC outperforms existing survival clustering methods on synthetic and real-world datasets, including clinical imaging data, and offers interpretable, structured clustering that reflects both covariates and outcomes. | - Proposes VaDeSC: a semi-supervised probabilistic clustering model<br>- Uses a Gaussian Mixture Variational Autoencoder to jointly model features and outcomes<br>- Survival outcomes are modeled with cluster-specific Weibull distributions<br>- Handles censored data and missing outcomes robustly<br>- Optimizes an ELBO using stochastic gradient variational inference (SGVI)<br>- Demonstrates superior clustering performance on synthetic, semi-synthetic, and real-world datasets<br>- Capable of discovering clusters reflecting true underlying feature-outcome mechanisms<br>- Framework highly adaptable to high-dimensional SNP beta/Z-score data | [PDF](papers/ICLR/2022_clustering/very/2773_a_deep_variational_approach_to.pdf) |
| Graphon Based Clustering and Testing of Networks: Algorithms and Theory | This paper introduces a new graphon-based framework for clustering network-valued data and performing two-sample testing, particularly when graphs lack vertex correspondence. It defines a novel distance between graphs by estimating their underlying graphons through sorting-and-smoothing transformations and measuring their L2-distance. Two clustering algorithms are proposed: Distance-based Spectral Clustering (DSC) and Similarity-based Semi-Definite Programming (SSDP), both with theoretical consistency guarantees. Experiments on simulated and real-world data show that their methods outperform graph kernels and graph matching approaches in accuracy and scalability, especially in small sample, large graph settings. | - Introduces graphon-based distance estimation using sorting-and-smoothing<br>- Proposes Distance-based Spectral Clustering (DSC) with consistency guarantees<br>- Develops Similarity-based Semi-Definite Programming (SSDP) with zero error asymptotic guarantees<br>- Applies graph distance to two-sample testing between networks<br>- Outperforms graph kernel and graph matching methods on simulated and real data<br>- Highly scalable to large graphs and effective with small samples<br>- Potential for adaptation to SNP data clustering by treating SNP-trait associations as graphon analogs | [PDF](papers/ICLR/2022_clustering/very/3907_graphon_based_clustering_and_t.pdf) |
| Contrastive Clustering to Mine Pseudo-Parallel Data for Unsupervised Translation | This paper presents LAgSwAV, a novel loss function based on SwAV contrastive clustering, designed to fine-tune language models to produce language-agnostic and semantically meaningful embeddings. It proposes using soft cluster assignments as sentence embeddings to mine pseudo-parallel data from monolingual corpora, followed by a rigorous multi-step filtering process to suppress noise. The mined data is used to augment training of unsupervised machine translation models, achieving state-of-the-art results on multiple translation benchmarks. | - Introduces LAgSwAV, a language-agnostic variant of SwAV clustering<br>- Uses soft cluster assignments to represent samples, enabling uncertainty modeling<br>- Applies a rigorous multi-step filter suite to detect and remove noise<br>- Develops rank-based cross-entropy loss to prioritize high-confidence pairs<br>- Achieves state-of-the-art results on standard and low-resource translation tasks<br>- Visual and quantitative evaluation of embedding quality and clustering robustness<br>- Soft clustering and rebalancing could inspire methods for SNP data with high noise | [PDF](papers/ICLR/2022_clustering/very/626_contrastive_clustering_to_mine.pdf) |
| ADA-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space | This paper presents ADA-NETS, a novel face clustering method that builds cleaner graphs by transforming features into a structure space and dynamically learning the number of neighbor connections for each data point. The method addresses the common problem of noise edges in graph construction, using a heuristic quality-driven adaptive filter based on an FŒ≤-score and a robust similarity metric combining cosine and Jaccard similarities. ADA-NETS achieves state-of-the-art clustering results on several datasets, showing strong generalization and robustness against noisy input data. | - Introduces adaptive neighbor discovery for graph construction<br>- Uses structure space transformation combining cosine and Jaccard similarities<br>- Focuses heavily on detecting and removing noise edges<br>- State-of-the-art clustering results on MS-Celeb-1M and other datasets<br>- Learned heuristic (FŒ≤-score) to balance precision and recall in neighborhood discovery<br>- GCN-based feature refinement with noise-resilient graph input<br>- Robust against changes in neighborhood size parameter k<br>- Methods could be transferred to SNP-level clustering with continuous effect sizes | [PDF](papers/ICLR/2022_clustering/very/773_ada_nets_face_clustering_via_a.pdf) |
| Near-Optimal Coresets for Robust Clustering | This paper presents a near-optimal method for constructing Œµ-coresets for robust clustering problems (k-median and k-means) with a known number of outliers. Their method isolates outliers and partitions inliers into geometric structures called rings and groups, applying careful sampling to preserve clustering objectives within Œµ-relative error. Their coresets significantly outperform prior baselines in accuracy and efficiency, achieving consistent low error and substantial speedups in real-world datasets. They also prove that their dependence on the number of outliers is nearly theoretically optimal. | - Introduces near-optimal Œµ-coreset construction for robust k-median and k-means clustering with m outliers.<br>- Coreset size is O(m + poly(k, Œµ‚Åª¬π)) with near-linear construction time.<br>- Utilizes a geometric decomposition into rings and groups to sample efficiently.<br>- Empirical experiments show consistent performance gains over uniform and sensitivity sampling.<br>- Enables approximately 100√ó speedup for approximate clustering algorithms while maintaining clustering quality.<br>- Demonstrates scalability and robustness even with large numbers of outliers.<br>- Highlights practical and theoretical advances that are adaptable to high-dimensional biological datasets like SNP beta/Z-score matrices. | [PDF](papers/ICLR/2023_clustering/very/3911_near_optimal_coresets_for_robu.pdf) |
| Most Discriminative Stimuli for Functional Cell Type Clustering | The paper presents a novel clustering algorithm designed to identify functional cell types in the retina and visual cortex by optimizing stimuli (Most Discriminative Stimuli, MDS) that elicit distinct responses from specific neuron groups while suppressing others. The approach involves an EM-style algorithm that alternates between optimizing stimuli for each cluster and reassigning neurons to their most responsive stimuli. This clustering algorithm is validated across species (mouse, marmoset, macaque) and recording techniques. It shows faster and more efficient identification of functional types compared to conventional methods, saving about 20% of experimental time. The algorithm is robust across initializations and provides interpretable results through visual representations of the optimized stimuli. | - EM-style clustering algorithm alternating between optimization and reassignment.<br>- Softmax-based objective function to differentiate clusters.<br>- Application across species (mouse, marmoset, macaque) and recording techniques.<br>- Improvement over conventional methods with a 20% reduction in experimental time.<br>- Potential applicability to clustering SNPs using beta/Z-scores as signals. | [PDF](papers/ICLR/2024_clustering/very/1897_Most_discriminative_stimu.pdf) |
| Local Graph Clustering with Noisy Labels | The paper introduces a novel local graph clustering method using noisy node labels. By constructing a weighted graph where edges between nodes with differing labels are penalized, the method improves clustering accuracy significantly compared to traditional diffusion-based approaches. Theoretical analysis provides conditions under which this method is effective, and empirical tests on real-world and synthetic datasets demonstrate substantial performance improvements. | - Introduction of Label-based Flow Diffusion (LFD) method<br>- Use of weighted graphs with label-based edge weights to handle noise<br>- Theoretical analysis providing sufficient conditions for effective clustering<br>- Empirical evaluation showing F1 score improvements up to 13%<br>- Adaptable framework applicable to SNP clustering tasks involving association measures | [PDF](papers/ICLR/2024_clustering/very/2084_Local_Graph_Clustering_wi.pdf) |
| Progressive Partial Optimal Transport for Deep Imbalanced Clustering | This paper presents a novel framework, Progressive Partial Optimal Transport (P2OT), for deep clustering on imbalanced datasets. The framework generates pseudo-labels progressively and incorporates an unbalanced optimal transport approach with matrix scaling algorithms for efficiency. A KL divergence constraint prevents degenerate solutions, and a progressive learning strategy allows for gradual incorporation of harder-to-cluster samples. P2OT achieves superior results on imbalanced datasets like CIFAR100, ImageNet-R, and iNaturalist2018 subsets, especially in medium and tail classes. Computational efficiency is improved, with P2OT being twice as fast as the Generalized Scaling Algorithm (GSA). | - P2OT framework addresses deep clustering on imbalanced datasets using pseudo-labeling.<br>- Formulates pseudo-label generation as a partial optimal transport problem.<br>- Incorporates a virtual cluster to absorb low-confidence samples, enhancing robustness.<br>- Uses a KL divergence constraint to avoid degenerate solutions.<br>- Gradual learning process governed by a progressively increasing parameter œÅ.<br>- Achieves state-of-the-art results on various imbalanced datasets.<br>- Computational efficiency improved by matrix scaling techniques. | [PDF](papers/ICLR/2024_clustering/very/3053_P_2_OT_Progressive_Partia.pdf) |
| Effective Pruning of Web-Scale Datasets Based on Complexity of Concept Clusters | This paper introduces a Density-Based Pruning (DBP) method for improving the efficiency and performance of machine learning models trained on large-scale datasets. DBP builds on previous pruning methods by clustering data embeddings with k-means, calculating complexity scores for clusters based on inter- and intra-cluster distances, and retaining samples from the most complex clusters. The method significantly reduces training compute while maintaining or improving performance on benchmarks such as ImageNet. Additionally, the approach is shown to generalize across multiple evaluation tasks and demonstrates state-of-the-art performance on the DataComp Medium benchmark. | - Uses k-means clustering to group data embeddings.<br>- Introduces Density-Based Pruning (DBP) which retains samples from complex clusters.<br>- Calculates complexity based on inter-cluster and intra-cluster distances.<br>- Uses a quadratic programming solver to manage dataset constraints.<br>- Achieves improved performance with reduced training costs on large-scale datasets. | [PDF](papers/ICLR/2024_clustering/very/5051_Effective_pruning_of_web_.pdf) |
| A Differentially Private Clustering Algorithm for Well-Clustered Graphs | This paper develops a graph clustering method that satisfies (Œµ, Œ¥)-differential privacy in the edge domain. It focuses on well-clustered graphs, where each cluster has high internal connectivity and low external connectivity. The solution is found via an SDP encoding the cluster structure, whose stability and low sensitivity permit adding minimal Gaussian noise. The final partitioning step uses spectral embedding plus k-means, leading to near-optimal misclassification rates compared to non-private baselines. Empirical tests on synthetic block-model data confirm the algorithm‚Äôs efficacy. | - Addresses cluster recovery in well-clustered graphs under (Œµ, Œ¥)-differential privacy<br>- Employs a specialized SDP with strong-convexity to bound sensitivity for adding minimal Gaussian noise<br>- Achieves near-optimal misclassification ratio compared to non-private cluster recovery<br>- Proves pure (Œµ)-DP is unattainable with small error, justifying (Œµ, Œ¥)-DP relaxation<br>- Experimental results on stochastic block model show better performance than naive randomization approaches | [PDF](papers/ICLR/2024_clustering/very/7708_A_Differentially_Private_.pdf) |
| Explaining Kernel Clustering via Decision Trees | This paper extends recent progress in explainable k-means clustering to the kernel setting by proposing Kernel IMM. It constructs an axis-aligned decision tree that approximates kernel k-means partitions while remaining interpretable. Central to the approach is a surrogate feature map, ensuring threshold splits in the original input space. The authors provide theoretical guarantees on the "price of explainability," showing near-optimal kernel cluster quality can be preserved. They also present greedy refinements (Kernel ExKMC, Kernel Expand) to improve clustering accuracy, with the trade-off of deeper trees. Empirical results demonstrate the method‚Äôs effectiveness on synthetic and real datasets, often rivaling non-interpretable kernel k-means performance. | - Generalizes iterative mistake minimization (IMM) to kernel k-means<br>- Introduces surrogate features to preserve axis-aligned interpretability<br>- Offers O(k^2) or dimension-sensitive bounds for the kernel "price of explainability"<br>- Greedy expansions (Kernel ExKMC/Expand) further refine the partition at the cost of deeper trees<br>- Empirically validated on various datasets (e.g. half-moon patterns, Iris), demonstrating near-optimal kernel cluster accuracy | [PDF](papers/ICLR/2024_clustering/very/7781_Explaining_Kernel_Cluster.pdf) |
| BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders | The paper introduces BasisDeVAE, a variational autoencoder framework that models feature behaviors in derivative space to perform simultaneous dimensionality reduction and feature-level clustering. By modeling derivatives rather than raw outputs, the method enables easy enforcement of monotonic or transient feature behavior, leading to better interpretability and robustness. Applied to synthetic, brain imaging, and single-cell data, BasisDeVAE outperforms previous methods in reconstructing trajectories and clustering features into biologically meaningful groups. | - Introduces DeVAE and BasisDeVAE, VAEs with derivative-based decoders.<br>- Performs simultaneous dimensionality reduction and feature-level clustering.<br>- Allows enforcement of monotonicity and transient behaviors in a flexible way.<br>- Demonstrates superior performance in clustering noisy and real-world biological data.<br>- Supports interpretable modeling, critical for pathway analysis in causal inference.<br>- Scalable to datasets with thousands of features (e.g., SNPs or genes). | [PDF](papers/ICML/2021_clustering/very/danks21a.pdf) |
| Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time | This paper introduces new algorithms for hierarchical agglomerative clustering (HAC) that work in nearly-linear time for graphs with sparse edge structures. It presents exact and approximate algorithms for complete-linkage, WPGMA, and average-linkage clustering using modular neighbor-heap structures and dynamic graph orientations to minimize computational cost. Experiments show major speedups (20x to 70x) without compromising clustering quality compared to traditional HAC methods. | - First nearly-linear time exact HAC algorithms for complete-linkage, WPGMA, and average-linkage clustering on graphs.<br>- Dynamic neighbor-heap data structures and graph orientation techniques to maintain clustering efficiency.<br>- Empirical evaluation shows 20x‚Äì70x speedups compared to traditional HAC implementations.<br>- Approximate clustering variants allow tradeoffs between speed and strict merge accuracy.<br>- Demonstrates scalability to millions of points and billions of edges using commodity hardware.<br>- Highly modular framework, allowing adaptation to different graph types and linkage methods. | [PDF](papers/ICML/2021_clustering/very/dhulipala21a.pdf) |
| Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures | The paper investigates the problem of selecting the best clustering similarity index for evaluating clustering algorithms. It defines desirable theoretical properties like symmetry, monotonicity, and constant baseline, then systematically analyzes existing indices (e.g., Adjusted Rand, Mutual Information, Jaccard). The authors find that many popular indices are biased and propose alternatives like the Correlation Coefficient and a new Correlation Distance measure. They show that poor index choices can lead to degraded real-world system performance, and provide a practical guide for selecting appropriate validation indices. | - Defines important properties for clustering similarity indices: constant baseline, monotonicity, symmetry, etc.<br>- Demonstrates that popular indices like NMI are often biased and inconsistent.<br>- Introduces the Correlation Distance (based on arccosine of Pearson correlation) as a novel, theoretically sound distance measure.<br>- Provides a methodology to select appropriate validation indices based on application needs.<br>- Highlights the risks of poor validation in production systems with a real-world case study (news aggregator).<br>- Suggests that choosing a good similarity measure is crucial for reliable clustering, especially when no 'true' cluster labels exist.<br>- Gives recommendations on when to prefer metrics like Adjusted Rand, Correlation Coefficient, or Sokal & Sneath. | [PDF](papers/ICML/2021_clustering/very/gosgens21a.pdf) |
| A Scalable Deterministic Global Optimization Algorithm for Clustering Problems | The paper proposes a scalable, deterministic global optimization method for minimum sum-of-squares clustering (MSSC). By formulating MSSC as a two-stage mixed-integer second-order cone programming (MISOCP) problem, the authors develop a reduced-space branch-and-bound (BB) algorithm that branches only on cluster centers, uses efficient closed-form bounds, and supports massive parallelism. Their method scales to datasets with up to 200,000 samples and guarantees convergence to a global Œµ-optimal solution. The approach significantly outperforms traditional branch-and-bound and heuristic methods like k-means, providing both strong performance and theoretical guarantees. | - Clustering formulated as a Mixed Integer Second Order Cone Programming (MISOCP) problem.<br>- Reduced-space branch-and-bound algorithm that only branches on cluster centers, independent of data sample size.<br>- Efficient lower and upper bounding methods using closed-form solutions, scenario-based sample grouping, and Lagrangian decomposition.<br>- Proves global convergence to Œµ-optimal solutions with formal convergence guarantees.<br>- Massively scalable: up to 200,000 samples solvable with parallelism.<br>- Strong practical performance on both synthetic and real-world datasets, compared against k-means and CPLEX.<br>- Open-source implementation available in Julia for reproducibility and future extension. | [PDF](papers/ICML/2021_clustering/very/hua21a.pdf) |
| Local Correlation Clustering with Asymmetric Classification Errors | This paper tackles a variant of the Correlation Clustering problem where objects are grouped based on noisy pairwise similarity information. It proposes minimizing the ‚Ñì‚Çö norm of the disagreement vector, focusing on reducing errors locally at each node rather than only globally. They model asymmetric classification errors, where positive and negative relationships have different weight ranges, and design a convex relaxation combined with a novel metric-space partitioning algorithm. Their method achieves strong polynomial-time approximation guarantees independent of the dataset size, providing robust clustering in noisy settings. | - Models clustering under noisy binary similarity/dissimilarity labels with variable edge weights.<br>- Focuses on minimizing ‚Ñì‚Çö norms (especially ‚Ñì‚ÇÇ and ‚Ñì‚àû) of local disagreement vectors rather than global sums.<br>- Designs a new metric-space partitioning algorithm supporting strong worst-case guarantees.<br>- Provides polynomial-time algorithms with approximation bounds dependent only on the noise parameter Œ±.<br>- Convex relaxation formulation that leads to practical and scalable rounding algorithms.<br>- Handles asymmetric errors naturally, matching real-world imbalance between true associations and false positives/negatives.<br>- Relevant for clustering SNPs based on beta/Z-scores while accounting for different confidence levels across SNPs. | [PDF](papers/ICML/2021_clustering/very/jafarov21a.pdf) |
| Feature Clustering for Support Identification in Extreme Regions | The paper develops MEXICO (Multivariate EXtreme Informative Clustering by Optimization), a novel method for clustering features that tend to be jointly large during extreme events. Working within the framework of multivariate Extreme Value Theory, they model feature clustering as an optimization problem over the probability simplex, promoting sparsity and disjointness among feature groups. Their method focuses on the extreme samples only, uses projected gradient ascent with regularization, and provides non-asymptotic convergence guarantees. MEXICO outperforms spectral clustering and spherical k-means on simulated extremes and improves anomaly detection performance on real-world datasets. | - MEXICO: A sparse optimization-based clustering method focused on extreme data points.<br>- Uses empirical risk minimization with a loss function designed for extreme regions.<br>- Optimization over probability simplex to enforce sparsity and mixture separation.<br>- Projected gradient ascent with regularization to produce disjoint clusters.<br>- Focuses on subspaces where extreme events are concentrated, aligning well with SNPs with large beta/Z-scores.<br>- Provides theoretical convergence guarantees (non-asymptotic excess risk bounds).<br>- Outperforms spectral clustering and spherical k-means in extreme feature clustering tasks.<br>- Demonstrates strong anomaly detection capabilities using the learned extreme structure. | [PDF](papers/ICML/2021_clustering/very/jalalzai21a.pdf) |
| ACE: Explaining Cluster from an Adversarial Perspective | This paper introduces ACE, a deep learning framework that integrates low-dimensional embedding, clustering, and cluster explanation into a single optimization process. ACE neuralizes the clustering step, making cluster assignments differentiable, and uses adversarial perturbations to find minimal explanatory feature sets. Applied to single-cell RNA-seq data, ACE outperforms baseline methods in finding highly discriminative, non-redundant gene panels even under noisy conditions. It generalizes to other domains such as image recognition, suggesting its wide applicability for interpretable clustering tasks. | - Proposes ACE: an integrated deep learning framework for embedding, clustering, and explaining data.<br>- Neuralizes clustering to make it differentiable, enabling attribution of input features to cluster membership.<br>- Uses adversarial perturbations to find minimal sets of important features (genes) for each cluster.<br>- Handles batch effects and technical noise through autoencoder-based embedding (e.g., using SAUCIE).<br>- Outperforms traditional methods (e.g., DESeq2, SHAP, SmoothGrad) in finding compact, non-redundant explanatory panels.<br>- Successfully applied to both simulated and real biological datasets, and generalized to MNIST image data.<br>- Strong candidate method for building interpretable, noise-resistant SNP clustering algorithms. | [PDF](papers/ICML/2021_clustering/very/lu21e.pdf) |
| Local Algorithms for Finding Densely Connected Clusters | This paper introduces two local graph clustering algorithms ‚Äî LocBipartDC for undirected graphs and EvoCutDirected for directed graphs ‚Äî designed to find pairs of densely connected clusters. The authors develop a novel double cover reduction, a simplify operator to ensure disjoint sets, and use PageRank and evolving set processes for efficient local computation. Their methods recover meaningful structures in real-world networks like U.S. migration data and international disputes, outperforming previous techniques in both accuracy and efficiency. | - Introduces LocBipartDC (for undirected graphs) and EvoCutDirected (for directed graphs).<br>- Uses a double cover construction to model two-cluster problems as single-cluster problems.<br>- Develops a simplify operator to ensure clean cluster separation.<br>- Uses Personalized PageRank and evolving set processes for local computation.<br>- Provides theoretical bounds on cluster quality (bipartiteness, flow ratio).<br>- Demonstrates success on synthetic stochastic block models and real-world datasets (migration, conflict networks).<br>- Highlights methods for handling noise and large graph sizes efficiently.<br>- Public code available for reproducibility. | [PDF](papers/ICML/2021_clustering/very/macgregor21a.pdf) |
| Hierarchical Clustering of Data Streams: Scalable Algorithms and Approximation Guarantees | This paper introduces scalable, hyperplane-based hierarchical clustering (HC) algorithms that are robust to the order of incoming data. Two key methods are developed: Random Cut Tree (RCT) for L1 distances and Uniform Radial Random Hyperplane (URRH) for L2 distances. These algorithms achieve strong approximation guarantees for standard HC objectives like MW Revenue and Dasgupta Cost. Experiments on synthetic and real-world datasets show that the methods are competitive, particularly excelling in noisy data settings where they outperform traditional dynamic clustering methods like BIRCH, PERCH, and GRINCH. | - Introduces Random Cut Tree (RCT) and Uniform Radial Random Hyperplane (URRH) algorithms for dynamic hierarchical clustering.<br>- Focuses on optimizing HC quality metrics like MW Revenue, CKMM Revenue, Dasgupta Cost, and MW Cost.<br>- Sequential property ensures clustering results are order-invariant ‚Äî critical for dynamic datasets.<br>- Approximation guarantees: 0.9-approximation for CKMM Revenue, 2-approximation for MW Cost, etc.<br>- Experimental validation on real datasets (MNIST, ImageNet, etc.) and synthetic noisy datasets.<br>- RCT/URRH outperform traditional methods in noisy environments.<br>- Provides a general hyperplane-based HC framework adaptable to various metrics.<br>- Potential adaptation for SNP beta/Z-score space clustering. | [PDF](papers/ICML/2021_clustering/very/rajagopalan21a.pdf) |
| Individual Preference Stability for Clustering | This paper introduces Individual Preference (IP) Stability, a new criterion for clustering where each point, on average, should be closer to points in its own cluster than points in any other cluster. The paper proves that finding an IP-stable clustering is NP-hard in general but provides efficient algorithms for special cases such as data on the real line or tree metrics. It also proposes approximation algorithms for nearly IP-stable clusterings, especially when clusters are well-separated. Extensive experiments show how existing algorithms (e.g., k-means++) perform with respect to IP-stability. | - Introduces Individual Preference (IP) Stability as a new clustering goal.<br>- Proves NP-hardness for general IP-stable clustering problems.<br>- Efficient algorithms for exact IP-stable clustering on the real line and tree metrics.<br>- Approximation algorithms for finding near IP-stable clusterings in general metric spaces.<br>- Uses tree embeddings and modified single-linkage approaches.<br>- Provides extensive experiments comparing IP-stability of standard clustering algorithms.<br>- Suggests modifications to standard clustering methods to improve stability.<br>- Concepts can be adapted to measure and enforce cluster meaningfulness for SNP data. | [PDF](papers/ICML/2022_clustering/very/ahmadi22a.pdf) |
| Interactive Correlation Clustering with Existential Cluster Constraints | The paper introduces existential cluster constraints (ECCs) as a new form of feedback for clustering problems. Instead of traditional must-link and cannot-link constraints, users express their mental model by specifying desired cluster-level features. The authors propose an efficient inference algorithm combining a semidefinite programming (SDP) relaxation and trellis-based dynamic programming to find clusterings that satisfy these constraints. Empirical results on author disambiguation tasks show that ECCs drastically reduce the amount of feedback needed to achieve high-quality clustering compared to traditional methods. | - Introduces existential cluster constraints (ECCs) as a novel, efficient user feedback mechanism for clustering.<br>- Extends the correlation clustering framework to incorporate ECCs.<br>- Develops a scalable inference algorithm combining semidefinite programming (SDP) relaxation and sparse cluster trellis dynamic programming.<br>- Proves NP-hardness of satisfying all ECCs exactly, motivating approximate solutions.<br>- Empirical results show ECCs require roughly half as much feedback to achieve good clustering compared to must-link/cannot-link feedback.<br>- Trellis structure allows efficient optimization over exponentially many possible clusterings.<br>- Ideas can be adapted to clustering SNPs using biological or statistical features to guide noise-resilient clustering. | [PDF](papers/ICML/2022_clustering/very/angell22a.pdf) |
| Gradient Based Clustering | This paper proposes a general and efficient clustering framework based on applying gradient steps to update cluster centers. Unlike traditional k-means which recomputes centers exactly, this method uses a single gradient step, enabling support for a wide range of cost functions, including robust losses like the Huber loss. The authors prove convergence guarantees even under arbitrary initialization and demonstrate strong robustness to noise in empirical experiments. The framework unifies and extends traditional k-means, Bregman clustering, and introduces a scalable way to handle complex, noise-robust clustering tasks. | - Introduces a general clustering framework using gradient steps for center updates.<br>- Supports a wide range of cost functions including robust ones like the Huber loss.<br>- Proves strong convergence guarantees under arbitrary initialization.<br>- Adapts to both traditional and non-standard clustering loss functions (Bregman and non-Bregman).<br>- Demonstrates robustness to noisy data using MNIST and Iris datasets.<br>- Provides a computationally cheaper alternative to exact optimization-based clustering.<br>- Applicable for designing SNP clustering methods that need robustness to noisy association measures. | [PDF](papers/ICML/2022_clustering/very/armacki22a.pdf) |
| An iterative clustering algorithm for the Contextual Stochastic Block Model with optimality guarantees | The paper presents an iterative refinement clustering algorithm for the Contextual Stochastic Block Model (CSBM), where both network structure (edges) and side information (covariates) are used for community detection. The method alternates between estimating model parameters and updating cluster assignments based on a least-squares criterion, achieving fast convergence and statistical optimality. The authors extend theoretical guarantees to multi-community settings and demonstrate superior performance on synthetic and real datasets, including applications to signed graphs and mixed information settings. | - Proposes an iterative refinement clustering algorithm (IR-LS) for CSBM models.<br>- Incorporates graph structure and covariates naturally without arbitrary weighting.<br>- Achieves exact recovery and optimal convergence under certain noise and separation conditions.<br>- Demonstrates robustness to weak separation in graph or covariate data alone.<br>- Extends theoretical guarantees beyond two-community settings.<br>- Supports clustering signed graphs and heterogeneous information.<br>- Provides a scalable and faster alternative to spectral or SDP-based methods.<br>- The approach is adaptable to clustering SNPs based on association statistics and metadata. | [PDF](papers/ICML/2022_clustering/very/braun22a.pdf) |
| Understanding Doubly Stochastic Clustering | This paper analyzes the effect of projecting affinity matrices onto the space of doubly stochastic matrices under the ‚Ñì2 norm, a normalization step shown empirically to improve clustering performance. It provides necessary and sufficient theoretical conditions for the resulting matrix to exhibit ideal clustering properties: no false connections between clusters and good within-cluster connectivity. The analysis extends to subspace clustering models, showing how geometric separation and data distribution affect outcomes. Extensive experiments on synthetic and real-world datasets confirm the theoretical findings, demonstrating substantial improvements in clustering accuracy and noise reduction. | - Focuses on projecting affinity matrices onto the set of doubly stochastic matrices under the ‚Ñì2 norm.<br>- Provides necessary and sufficient conditions for eliminating false inter-cluster connections.<br>- Shows how to guarantee both no false connections and full within-cluster connectivity.<br>- Applies analysis to subspace clustering, modeling scenarios similar to grouped SNP effects.<br>- Introduces methods to tune sparsity/noise trade-offs via a scaling parameter (Œ∑).<br>- Demonstrates state-of-the-art empirical performance on noisy clustering tasks.<br>- Offers mathematical tools to adapt normalization strategies for SNP data denoising. | [PDF](papers/ICML/2022_clustering/very/ding22a.pdf) |
| The Complexity of k-Means Clustering when Little is Known | The paper explores the complexity of performing k-means clustering when much of the data is missing or irrelevant, focusing on sparse, incomplete datasets. It introduces new algorithms that use graph-theoretical properties (treewidth, local feedback edge number) to achieve tractable clustering. Three main algorithms are developed: for bounded-domain (integer) data using incidence treewidth, for real-valued data using primal treewidth, and for real-valued data using local feedback edge number of the incidence graph. The approach heavily uses parameterized complexity and dynamic programming over tree decompositions. | - Study of k-means clustering on incomplete data matrices.<br>- Introduction of incidence graph and primal graph representations for missing data.<br>- Use of treewidth and local feedback edge number as key parameters.<br>- Fixed-parameter tractable (FPT) algorithms for bounded-domain and real-valued data.<br>- Dynamic programming over tree decompositions to merge partial clustering solutions.<br>- Potential inspiration for handling sparse or noisy SNP effect matrices. | [PDF](papers/ICML/2022_clustering/very/ganian22a.pdf) |
| Simultaneous Graph Signal Clustering and Graph Learning | This paper proposes GRASCale, a method that simultaneously clusters graph signals and learns a separate graph structure for each cluster. It extends spectral clustering by incorporating both the pairwise similarities between signals and their smoothness over corresponding graphs. The optimization is performed using block coordinate descent with prox-linear updates and enhanced initialization through consensus clustering. Experiments on synthetic data and the MNIST dataset show that GRASCale outperforms previous methods like GLMM and K-Graphs in both clustering accuracy and graph learning quality, achieving better interpretability and robustness. | - Extends spectral clustering with graph signal smoothness regularization.<br>- Simultaneous clustering and learning of cluster-specific graphs (GRASCale).<br>- Efficient optimization using block coordinate descent and prox-linear updates.<br>- Consensus clustering for better initialization and avoiding poor local minima.<br>- Validated on synthetic data and MNIST digit clustering, achieving superior performance.<br>- Applicable for heterogeneous data where different clusters may have different underlying structures.<br>- Open-source code available: https://github.com/SPLab-aviyente/GRASCale | [PDF](papers/ICML/2022_clustering/very/karaaslanli22a.pdf) |
| A Tighter Analysis of Spectral Clustering, and Beyond | This paper studies classical spectral clustering, showing it works under weaker conditions than previously known. It also proposes using fewer eigenvectors than the number of clusters, leading to better performance in some settings. The concept of a 'meta-graph' is introduced to better represent cluster relations, and a stronger theoretical framework is developed to support these findings. Extensive empirical validation on synthetic and real-world datasets (BSDS, MNIST, USPS) demonstrates the practicality of the methods. | - Tighter theoretical guarantees for spectral clustering, requiring weaker assumptions.<br>- Proposal to use fewer than k eigenvectors for clustering, leading to better results under certain conditions.<br>- Introduction of meta-graphs to represent interactions between clusters.<br>- Empirical validation on synthetic and real-world datasets (BSDS, MNIST, USPS).<br>- Could inform dimension reduction, noise handling, and clustering strategies for SNP data. | [PDF](papers/ICML/2022_clustering/very/macgregor22a.pdf) |
| Sublinear-Time Clustering Oracle for Signed Graphs | This paper presents a local clustering oracle for signed graphs that allows rapid classification of nodes into clusters without reading the full graph. It uses lazy signed random walks to build sparse vector representations of nodes and introduces a novel pseudometric for clustering based on spectral properties of the signed Laplacian. The method preprocesses in sublinear time and can correctly classify most vertices. Empirical evaluations show strong performance on synthetic and real-world datasets. | - Introduction of a local clustering oracle for signed graphs with sublinear preprocessing and query times.<br>- Development of a new pseudometric for comparing node embeddings from signed random walks.<br>- Use of lazy signed random walks to explore graph neighborhoods efficiently.<br>- Novel theoretical analysis connecting random walks to spectral properties of signed Laplacians.<br>- Evaluation on synthetic data and real-world Wikipedia-based datasets.<br>- Provision of new datasets and open-source code for reproducibility.<br>- Potentially inspiring for SNP clustering by modeling association signs and detecting noise clusters. | [PDF](papers/ICML/2022_clustering/very/neumann22a.pdf) |
| Practical Nearly-Linear-Time Approximation Algorithms for Hybrid and Overlapping Graph Clustering | The paper introduces efficient algorithms for finding overlapping and hybrid clusters in large graphs, based on new extensions of classical conductance-based partitioning objectives. It proposes the Œµ-overlapping ratio-cut and Œª-hybrid ratio-cut formulations, designs scalable algorithms using s-t maximum flows and cut-matching games, and provides the first nearly-linear-time O(log n)-approximation algorithms for overlapping clustering. Extensive empirical evaluation shows the approach significantly outperforms existing methods on both synthetic and real-world networks. | - Introduces Œµ-Overlapping Ratio-Cut (Œµ-ORC) and Œª-Hybrid Ratio-Cut (Œª-HCUT) objectives for overlapping clustering.<br>- Designs HybridImprove and OverlapImprove algorithms based on s-t max-flow and cut-matching games.<br>- Provides nearly-linear-time O(log n)-approximation guarantees for overlapping clustering.<br>- Successfully applies the methods to synthetic (Overlapping Stochastic Block Models) and real-world social networks.<br>- Shows robustness to noise and the ability to separate overlapping communities, an important feature for SNP clustering. | [PDF](papers/ICML/2022_clustering/very/orecchia22a.pdf) |
| ClusterFuG: Clustering Fully Connected Graphs by Multicut | This paper introduces a method for efficiently clustering fully connected graphs, using a dense multicut approach based on feature vector inner products. By replacing explicit edge storage with on-demand similarity computation and using scalable greedy contraction algorithms over k-nearest neighbor graphs, they achieve significant improvements in memory, speed, and scalability. Their method infers the number of clusters automatically and can isolate noise by adjusting an affinity strength parameter. Applications to image segmentation and large-scale clustering (like ImageNet) show competitive or superior performance compared to classical methods like k-means. | - Dense multicut formulation using inner products of feature vectors<br>- Greedy Additive Edge Contraction (GAEC) adapted for dense graphs<br>- Efficient scaling using nearest-neighbor sparsification and incremental updates<br>- Affinity strength tuning (Œ±) to favor small or large clusters, useful for noise handling<br>- No need to predefine the number of clusters<br>- Open-source code available at https://github.com/aabbas90/cluster-fug<br>- Empirical validation on ImageNet clustering and Cityscapes panoptic segmentation<br>- Potential for extension to incorporate biological priors into clustering | [PDF](papers/ICML/2023_clustering/very/abbas23a.pdf) |
| K-SHAP: Policy Clustering Algorithm for Anonymous Multi-Agent State-Action Pairs | The paper introduces K-SHAP, a method for clustering anonymous observations into distinct agent strategies. It first learns a 'world policy' through imitation learning to predict actions from states. Then, it uses SHAP explanations to interpret individual decisions, and finally clusters these explanations via K-means to identify behaviorally coherent groups. Tested on synthetic market data and real NASDAQ data, K-SHAP outperforms baseline clustering algorithms by a large margin. The method proves effective for handling anonymous, noisy, and high-dimensional data. | - Uses a 'world policy' model to learn from anonymous observations (state-action pairs).<br>- Applies SHAP (Shapley Additive Explanations) to generate local explanations for each data point.<br>- Clusters the SHAP value space, not the raw feature space, leading to better cluster separation.<br>- Shows strong performance improvements over classical clustering methods like k-means and ClusterGAN.<br>- Demonstrates robustness in real-world financial data and synthetic multi-agent simulations.<br>- Relevant for clustering settings where data points are anonymized, noisy, or heterogeneous.<br>- Introduces a flexible three-phase framework (Imitation Learning ‚Üí SHAP Explanation ‚Üí Clustering).<br>- Provides a pathway to adapt feature attribution-based clustering for genomic or SNP data. | [PDF](papers/ICML/2023_clustering/very/coletta23a.pdf) |
| Fast Combinatorial Algorithms for Min Max Correlation Clustering | This paper develops fast combinatorial algorithms for Min-Max correlation clustering on complete graphs. By introducing a novel 'correlation metric' that measures the agreement between nodes' neighborhoods, they construct approximate clusterings that minimize the maximum number of disagreements across vertices. They achieve significant speedups (up to 10,000 nodes) compared to prior LP/SDP-based methods, while maintaining good clustering quality. Their techniques allow for scaling to large datasets and have practical applications in social network and biological data clustering. | - Introduces the 'correlation metric' based on local edge agreement in signed graphs.<br>- Focuses on Min-Max objective: minimizing the maximum disagreement across vertices.<br>- Provides exact and approximate (sampling-based) fast clustering algorithms.<br>- Scales to large graphs (tested on Facebook networks with up to 10,000 nodes).<br>- Clusters produced often recover 'ground truth' communities.<br>- Outperforms LP-based methods in speed without large losses in clustering quality.<br>- Potentially adaptable to SNP clustering based on beta/Z-score concordance. | [PDF](papers/ICML/2023_clustering/very/davies23a.pdf) |
| Topological Point Cloud Clustering | The paper introduces Topological Point Cloud Clustering (TPCC), a method that clusters data points based on their contribution to global topological features rather than local proximity. TPCC constructs a simplicial complex, computes Hodge-Laplacians, extracts eigenvectors associated with important topological structures (like loops and cavities), and uses subspace clustering. Each point is assigned a 'topological signature' summarizing its participation across dimensions, leading to a final clustering that is robust against noise. TPCC outperforms traditional clustering algorithms on synthetic and real-world datasets, and offers theoretical guarantees for synthetic topological structures. | - Introduces Topological Point Cloud Clustering (TPCC) synthesizing spectral clustering and TDA.<br>- Clusters based on contributions to global topological features (not just local distances).<br>- Uses Hodge-Laplacians and 0-eigenvectors to capture multi-scale structure.<br>- Robust to noise due to topological invariance.<br>- Shows strong performance on synthetic and real-world noisy datasets.<br>- Highlights theoretical guarantees for correct clustering in synthetic topological structures.<br>- Potential inspiration for noise-resistant feature extraction in non-spatial data like SNP association measures. | [PDF](papers/ICML/2023_clustering/very/grande23a.pdf) |
| High-dimensional Clustering onto Hamiltonian Cycle | This paper proposes HCHC, a framework for clustering and visualizing high-dimensional data by combining local and global structural learning and mapping clusters onto a Hamiltonian cycle. First, the GLDC deep clustering method learns probability distributions over clusters that preserve both local similarity and global separation. Then, using cluster similarities based on Pearson correlations, cluster anchors are arranged optimally on a circle using an approximate Hamiltonian cycle. Samples are positioned according to their cluster probabilities, allowing clear identification of clusters, inter-cluster similarities, and outliers. HCHC shows superior clustering accuracy and visualization quality on multiple real-world datasets. | - Introduces GLDC (Global-Local Deep Clustering) balancing local compactness and global separation.<br>- Learns soft cluster assignment probabilities, suitable for noisy data like SNPs.<br>- Detects and isolates outliers based on low cluster assignment probabilities.<br>- Uses Pearson correlation between clusters to define inter-cluster similarities.<br>- Maps clusters onto a circle via Hamiltonian cycle optimization for visualization.<br>- Outperforms traditional clustering and visualization methods (e.g., k-means, t-SNE, UMAP) on multiple datasets.<br>- Framework and methods are adaptable to association score (beta/Z-score) data for causal inference pipelines. | [PDF](papers/ICML/2023_clustering/very/huang23d.pdf) |
| Dink-Net: Neural Clustering on Large Graphs | This paper introduces Dink-Net, a scalable deep graph clustering method capable of clustering graphs with millions of nodes. It unifies representation learning and clustering optimization using a node discriminate module and a neural clustering module. Dink-Net optimizes cluster assignments with two adversarial losses: cluster dilation (pushing clusters apart) and cluster shrink (pulling nodes toward centers), enabling mini-batch training and excellent scalability. Experiments show it outperforms other methods on massive datasets like ogbn-papers100M. | - Proposes Dink-Net, a scalable deep graph clustering framework.<br>- Introduces cluster dilation loss and cluster shrink loss optimized adversarially.<br>- Unifies representation learning and clustering in an end-to-end fashion.<br>- Supports mini-batch training for handling extremely large graphs.<br>- Demonstrates strong performance on ogbn-papers100M (111M nodes).<br>- Could inspire methods for clustering SNPs by association strength while isolating noise clusters.<br>- Official GitHub: https://github.com/yueliu1999/Dink-Net | [PDF](papers/ICML/2023_clustering/very/liu23v.pdf) |
| Deep Clustering with Incomplete Noisy Pairwise Annotations: A Geometric Regularization Approach | The paper addresses the problem of clustering with incomplete and noisy pairwise annotations by proposing a geometric regularization approach for deep constrained clustering (DCC). It analyzes the identifiability properties of logistic loss in DCC, introduces a confusion matrix to model annotation noise, and proposes the VolMaxDCC method, which uses volume maximization to ensure robust and generalizable clustering. Extensive experiments on real-world datasets demonstrate that VolMaxDCC significantly outperforms existing methods, especially in noisy annotation settings. | - Formal identifiability results for standard and noisy DCC losses.<br>- Introduction of a confusion matrix model to account for noisy pairwise annotations.<br>- Proposed VolMaxDCC method: jointly optimizing data embeddings and a geometric regularizer.<br>- Volume maximization regularization to enforce cluster separability and robustness.<br>- Demonstrated strong generalization to unseen data, crucial for causal inference applications.<br>- Extensive experiments on datasets (STL-10, CIFAR-10, ImageNet-10) with both synthetic and real-world noisy annotations.<br>- Codebase available for reproducibility: github.com/ductri/VolMaxDCC. | [PDF](papers/ICML/2023_clustering/very/nguyen23d.pdf) |
| Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering | This paper presents DGCN, a graph-agnostic clustering framework that addresses the challenge of clustering nodes without prior knowledge of graph homophily. It reconstructs two auxiliary graphs (homophilic and heterophilic) from the original data, applies a mixed low- and high-frequency filter, and decouples the learned embeddings into separate attribute and structure spaces. DGCN achieves state-of-the-art clustering performance across both homophilic and heterophilic graphs. | - Proposes graph reconstruction to build separate homophilic and heterophilic graphs from raw data.<br>- Introduces a mixed graph filter that captures both low- and high-frequency information.<br>- Uses dual subspace embeddings to separate attribute and structure information.<br>- Outperforms state-of-the-art clustering methods across 11 benchmark datasets, especially in heterophilic settings.<br>- Demonstrates robustness in real-world, unsupervised graph clustering scenarios.<br>- Concepts generalize to any data where relationships (homophilic or heterophilic) are unknown ‚Äî highly applicable to SNP clustering with noisy beta/Z-scores.<br>- Code available for reference: github.com/Panern/DGCN. | [PDF](papers/ICML/2023_clustering/very/pan23b.pdf) |
| Optimal LP Rounding and Linear-Time Approximation Algorithms for Clustering Edge-Colored Hypergraphs | This paper improves algorithms for clustering edge-colored hypergraphs, aiming to minimize 'mistakes' where node groupings don't match hyperedge colors. The authors design an optimal LP rounding method and fast combinatorial algorithms with tight theoretical guarantees. Key contributions include a randomized LP rounding technique using thresholds, matching integrality gap proofs, and reductions to known problems like Vertex Cover. | - Focus on clustering multiway colored relationships (hypergraphs).<br>- Develops optimal LP rounding for clustering with tight integrality gaps.<br>- Introduces a linear-time combinatorial 2-approximation algorithm.<br>- Uses randomized thresholding for LP rounding to handle mistakes.<br>- Proves hardness results through reductions to Vertex Cover.<br>- Results are scalable and practically verified via experiments. | [PDF](papers/ICML/2023_clustering/very/veldt23a.pdf) |
| GC-Flow: A Graph-Based Flow Network for Effective Clustering | The paper proposes GC-Flow, a generative graph neural network that replaces GCN layers with normalizing flows to improve clustering quality. By modeling p(x|y)p(y) instead of p(y|x), GC-Flow structures the representation space into a Gaussian mixture, yielding well-separated clusters while maintaining strong classification performance. It integrates graph convolutions within the flow architecture and proposes a determinant lemma to efficiently handle graph-induced dependencies. Extensive experiments show superior clustering on benchmark datasets. | - Proposes GC-Flow, a normalizing flow-based GNN that enables clustering-focused node embeddings.<br>- Models p(x|y)p(y) to encourage Gaussian mixture latent spaces.<br>- Develops a determinant lemma to handle graph adjacency during density estimation.<br>- Shows substantial improvements in clustering quality (silhouette scores) across standard graph datasets.<br>- Suggests parameterizations of the adjacency matrix to further boost clustering performance.<br>- Experimental results include comparisons to contrastive learning methods and classic GNNs. | [PDF](papers/ICML/2023_clustering/very/wang23y.pdf) |
| Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data | The paper introduces iterative Likelihood Adjusted SDP (iLA-SDP), a clustering method designed for heterogeneous data where clusters may have different shapes or covariances. Building on semidefinite programming (SDP) ideas, iLA-SDP treats cluster labels as parameters, profiles out centroids, and iteratively updates both cluster labels and covariances to maximize observed data likelihood. The method achieves better stability and lower mis-clustering error than traditional methods like K-means and EM, especially under challenging conditions. It provides theoretical guarantees for exact recovery and performs strongly on real-world datasets. | - Introduces likelihood-adjusted SDP (LA-SDP) to handle heterogeneous clusters<br>- Profiles out centroids to avoid sensitivity to their configuration<br>- Alternates between SDP-based assignment estimation and covariance matrix updating<br>- Theoretically guarantees exact recovery under separability conditions<br>- Empirically outperforms K-means, EM, and traditional SDP on real-world datasets<br>- Robust to poor initializations and varying cluster shapes<br>- Suggests extensions for high-dimensional or large-scale data using Burer-Monteiro factorization | [PDF](papers/ICML/2023_clustering/very/zhuang23a.pdf) |
| Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering | The paper introduces the biharmonic distance, a new graph distance metric that captures global graph structure better than effective resistance, and proposes k-harmonic distances as generalizations. They develop new clustering algorithms based on these distances, including biharmonic k-means and biharmonic Girvan-Newman clustering. These methods are shown to outperform traditional spectral clustering approaches in detecting meaningful structure and sparse cuts, both theoretically and empirically, across a range of datasets. | - Introduces biharmonic distance as a graph metric sensitive to global structure<br>- Proposes biharmonic k-means and biharmonic Girvan-Newman clustering algorithms<br>- Develops k-harmonic distance for tunable clustering sensitivity<br>- Strong theoretical foundation: Foster-like theorems, sparsity and cuts<br>- Outperforms effective resistance and spectral clustering on multiple datasets<br>- Methods are scalable, noise-robust, and offer low-rank approximation options<br>- Direct relevance to modeling SNP associations as graphs and clustering meaningful groups | [PDF](papers/ICML/2024_clustering/very/black24a.pdf) |
| A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering | The paper develops a fast, near-linear time algorithm for graph clustering under a semi-random model where adversaries can add edges within clusters and delete edges between clusters. By combining matrix multiplicative weights optimization, geometric expansion properties, and probabilistic heavy vertex removal techniques, the algorithm finds balanced cuts with strong approximation guarantees. It significantly improves over prior methods that required heavy SDP solvers and provides practical approaches for clustering in noisy, semi-random graphs. Applications extend to balanced cuts, sparsest cuts, and hierarchical clustering under semi-random perturbations. | - Develops a near-linear time O(1)-approximation algorithm for graph clustering under semi-random models<br>- Uses matrix multiplicative weights and probabilistic oracles instead of heavy SDP solvers<br>- Leverages geometric expansion properties to maintain structure despite perturbations<br>- Handles adversarial edge additions/deletions naturally<br>- Applicable to balanced cut, sparsest cut, and hierarchical clustering objectives<br>- Strong theoretical robustness against semi-random noise<br>- High scalability, appropriate for large datasets like genomic SNP data<br>- Bridges the gap between worst-case theory and practical clustering needs | [PDF](papers/ICML/2024_clustering/very/cohen-addad24c.pdf) |
| A3S: A General Active Clustering Method with Pairwise Constraints | The paper introduces A3S (Adaptive Active Aggregation and Splitting), a scalable, general active clustering framework that improves clustering quality with minimal human queries. It first adaptively over-clusters data and then strategically merges or splits clusters based on expected gains in normalized mutual information (NMI), guided by a theoretical analysis. A3S demonstrates superior performance over traditional methods, requiring fewer queries and scaling effectively to large datasets, while robustly handling noise and outliers through strategic purity testing and transitive inference. | - Proposes A3S: a two-stage adaptive active clustering framework<br>- Uses information-theoretic NMI analysis to guide merging and splitting<br>- Efficient querying strategy based on expected NMI gain<br>- Robust outlier detection and handling through purity testing and splitting<br>- Scalable to massive datasets (up to 100k samples)<br>- Flexible to initial clustering algorithms and noise in the data<br>- Demonstrated superior performance over state-of-the-art active clustering baselines<br>- Directly relevant to SNP clustering based on effect size similarity and noise isolation | [PDF](papers/ICML/2024_clustering/very/deng24b.pdf) |
| LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering | The paper introduces LSEnet, a deep learning framework for graph clustering that does not require the number of clusters to be predefined. It formulates a differentiable structural information (DSI) objective to capture the graph's intrinsic structure and constructs a partitioning tree to cluster nodes. LSEnet leverages hyperbolic space (Lorentz model) to embed nodes and uses manifold-based graph convolutions to integrate node features. Experiments show that LSEnet outperforms many baselines on benchmark datasets in terms of clustering accuracy, scalability, and robustness without prior knowledge of cluster number. | - Differentiable Structural Information (DSI) minimizes graph uncertainty to reveal clusters without predefining K.<br>- LSEnet models graphs in hyperbolic (Lorentz) space, which captures hierarchical or tree-like structures better.<br>- Integrates node features with structure via manifold-based convolutional layers.<br>- Outperforms strong deep graph clustering baselines even without prior knowledge of cluster number.<br>- Efficient in terms of computation and scalable to large graphs.<br>- Theory-backed connection between structural entropy minimization and cluster conductance.<br>- Visualization shows clear separation of clusters in hyperbolic space.<br>- Potentially transferable methodology to cluster SNPs based on association measures (betas/Z-scores) and detect noise clusters. | [PDF](papers/ICML/2024_clustering/very/sun24g.pdf) |
| Interpretable Deep Clustering for Tabular Data | This paper introduces Interpretable Deep Clustering (IDC), a two-stage framework for clustering tabular data with strong interpretability. IDC first uses a Gating Network and Autoencoder to select informative features for each data point via self-supervised learning. Then, it predicts cluster assignments using a coding-rate-based loss function to encourage compact, well-separated clusters. It provides explanations at both the sample and cluster levels, enabling faithful, diverse, and generalizable interpretations. The method demonstrates state-of-the-art clustering and interpretability on synthetic, biomedical, text, and physics tabular datasets, and is robust to lack of domain-specific augmentations. | - Introduces self-supervised feature selection using sample-specific gates for sparse reconstruction.<br>- Uses a modified Maximum Coding Rate Reduction loss to enhance cluster compactness and separability.<br>- Provides interpretability both at sample-level and cluster-level by identifying informative features.<br>- Demonstrates strong performance across biomedical and general tabular datasets without relying on domain-specific augmentations.<br>- Proposes metrics for measuring interpretability (diversity, faithfulness, uniqueness, generalizability).<br>- Public code available on GitHub for replication and extension. | [PDF](papers/ICML/2024_clustering/very/svirsky24a.pdf) |
| MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations | This paper proposes MC-GTA, a model-based clustering method that accounts for metric autocorrelation in data (e.g., spatial, temporal). It uses Wasserstein-2 distances between local Gaussian Markov Random Field models, fitting a generalized multivariate semivariogram to model autocorrelation. Clustering is formulated as minimizing a hinge loss based on goodness-of-fit tests. MC-GTA achieves better clustering quality, speed, and robustness compared to prior methods such as TICC and STICC. | - Model-based clustering using Wasserstein-2 distance between GMRF models.<br>- Generalized semivariogram for multivariate metric autocorrelation.<br>- Clustering loss based on hinge loss from goodness-of-fit tests, avoiding unstable EM.<br>- Demonstrated superior performance on real and synthetic datasets.<br>- Efficient and robust optimization process suitable for large datasets.<br>- Public implementation available (https://github.com/Octopolugal/MC-GTA.git). | [PDF](papers/ICML/2024_clustering/very/wang24av.pdf) |
| CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age | CROCS is a supervised contrastive learning framework that learns attribute-specific prototypes to enable clustering and retrieval of cardiac signals based on multiple patient attributes like disease class, sex, and age. It employs a combination of hard and soft assignment strategies, encouraging signals to be close to prototypes sharing similar attributes. Additionally, CROCS arranges prototypes semantically using Hamming distances over attributes. The method outperforms state-of-the-art techniques in clustering accuracy and retrieval precision on ECG datasets and offers interpretable representations. | - Introduces CROCS, a supervised contrastive learning framework for clustering and retrieval.<br>- Learns clinical prototypes representing combinations of patient attributes (disease, sex, age).<br>- Employs hard and soft assignment strategies to map signals to prototypes.<br>- Incorporates a regularization mechanism for semantic arrangement of prototypes.<br>- Demonstrates state-of-the-art clustering and retrieval performance on ECG datasets.<br>- Prototype learning enables both clustering of unlabelled signals and attribute-specific retrieval.<br>- Framework is robust to partial/noisy labels and enhances interpretability.<br>- Techniques can inspire clustering strategies for noisy, partially informative SNP association data. | [PDF](papers/NIPS/2021_clustering/very/1691_crocs_clustering_and_retrieval.pdf) |
| Solving Soft Clustering Ensemble via k-Sparse Discrete Wasserstein Barycenter | This paper proposes solving the soft clustering ensemble problem by linking it to the k-sparse Discrete Wasserstein Barycenter (DWB) problem. The authors introduce efficient sampling-based algorithms with provable approximation guarantees to find a consensus clustering from multiple soft clusterings. Their approach significantly improves both computational efficiency and clustering quality. They also provide a detailed theoretical analysis proving that with enough input clusterings, the consensus solution converges to the ground truth. Their experiments demonstrate strong performance against classic ensemble methods across multiple datasets. | - Soft clustering ensemble modeled via k-sparse Discrete Wasserstein Barycenter (DWB).<br>- Efficient sampling-based approximation algorithms with provable guarantees.<br>- Use of Wasserstein distance ensures robustness to noise and interpretability.<br>- Consensus analysis showing convergence to ground truth with enough inputs.<br>- Handles soft (fuzzy) clustering outputs rather than hard assignments.<br>- Applicable to high-dimensional and noisy data.<br>- Demonstrated superior performance compared to graph partitioning and matrix-based consensus methods.<br>- Provides a geometric framework useful for understanding and adapting to SNP association data. | [PDF](papers/NIPS/2021_clustering/very/1914_solving_soft_clustering_ensemb.pdf) |
| Nearly-Tight and Oblivious Algorithms for Explainable Clustering | This paper presents randomized algorithms for producing explainable clusterings with minimal loss in clustering quality compared to optimal, non-explainable clusterings. The algorithms create threshold trees based only on cluster centers, ignoring individual data points. They achieve nearly optimal guarantees for k-medians and k-means objectives and show strong lower bounds on how much worse explainable clustering must be. The methods are simple, fast, oblivious to data points, and robust to overfitting. | - Randomized threshold cuts separate clusters based on centers, not data points<br>- Achieves O(log^2 k) approximation for k-medians and O(k log^2 k) for k-means<br>- Oblivious to the number of data points, fast O(dk log^2 k) runtime<br>- Strong theoretical lower bounds matching the upper bounds up to polylog factors<br>- Potential for robustness against noise due to randomization<br>- No direct mechanism for detecting noise clusters, would need extension | [PDF](papers/NIPS/2021_clustering/very/3303_nearly_tight_and_oblivious_alg.pdf) |
| Uniform Concentration Bounds toward a Unified Framework for Robust Clustering | This paper presents a robust, theoretically principled framework for center-based clustering using a Median-of-Means (MoM) estimation approach. It enhances robustness to outliers and noise without requiring restrictive assumptions on the data distribution or dimensions. The methods apply to a general class of distance measures (Bregman divergences) and achieve strong non-asymptotic guarantees on error rates, including O(n^-1/2) convergence. Empirical results show that the MoM-powered clustering methods outperform standard clustering methods, especially under high noise conditions. | - Introduces robust clustering based on Median-of-Means (MoM) estimation<br>- Achieves O(n^-1/2) convergence under minimal assumptions<br>- Applicable to various dissimilarity measures (Bregman divergences)<br>- Provides strong non-asymptotic statistical guarantees<br>- Empirical validation showing improved robustness over standard methods<br>- Handles outliers without assumptions on their distribution<br>- Directly useful for designing SNP clustering methods capable of isolating noise clusters | [PDF](papers/NIPS/2021_clustering/very/3728_uniform_concentration_bounds_t.pdf) |
| Fuzzy Clustering with Similarity Queries | This paper proposes a robust, semi-supervised framework for fuzzy clustering that minimizes the number of human (oracle) queries needed. By leveraging similarity queries instead of full membership labels, the authors develop algorithms that can efficiently and accurately approximate optimal fuzzy clusterings. The methods are designed to handle overlapping clusters, noisy oracle responses, and achieve strong theoretical guarantees. Experimental results confirm the practical effectiveness of their approach. | - Introduces fuzzy clustering with limited similarity queries<br>- Efficient two-phase and sequential algorithms with theoretical guarantees<br>- Handles noisy oracle responses through robust averaging techniques<br>- Supports overlapping cluster membership naturally<br>- Relevant to contexts with noisy data and partial cluster assignment, like SNPs<br>- Practical and scalable (sublinear in number of points)<br>- Experimental validation on real-world datasets | [PDF](papers/NIPS/2021_clustering/very/4091_fuzzy_clustering_with_similari.pdf) |
| Coresets for Clustering with Missing Values | This paper presents efficient algorithms for constructing coresets ‚Äî small, weighted subsets of data ‚Äî that approximate clustering objectives (such as k-means or k-median) even when the data contains missing values. Their method avoids the need for data imputation, uses importance sampling, and dynamically constructs coresets based on observed dimensions. They prove strong theoretical guarantees and demonstrate empirical improvements over baselines, achieving accurate clustering with major speedups on large, real-world datasets. | - First coreset construction for clustering with multiple missing values<br>- Near-linear time algorithm based on importance sampling and dynamic Gonzalez‚Äôs algorithm<br>- Avoids imputation; computes distances only over observed dimensions<br>- Robust to missing data without strong assumptions<br>- Significant speedup (5x or more) over full-data clustering<br>- Empirical validation across real and synthetic datasets<br>- Potentially adaptable to SNP-level clustering for causal inference | [PDF](papers/NIPS/2021_clustering/very/5096_coresets_for_clustering_with_m.pdf) |
| Deep Conditional Gaussian Mixture Model for Constrained Clustering | This paper introduces DC-GMM, a deep generative model for constrained clustering that integrates domain knowledge in the form of probabilistic pairwise constraints. Using a VAE framework with a Gaussian Mixture Model prior, DC-GMM modifies the traditional ELBO objective to incorporate constraint information, allowing for soft, uncertain relationships between data points. The method achieves state-of-the-art clustering performance across image, text, and real-world medical datasets, and shows strong robustness to noisy constraint data. Its generative structure allows for not only clustering but also data generation and uncertainty estimation. | - Proposes Deep Conditional Gaussian Mixture Model (DC-GMM) for constrained clustering.<br>- Based on Variational Autoencoder (VAE) with Gaussian Mixture Model prior.<br>- Incorporates soft, probabilistic pairwise constraints into the clustering process.<br>- Introduces the Conditional ELBO objective for efficient training.<br>- Demonstrates robustness to noisy or inconsistent constraints.<br>- Achieves state-of-the-art clustering results on diverse benchmark datasets.<br>- Supports clustering control by choosing different constraint sets.<br>- Probabilistic and generative: enables uncertainty quantification and sample generation. | [PDF](papers/NIPS/2021_clustering/very/671_deep_conditional_gaussian_mixt.pdf) |
| Active Clustering for Labeling Training Data | This paper develops active clustering algorithms that use only pairwise similarity queries to reconstruct a partition of a dataset, minimizing human labeling effort. It introduces optimal querying strategies (chordal algorithms) under a uniform partition model and proposes efficient clique-based strategies under fixed-class settings. The authors also handle noisy query answers with robust correction methods and provide theoretical complexity guarantees. The work offers practical guidelines for when pairwise clustering is more efficient than direct labeling. | - Active clustering framework using pairwise same-class queries<br>- Optimal chordal algorithms minimize query complexity in uniform model<br>- Efficient clique-based strategies for fixed-number-of-classes settings<br>- Handles noisy similarity queries with robust error correction<br>- Query complexity scales nearly linearly with dataset size under reasonable assumptions<br>- Practical analysis for deciding between direct labeling vs pairwise querying<br>- Highly relevant for clustering SNPs based on noisy association signals | [PDF](papers/NIPS/2021_clustering/very/8993_active_clustering_for_labeling.pdf) |
| Multi-Facet Clustering Variational Autoencoders | This paper introduces MFCVAE, a probabilistic deep learning model capable of simultaneously learning multiple independent clusterings ('facets') from high-dimensional data in an unsupervised way. Each facet is modeled with a Mixture-of-Gaussians prior, and a ladder-like neural network architecture progressively disentangles these facets. They extend theoretical results for optimal posterior estimation (VaDE trick) to multi-facet settings and demonstrate that their method can capture multiple latent structures with stability, compositionality, and diversity across different datasets like MNIST, 3DShapes, and SVHN. | - Proposes Multi-Facet Clustering VAEs with separate Mixture-of-Gaussians priors for different facets<br>- Fully unsupervised, probabilistic model trained end-to-end<br>- Introduces a progressive training schedule and ladder architecture to disentangle facets<br>- Extends the VaDE trick for optimal ELBO optimization without sampling bias<br>- Achieves strong empirical results in multi-facet clustering, compositionality, and diversity<br>- Highly relevant for clustering SNPs by multiple biological characteristics while handling noise<br>- Robust theoretical foundations and practical empirical validation | [PDF](papers/NIPS/2021_clustering/very/9110_multi_facet_clustering_variati.pdf) |
| MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering | This paper proposes MiCE, a clustering framework that combines contrastive learning with a latent mixture model to jointly learn data representations and cluster assignments. MiCE uses a gating function to partition the data among specialized 'experts' and applies an EM-like optimization procedure to maximize a single probabilistic objective. Empirical results show that MiCE outperforms other unsupervised clustering methods on several image datasets, and that jointly learning embeddings and clusters improves clustering quality significantly. | - Combines contrastive learning with probabilistic clustering under a unified objective.<br>- Employs a mixture of experts framework where each expert specializes in a subset of data.<br>- Uses a scalable EM algorithm for joint optimization of clustering and representation learning.<br>- Handles noise and cluster degeneracy through analytical updates and ELBO maximization.<br>- Shows significant improvement over baseline methods in unsupervised image clustering benchmarks. | [PDF](papers/ICLR/2021_clustering/likely/1026_mice_mixture_of_contrastive_ex.pdf) |
| Clustering-Friendly Representation Learning via Instance Discrimination and Feature Decorrelation | The paper introduces IDFD, a deep learning framework for learning clustering-friendly representations by combining instance discrimination and feature decorrelation. It uses a softmax-based decorrelation constraint to stabilize learning, and shows that proper temperature tuning allows the model to approximate the behavior of spectral clustering. Empirical results on image datasets like CIFAR-10 and ImageNet-10 show that IDFD outperforms traditional and recent deep clustering methods, achieving stable and high-quality clustering performance. | - Proposes a novel softmax-formulated feature decorrelation loss to promote independent latent features.<br>- Uses instance discrimination to learn similarity-preserving embeddings without labels.<br>- Connects the method theoretically to spectral clustering behavior under certain conditions.<br>- Demonstrates that temperature tuning is critical for achieving clustering-friendly embeddings.<br>- Shows strong empirical performance across multiple deep network architectures and datasets. | [PDF](papers/ICLR/2021_clustering/likely/1734_clustering_friendly_representa.pdf) |
| Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition | The paper introduces Deep Repulsive Clustering (DRC), a method that clusters ordered data (such as images with an associated rank) by separating out identity-related and order-related features using an Order-Identity Decomposition (ORID) network. Clusters are formed based on identity features with a repulsive term to maximize separation, and ranks are predicted using a MAP approach within clusters. The method shows superior performance in tasks like facial age estimation and aesthetic score prediction. | - Development of ORID network to separate order-related and identity-related features.<br>- Use of repulsive term in clustering to increase inter-cluster distance and avoid noisy mixtures.<br>- Application to ordered tasks like age estimation, aesthetic score prediction, and historical image classification.<br>- Proposes a MAP-based decision rule for rank estimation within clusters.<br>- Highlights practical issues around noise, bias, and ethical implications of clustering ordered human data. | [PDF](papers/ICLR/2021_clustering/likely/516_deep_repulsive_clustering_of_o.pdf) |
| Evolutionary Diversity Optimization with Clustering-Based Selection for Reinforcement Learning (EDO-CS) | This paper introduces EDO-CS, an evolutionary algorithm designed to find a set of policies that are both high-quality and behaviorally diverse in reinforcement learning. EDO-CS clusters candidate policies by their behavioral features and selects the best-performing policy from each cluster to encourage exploration across diverse strategies while maintaining high rewards. It uses evolution strategies for policy updates and a multi-armed bandit model to dynamically adjust the balance between quality and diversity. EDO-CS outperforms previous quality-diversity methods across deceptive, multi-modal, and standard reinforcement learning tasks. | - Proposes clustering-based selection to ensure behavioral diversity while maximizing quality<br>- Uses evolution strategies (ES) for efficient optimization of policy parameters<br>- Introduces adaptive balancing of quality vs. diversity using multi-armed bandits<br>- Manages an archive to maintain diverse, high-quality solutions<br>- Demonstrates superior performance across a range of RL environments<br>- Addresses trade-offs between exploration and exploitation explicitly and dynamically<br>- Very scalable and efficient even for high-dimensional problems<br>- Behavior-space clustering ideas can transfer to SNP effect clustering | [PDF](papers/ICLR/2022_clustering/likely/2401_evolutionary_diversity_optimiz.pdf) |
| Contrastive Fine-Grained Class Clustering via Generative Adversarial Networks (C3-GAN) | This paper proposes C3-GAN, a method for unsupervised fine-grained class clustering by combining contrastive learning with generative adversarial networks. C3-GAN creates an embedding space where clusters are explicitly separable by treating latent codes as cluster centroids and optimizing contrastive loss between real/generated images and their latent codes. It also decomposes scenes into background and foreground components without needing manual labels. Experiments on four fine-grained datasets show that C3-GAN achieves state-of-the-art clustering accuracy and normalized mutual information, while also alleviating GAN mode collapse problems. | - Introduces contrastive loss for clustering-friendly embedding learning<br>- Latent codes act as cluster centroids during GAN training<br>- Scene decomposition (foreground vs background) without manual labels<br>- State-of-the-art clustering results on fine-grained image datasets<br>- Combines adversarial and contrastive learning objectives<br>- Improves robustness against mode collapse in GANs<br>- Concepts could inspire contrastive loss designs for SNP beta/Z-score clustering | [PDF](papers/ICLR/2022_clustering/likely/2694_contrastive_fine_grained_class.pdf) |
| On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation | This paper analyzes the reasons behind the success of MAUVE, a text generation evaluation metric. It shows that MAUVE's success mainly stems from comparing clustered embeddings of text samples instead of comparing raw strings directly. By clustering based on language model embeddings, the method focuses on coherence and syntax rather than superficial features, resulting in better correlation with human judgments. The paper proposes that simpler divergences applied to clustered embeddings can work even better than MAUVE's complex divergence. It also probes cluster sensitivity to different linguistic features, revealing robustness to noise and sensitivity to coherence-level changes. | - Embedding-based clustering improves evaluation quality compared to raw data comparisons.<br>- Cluster-based methods are more sensitive to meaningful (coherence/syntax) than superficial (word/article) changes.<br>- Classical divergences (e.g., KL, JS) work well when applied on clusters.<br>- Perturbation studies help identify robustness and noise susceptibility of clustering methods.<br>- PCA and K-means are effective techniques to cluster high-dimensional embeddings.<br>- Bias-variance trade-offs need careful analysis when switching from fine-grained to clustered distributions. | [PDF](papers/ICLR/2023_clustering/likely/2641_on_the_usefulness_of_embedding.pdf) |
| Statistical Guarantees for Consensus Clustering | This paper tackles the problem of combining multiple different clusterings of the same data (where labels might be permuted) into a robust consensus clustering. It proposes lifting the clustering to the association matrix space to avoid label-switching problems and applies aggregation techniques like K-means and spectral clustering. A random perturbation model is introduced to formally analyze noise, and a local refinement step is shown to achieve nearly optimal error rates. Empirical experiments validate that their methods outperform existing approaches, especially under noisy or unbalanced conditions. | - Lifting clustering to association matrices to avoid label-switching issues.<br>- Basic and spectral aggregation algorithms with statistical consistency proofs.<br>- Random Perturbation Model (RPM) to model noisy inputs.<br>- Local refinement algorithm to boost clustering quality toward optimal rates.<br>- Theoretical guarantees: misclassification rates decay exponentially with the number of input clusterings.<br>- Extensive experiments showing improvements over traditional consensus clustering methods. | [PDF](papers/ICLR/2023_clustering/likely/2988_statistical_guarantees_for_con.pdf) |
| Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders | This paper introduces CMVAE, a multimodal VAE that enforces a clustering structure in the latent space by using a mixture-based prior. It includes a post-hoc method to automatically select the optimal number of clusters via an entropy-based criterion. The authors also incorporate diffusion probabilistic models to refine generated samples, leading to sharper and more realistic outputs. Experiments on image-text data show improved clustering accuracy, generative quality, and flexibility in handling missing modalities compared to earlier multimodal VAEs. | - Proposes CMVAE with a mixture-based latent prior to encode clusters<br>- Introduces a post-hoc method to prune redundant clusters using entropy<br>- Incorporates diffusion models (DDPMs) to sharpen sample quality<br>- Demonstrates improved multimodal generative performance and missing-modality handling<br>- Highlights a weakly-supervised approach, leveraging multiple data views | [PDF](papers/ICLR/2024_clustering/likely/5416_Deep_Generative_Clusterin.pdf) |
| Image Clustering via the Principle of Rate Reduction in the Age of Pre-Trained Models | This paper proposes a pipeline called CPP, integrating powerful pre-trained models (e.g. CLIP) with a rate-reduction-based clustering objective (Manifold Linearizing and Clustering, or MLC). The approach handles large, unlabeled image datasets, infers the optimal number of clusters post-hoc using a coding-length criterion, and leverages multimodal capabilities to automatically label clusters. The method attains state-of-the-art accuracy on challenging benchmarks and effectively scales to millions of samples. | - Employs a rate-reduction approach (MLC) to build orthogonal low-dimensional subspaces for each cluster.<br>- Uses high-quality, pre-trained image embeddings (CLIP) to enable efficient and scalable clustering.<br>- Provides a post-hoc method to select the optimal number of clusters (avoiding multiple re-trainings).<br>- Automates cluster labeling through vision‚Äìlanguage alignment for generating descriptive captions.<br>- Achieves state-of-the-art performance on standard benchmarks (CIFAR, ImageNet) and more eclectic image collections. | [PDF](papers/ICLR/2024_clustering/likely/5846_Image_Clustering_via_the_.pdf) |
| Correlation Clustering in Constantly Many Parallel Rounds | The paper proposes a new parallel algorithm for correlation clustering that works in a constant number of rounds under the MPC model. It introduces a local notion of 'agreement' between nodes to trim weak edges and builds dense clusters by finding connected components in the trimmed graph. The method achieves a constant-factor approximation to the optimal clustering and is empirically shown to be much faster and comparably accurate compared to prior methods like Parallel Pivot and ClusterWild on massive datasets such as DBLP, UK-2005, and Twitter-2010. | - Introduces a constant-round MPC algorithm for correlation clustering based on local agreement trimming.<br>- Defines new measures of weak and strong node agreement to guide clustering decisions.<br>- Uses dense connected components of a trimmed graph to define clusters efficiently.<br>- Theoretical guarantee: constant-factor approximation to optimal correlation clustering cost.<br>- Empirical validation on massive graphs shows faster runtime and better clustering quality than prior methods.<br>- Highlights scalability challenges and opportunities in clustering noisy, large-scale data. | [PDF](papers/ICML/2021_clustering/likely/cohen-addad21b.pdf) |
| On Variational Inference in Biclustering Models | The paper studies parameter estimation in biclustering models using variational inference (VI). It develops tight theoretical error bounds showing that VI estimators achieve minimax optimal rates under mild assumptions. A coordinate ascent variational inference (CAVI) algorithm is proposed and analyzed, with proofs of local and partial global convergence. Empirical experiments validate the theoretical findings in Bernoulli and Poisson biclustering models. | - Establishes minimax-optimal estimation bounds for variational inference in biclustering models.<br>- Introduces and analyzes the CAVI algorithm with local and partial global convergence guarantees.<br>- Addresses partial observation settings, providing robustness to missing data.<br>- Demonstrates that VI can produce accurate cluster assignments under mild model assumptions.<br>- Highlights sensitivity of convergence to initialization and model misspecification.<br>- Provides detailed empirical validation on Bernoulli and Poisson synthetic datasets. | [PDF](papers/ICML/2021_clustering/likely/fang21b.pdf) |
| On the Price of Explainability for Some Clustering Problems | The paper studies the tradeoff between clustering quality and explainability, defining the 'price of explainability' as the unavoidable loss in objective function quality when clusters must be explainable via simple decision trees. They analyze this tradeoff for k-means, k-medians, k-centers, and maximum-spacing objectives, providing new upper and lower bounds. They also introduce Ex-Greedy, a new efficient greedy algorithm for constructing explainable k-means clusterings, showing that it performs better than prior methods across multiple datasets. Their work highlights that the cost of explainability is often small, especially in low dimensions. | - Defines and analyzes the 'price of explainability' for several clustering objectives (k-means, k-medians, k-centers, maximum-spacing).<br>- Proposes Ex-Greedy, an efficient greedy algorithm for explainable k-means clustering.<br>- Provides tighter upper and lower bounds on clustering quality loss when enforcing explainability.<br>- Demonstrates empirically that Ex-Greedy often outperforms previous explainable clustering methods.<br>- Highlights that in low dimensions, the cost of requiring explainability is often quite small.<br>- Explores clustering structures based on decision-tree partitions, leading to interpretable cluster definitions.<br>- Offers a framework that could be adapted to interpretable SNP clustering using association statistics. | [PDF](papers/ICML/2021_clustering/likely/laber21a.pdf) |
| Learning of Cluster-based Feature Importance for Electronic Health Record Time-series | This paper presents CAMELOT, a supervised deep learning model that clusters EHR time-series data while addressing challenges like class imbalance and cluster collapse. It introduces a feature-time attention mechanism to enhance interpretability and two novel loss functions to balance cluster use and promote diversity. The model outperforms benchmarks like Time-Series K-Means, SOM-VAE, and AC-TPC in clustering separability and outcome prediction tasks, and provides clear, interpretable mappings of which features and timepoints drive cluster assignment. | - Proposes CAMELOT: a supervised deep learning model for clustering EHR time-series data.<br>- Introduces feature-time attention mechanisms for cluster interpretability.<br>- Develops loss functions to prevent cluster collapse and address class imbalance.<br>- Benchmarked against TSKM, SOM-VAE, and AC-TPC, achieving better cluster separability and prediction accuracy.<br>- Potentially adaptable to static SNP data by modifying the feature attention framework and adjusting loss functions for continuous association scores instead of categorical outcomes.<br>- GitHub repository available: https://github.com/hrna-ox/camelot-icml | [PDF](papers/ICML/2022_clustering/likely/aguiar22a.pdf) |
| Fair and Fast k-Center Clustering for Data Summarization | This paper introduces a new clustering algorithm that simultaneously addresses two fairness goals: ensuring fair demographic representation across clusters and preventing distorted cluster sizes where some clusters represent many more data points than others. They define the Private Representative k-Center (PRIV-REP-KC) problem, propose a backbone-and-realization framework, and develop a fast algorithm with a 15-approximation guarantee and near-linear runtime. Extensive experiments on real-world datasets show their method leads to fairer, more balanced, and efficient clustering outcomes. | - Defines Private Representative k-Center (PRIV-REP-KC) combining demographic fairness and minimum cluster size constraints.<br>- Proposes a fast backbone-based clustering framework scalable to large datasets (O(nk¬≤ + k‚Åµ) runtime).<br>- Introduces flow-based optimization for enforcing private clustering efficiently.<br>- Achieves a 15-approximation factor with strong empirical results.<br>- Highlights benefits of handling multiple fairness aspects simultaneously rather than sequentially.<br>- Shows improved balance and fairness in clustering across multiple datasets (Adult, Diabetes, Electric, Query datasets).<br>- Ideas of backbone stability and minimum cluster size constraints are adaptable to SNP clustering with noise isolation goals. | [PDF](papers/ICML/2022_clustering/likely/angelidakis22a.pdf) |
| Online and Consistent Correlation Clustering | This paper proposes the AGREE-ON algorithm for online correlation clustering, where points (nodes) arrive sequentially and clustering must be updated dynamically. The method clusters a signed graph, aiming to minimize both the clustering cost and the number of changes (recourse) to previous assignments. AGREE-ON reuses a static clustering subroutine (Agreement) and applies lazy reassignment strategies, achieving provable logarithmic recourse per node and constant-factor approximation to optimal clustering. Experimental results on real-world datasets demonstrate superior stability and clustering quality compared to prior methods. | - Introduces AGREE-ON: a dynamic, stable online clustering algorithm for signed graphs.<br>- Applies lazy cluster reassignments to minimize recourse while maintaining solution quality.<br>- Uses Œµ-agreement to define similarity robustly, even with noisy or evolving data.<br>- Proves logarithmic recourse per node and constant factor approximation bounds.<br>- Demonstrates empirical superiority over baseline methods like Pivot.<br>- Framework is adaptable to clustering SNPs based on noisy association measures.<br>- Highlights trade-offs between cluster quality and stability, highly relevant for causal inference applications. | [PDF](papers/ICML/2022_clustering/likely/cohen-addad22a.pdf) |
| Global Optimization of K-Center Clustering | This paper presents a global optimization approach to the k-center clustering problem, introducing a reduced-space branch-and-bound algorithm that guarantees convergence to a global optimum. It applies feasibility-based bounds tightening techniques to efficiently prune the search space, making the method scalable to datasets with millions of samples. Extensive empirical results demonstrate that the method significantly outperforms heuristic clustering approaches and commercial solvers like CPLEX, achieving very small optimality gaps within reasonable computational times. | - Introduces a reduced-space branch-and-bound algorithm for global k-center clustering.<br>- Designs feasibility-based bounds tightening (FBBT) techniques to prune search regions and accelerate convergence.<br>- Proves finite-step convergence to exact global optima.<br>- Demonstrates scalability to datasets with over 14 million samples.<br>- Open-source Julia implementation available.<br>- Highlights the weaknesses of heuristic clustering (e.g., farthest-first) compared to globally optimal solutions.<br>- Methods could inform SNP clustering if adapted to biologically relevant distance metrics. | [PDF](papers/ICML/2022_clustering/likely/shi22b.pdf) |
| Deep Safe Incomplete Multi-view Clustering: Theorem and Algorithm | The paper presents DSIMVC, a framework for clustering incomplete multi-view data without degrading performance compared to clustering on only complete data. It introduces a bi-level optimization where missing views are imputed from learned semantic neighbors, and incomplete samples are weighted to reduce the impact of poor imputations. The framework offers theoretical guarantees on performance and shows superior experimental results across multiple datasets, even at high missingness rates. | - Proposes Deep Safe Incomplete Multi-View Clustering (DSIMVC) with theoretical safety guarantees.<br>- Dynamic neighbor mining based on learned feature representations.<br>- Automatic weighting of incomplete samples to minimize the risk of performance degradation.<br>- Bi-level optimization framework combining safe learning and semantic imputation.<br>- Empirically validated on multiple datasets like BDGP, MNIST-USPS, CCV, and Multi-Fashion.<br>- Could inspire robust clustering and noise isolation strategies for SNP association studies. | [PDF](papers/ICML/2022_clustering/likely/tang22c.pdf) |
| Correlation Clustering via Strong Triadic Closure Labeling: Fast Approximation Algorithms and Practical Lower Bounds | This paper proposes new fast and scalable approximation algorithms for correlation clustering, particularly for the cluster editing and cluster deletion variants. By drawing connections to strong triadic closure labeling problems, the authors develop a Match-Flip-Pivot (MFP) framework that avoids expensive linear programming relaxations. Their methods provide practical lower bounds, fast approximate solutions, and robust a posteriori approximation guarantees, all while scaling to graphs with millions of nodes. | - Introduces the Match-Flip-Pivot (MFP) algorithm for fast approximate correlation clustering.<br>- Develops strong triadic closure labeling connections to clustering with robustness guarantees.<br>- First purely combinatorial approximation algorithms for cluster deletion with 4-approximation guarantees.<br>- Enables scalable clustering on datasets with millions of nodes.<br>- Practical methods for generating a posteriori quality guarantees for clustering results.<br>- Highlights practical applications including biological network clustering, which could inspire SNP applications. | [PDF](papers/ICML/2022_clustering/likely/veldt22a.pdf) |
| Bregman Power k-Means for Clustering Exponential Family Data | This paper introduces Bregman Power k-Means, a scalable and robust clustering method that extends classical k-means to data generated from exponential family distributions by leveraging Bregman divergences and power mean annealing. The method achieves closed-form updates via majorization-minimization, avoids poor local minima, and delivers strong theoretical guarantees, including convergence and generalization bounds. Experiments on synthetic and real-world data demonstrate superior performance over standard k-means and existing Bregman clustering approaches. | - Introduces Bregman Power k-Means, combining Bregman divergences with power mean annealing.<br>- Provides closed-form updates using majorization-minimization (MM), making it efficient and scalable.<br>- Theoretical guarantees for generalization error and convergence without bounded support assumptions.<br>- Demonstrated superior performance across Gaussian, Poisson, Gamma, and Binomial data.<br>- Outperforms standard k-means and Bregman hard clustering in robustness and accuracy.<br>- Applicable to clustering problems where data do not naturally form spherical clusters, such as SNP association data. | [PDF](papers/ICML/2022_clustering/likely/vellal22a.pdf) |
| Optimal Clustering with Noisy Queries via Multi-Armed Bandit | This paper addresses the problem of clustering when the available pairwise information is noisy, proposing a new polynomial-time algorithm that uses multi-armed bandit techniques to efficiently recover clusters. The authors introduce a best-arm identification framework for assigning cluster labels with minimal noisy queries, achieving nearly optimal query complexity and proving tight lower bounds. The approach is particularly effective for large-scale clustering under uncertainty and noise. | - Proposes a multi-armed bandit based clustering algorithm under noisy oracle conditions.<br>- Achieves optimal query complexity up to polynomial factors: O(n(k+log n)/Œ¥¬≤).<br>- Proves a new lower bound of ‚Ñ¶(n log n/Œ¥¬≤) on query complexity.<br>- Introduces efficient recursive clustering strategies removing constant fractions of uncertain points per round.<br>- Establishes strong connections between clustering with noisy queries and stochastic block models (SBM).<br>- Could inspire confidence-driven SNP clustering strategies in the presence of noisy association signals. | [PDF](papers/ICML/2022_clustering/likely/xia22a.pdf) |
| Fast Algorithms for Distributed k-Clustering with Outliers | This paper introduces two fast sampling-based algorithms for distributed k-clustering with outliers: Inliers-Recalling Sampling and Space-Narrowing Sampling. These algorithms achieve constant-factor bi-criteria approximations (good clustering quality while allowing a small fraction of outliers), minimize communication costs between machines, and reduce local computation time to be linear in the data size. By removing dependence on difficult-to-estimate parameters like aspect ratio and using adaptive sampling, the methods show strong theoretical guarantees and practical improvements on large real-world datasets compared to previous algorithms. | - Proposes Inliers-Recalling Sampling for robust outlier handling without aspect ratio dependence.<br>- Proposes Space-Narrowing Sampling for faster two-round distributed clustering.<br>- Achieves bi-criteria constant-factor approximation guarantees for k-center, k-median, and k-means with outliers.<br>- Focuses on distributed settings, emphasizing communication and computational efficiency.<br>- Experiments show significant gains in speed and communication efficiency over previous methods.<br>- Strategies for adaptive sampling and outlier recovery could inspire robust SNP clustering algorithms. | [PDF](papers/ICML/2023_clustering/likely/huang23f.pdf) |
| On Coresets for Clustering in Small Dimensional Euclidean Spaces | The paper explores the construction of small coresets for the k-median clustering problem in small-dimensional Euclidean spaces. It provides nearly tight upper and lower bounds for coreset sizes, showing improved constructions for 1D (OÃÉ(Œµ‚Åª¬π/¬≤) size) and introducing discrepancy-based methods for higher dimensions. The authors present both theoretical improvements and practical constructions, highlighting how smaller coresets can accelerate clustering algorithms, reduce memory needs, and enable more efficient streaming and distributed computations. | - Develops small, efficient coresets for k-median clustering in small dimensions.<br>- Provides tight upper and lower bounds for coreset sizes in 1D and low dimensions.<br>- Introduces adaptive cumulative error partitioning for better coreset construction.<br>- Uses discrepancy theory and hierarchical decomposition for error management.<br>- Highlights applications in speeding up clustering algorithms, streaming, and distributed settings.<br>- Coreset construction could be adapted to SNP clustering to reduce computational cost. | [PDF](papers/ICML/2023_clustering/likely/huang23h.pdf) |
| The Catalog Problem: Clustering and Ordering Variable-Sized Sets | The paper introduces the Catalog Problem: clustering elements from variable-sized sets and ordering the resulting clusters without knowing the number of clusters in advance. It proposes the Neural Ordered Clusters (NOC) model, a deep learning framework that simultaneously learns to cluster elements, predict cluster sizes, and order clusters using enhanced pointer networks. NOC leverages set transformers, adaptive cardinality prediction, and latent variable modeling to address challenges in relational reasoning and clustering with flexible constraints. It achieves state-of-the-art results on synthetic and real-world datasets. | - Defines the Catalog Problem: joint clustering and ordering of variable-sized sets.<br>- Proposes the Neural Ordered Clusters (NOC) model combining clustering, cardinality prediction, and ordering.<br>- Learns an adaptive, input-dependent number of clusters.<br>- Uses Set Interdependence Transformers for relational set encoding.<br>- Utilizes latent variable models and enhanced pointer networks for ordering clusters.<br>- Outperforms baselines in both clustering quality (V-Measure) and ordering (Kendall‚Äôs tau) on synthetic and real-world datasets (e.g., PROCAT).<br>- Provides freely available code and datasets for reproducibility. | [PDF](papers/ICML/2023_clustering/likely/jurewicz23a.pdf) |
| Cluster Explanation via Polyhedral Descriptions | This paper introduces a new method to explain clusters in a dataset by constructing polyhedra (geometric shapes formed by intersections of half-spaces) around each cluster. The method formulates the problem as an integer program to optimize for low complexity (few conditions) or sparsity (few features). It uses column generation to handle the exponentially large search space of possible separating half-spaces and introduces a grouping technique to scale to large datasets. Empirical results show the approach outperforms decision-tree-based and prototype-based cluster explanations in both accuracy and interpretability on multiple benchmark datasets. | - Defines the Polyhedral Description Problem (PDP) for explaining clusters.<br>- Formulates PDP as an integer program, minimizing either complexity or sparsity.<br>- Uses column generation to efficiently search over an exponential number of possible half-spaces.<br>- Introduces a novel grouping strategy to scale explanations to large datasets.<br>- Demonstrates superior cluster description accuracy and interpretability compared to state-of-the-art baselines (CART, IMM, PROTO).<br>- Applicable for explaining black-box clusterings without needing to re-learn clusters.<br>- The approach emphasizes a trade-off between interpretability and explanation accuracy, which could inspire strategies for cluster quality assessment in genomic data. | [PDF](papers/ICML/2023_clustering/likely/lawless23a.pdf) |
| End-to-end Differentiable Clustering with Associative Memories | The paper proposes ClAM, a clustering algorithm based on Associative Memories (AMs), which enables end-to-end differentiable clustering while maintaining discrete cluster assignments. ClAM uses an energy minimization framework where data points evolve toward cluster prototypes through gradient descent, preserving hard cluster assignments. It also introduces a novel self-supervised loss based on masked pattern completion to improve clustering quality. Empirical evaluation across various datasets shows significant improvements (up to 60%) over k-means and other clustering baselines. | - Clustering framework based on Associative Memories (AMs) with end-to-end differentiability<br>- Maintains discrete hard assignments while using SGD optimization<br>- Introduces a self-supervised loss based on masked input and pattern completion<br>- Demonstrated improvements of up to 60% over k-means in Silhouette Coefficient<br>- Extensible to weighted and spherical clustering settings<br>- Potentially applicable to SNP data by adjusting the energy function and distance measures<br>- Prototypes evolve dynamically and can model interpretable cluster centroids<br>- Open-source code available at https://github.com/bsaha205/clam | [PDF](papers/ICML/2023_clustering/likely/saha23a.pdf) |
| Multi-class Graph Clustering via Approximated Effective p-Resistance | This paper proposes a novel method for multi-class clustering on graphs using an approximation of effective p-resistance, a distance measure induced by the graph p-Laplacian. The method provides a fast approximation that becomes exact for trees and is scalable to large graphs. By tuning the parameter p, the method can bias towards clusters based on internal connectivity or shortest-path distances. A theoretical justification links p-resistance to semi-supervised learning, and empirical experiments demonstrate improved clustering performance compared to existing methods. | - Proposes fast approximated effective p-resistance for graph-based clustering<br>- Theoretical bounds guarantee approximation quality; exact for tree graphs<br>- Parameter p allows tuning between connectivity-based and path-based clusters<br>- Semi-supervised learning interpretation supports clustering quality<br>- Applies k-medoids to resistance distance matrix for clustering<br>- Outperforms classical p-Laplacian spectral clustering and resistance-based clustering in experiments<br>- Scalable and robust to noise in graph structure | [PDF](papers/ICML/2023_clustering/likely/saito23a.pdf) |
| Discrete-Continuous Optimization Framework for Simultaneous Clustering and Training in Mixture Models | This paper proposes PRESTO, a framework that jointly learns data partitions (clusters) and specialized models in mixture models without assuming a prior generative model. It reformulates clustering as a discrete-continuous optimization problem with a matroid constraint and uses submodularity-inspired approximations to find near-optimal solutions. PRESTO outperforms traditional clustering and mixture model methods on multiple datasets and achieves strong performance in memory-constrained environments. | - Introduces PRESTO, a discrete-continuous optimization algorithm for clustering and training simultaneously.<br>- No assumptions on data generative model ‚Äî flexible for real-world, noisy datasets.<br>- Optimization formulated using matroid span constraints and Œ±-submodular minimization.<br>- Provides theoretical approximation guarantees even when model estimates are imperfect.<br>- Outperforms K-means++, GMM, Mixture of Experts, and others on classification tasks.<br>- Demonstrates excellent memory-accuracy tradeoffs, relevant for resource-limited settings.<br>- PRESTO could inspire SNP clustering methods with modular fitting and separation of weak/strong signals. | [PDF](papers/ICML/2023_clustering/likely/sangani23a.pdf) |
| Partial Optimality in Cubic Correlation Clustering | The paper studies the cubic correlation clustering problem, which generalizes pairwise clustering to include triplet costs. It focuses on establishing partial optimality conditions that allow parts of the clustering solution to be fixed as optimal before full optimization. The authors design efficient algorithms for detecting such conditions by reducing subproblems to min-cut problems. Experiments show that these methods can fix a large fraction of variables efficiently on synthetic and geometric datasets, particularly under low noise conditions. | - Introduces partial optimality for cubic (triplet-based) correlation clustering<br>- Efficiently identifies subsets of clustering solutions that can be fixed early using min-cut reductions<br>- Focuses on complete graphs with pairwise and triplet costs<br>- Demonstrates practical efficiency (runtime O(n^5.6)) on synthetic and geometric datasets<br>- Highlights that cut-based conditions are more effective than join-based conditions in practice<br>- Methods potentially adaptable to prune noise clusters in SNP clustering scenarios | [PDF](papers/ICML/2023_clustering/likely/stein23a.pdf) |
| Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond | The paper introduces a new data selection method combining k-means clustering with sensitivity sampling to create small, representative subsets of large datasets. Data points are clustered based on embeddings, and sampled with probability depending on both their distance to cluster centers and local model loss, assuming the loss function is H√∂lder continuous. The method is computationally efficient, robust to outliers, and offers strong theoretical guarantees. It is validated across tasks such as fine-tuning large language models and linear regression. | - Proposes clustering-based data selection using k-means and sensitivity sampling<br>- Works under H√∂lder continuity assumptions, relaxing Lipschitz constraints<br>- Efficiently approximates loss distributions with few model evaluations<br>- Outperforms uniform and classic coreset-based methods empirically<br>- Applies to neural networks, linear regression, and fine-tuning LLMs<br>- Sampling probability balances distance from cluster centers and model loss<br>- Robust against outliers, making it suitable for noisy data scenarios | [PDF](papers/ICML/2024_clustering/likely/axiotis24a.pdf) |
| Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models | The paper introduces Pruned Pivot, a new scalable algorithm for correlation clustering that improves upon the classic PIVOT method. It operates efficiently in dynamic, parallel (MPC), and local computation (LCA) settings. The method prunes query trees to limit exploration and maintains a clustering that is within a 3+Œµ approximation of the optimal. It allows for constant expected update time for graph changes, reduces computational complexity in massive graphs, and empirically matches the clustering quality of traditional approaches while being much faster. | - Introduces Pruned Pivot, a fast, scalable variant of the PIVOT algorithm for correlation clustering<br>- Maintains 3+Œµ approximation with constant expected update time in dynamic graphs<br>- Suitable for massive graphs: runs efficiently in parallel (MPC) and local (LCA) computation models<br>- Natural way to identify and isolate noise clusters (e.g., singleton 'unlucky' nodes)<br>- Experimental results show performance within 1% of traditional methods with much lower computational cost<br>- Concepts of agreement and pruning fit well with SNP grouping based on beta/Z-score similarities<br>- Provides theoretical guarantees and empirical validation | [PDF](papers/ICML/2024_clustering/likely/dalirrooyfard24a.pdf) |
| Dynamic Spectral Clustering with Provable Approximation Guarantee | This paper proposes a dynamic spectral clustering algorithm that efficiently clusters graphs as they evolve over time with new vertices and edges. It introduces a dynamic cluster-preserving sparsifier and a contracted supernode graph to approximate the cluster structure with provable guarantees. The method achieves O(1) amortized update time and sublinear query time, making it extremely fast while maintaining clustering quality. Experiments on synthetic and real-world datasets confirm that the approach produces near-optimal clusters much faster than re-running standard spectral clustering methods from scratch. | - Introduces dynamic spectral clustering with strong theoretical guarantees.<br>- Maintains cluster-preserving sparsifiers updated in O(1) amortized time.<br>- Uses contracted graphs of supernodes to track cluster structures efficiently.<br>- Detects changes in cluster number via eigen-gap analysis.<br>- Scales to graphs with hundreds of thousands of nodes and millions of edges.<br>- Applicable for clustering noisy, evolving data ‚Äî matches SNP clustering needs.<br>- Can inspire SNP graph construction and dynamic cluster updating frameworks. | [PDF](papers/ICML/2024_clustering/likely/laenen24a.pdf) |
| Image Clustering with External Guidance | This paper introduces a new paradigm called externally guided clustering, where external knowledge (textual semantics from WordNet) is used to guide the clustering process instead of relying solely on internal data patterns. The authors propose TAC (Text-Aided Clustering), which retrieves discriminative nouns for images, combines image and text features, and applies cross-modal mutual distillation to enhance clustering quality. TAC achieves state-of-the-art clustering performance on several benchmarks without needing class labels or heavy retraining. | - Introduces externally guided clustering paradigm using external knowledge sources.<br>- Proposes TAC (Text-Aided Clustering) combining image and text modalities.<br>- Retrieves discriminative nouns for each image cluster from WordNet.<br>- Applies cross-modal mutual distillation to enhance clustering quality.<br>- Achieves state-of-the-art results on multiple clustering benchmarks including ImageNet-1K.<br>- Suggests general potential of leveraging external annotations in clustering problems. | [PDF](papers/ICML/2024_clustering/likely/li24aa.pdf) |
| Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation | This paper introduces a theoretically grounded, scalable multiple kernel clustering (MKC) method called SMKC. Assuming that data follow an isotropic Gaussian distribution, the authors show that the expectation of a kernel matrix is approximately low-rank. They propose to approximate each base kernel by a rank-k matrix, fuse these into a consensus matrix, and optimize this structure efficiently using an anchor-based strategy, thus enabling clustering on very large datasets. Extensive experiments validate both the theoretical bounds and practical effectiveness of the method. | - Theoretically analyzes kernel matrix expectation under Gaussian data assumptions.<br>- Proposes Scalable Multiple Kernel Clustering (SMKC) using rank-k approximations.<br>- Anchor-based methods for handling large-scale datasets efficiently.<br>- No need for hyper-parameter tuning, making it practical for real-world data.<br>- Provides non-asymptotic theoretical guarantees for approximation errors.<br>- Outperforms several state-of-the-art MKC methods in experiments on large datasets. | [PDF](papers/ICML/2024_clustering/likely/liang24g.pdf) |
| Hierarchical Clustering: O(1)-Approximation for Well-Clustered Graphs | This paper develops two efficient hierarchical clustering algorithms with constant-factor (O(1)) approximation guarantees relative to Dasgupta's cost function. One method targets high-conductance graphs via simple degree-based clustering, while the main contribution (PruneMerge) applies to well-clustered graphs by using spectral partitioning, critical node pruning, and careful merging. Empirical results on synthetic and real-world datasets show strong improvements over prior work, particularly when cluster structures are heterogeneous or noisy. | - O(1)-approximation hierarchical clustering for well-clustered graphs<br>- HCwithDegrees: degree-based fast clustering for high-conductance graphs<br>- PruneMerge: prune-and-merge strategy for handling heterogeneous cluster structures<br>- Introduction of critical nodes for finer control over hierarchical structure<br>- Strong decomposition lemma ensures low cross-cluster noise<br>- Empirical validation: better performance on synthetic graphs with variable cluster densities and sizes<br>- Methodologies readily adaptable to SNP clustering tasks with minor adjustments | [PDF](papers/NIPS/2021_clustering/likely/10489_hierarchical_clustering_o_1_ap.pdf) |
| Row-clustering of a Point Process-valued Matrix | This paper introduces a novel clustering framework (MM-MPP) for row-level clustering of matrices where entries are marked point processes. It models each event sequence using multi-level log-Gaussian Cox processes (LGCPs) and clusters rows using a semi-parametric Expectation-Solution (ES) algorithm, efficiently estimated via functional principal component analysis (FPCA). The method demonstrates significant improvements in clustering accuracy and computational efficiency over competing approaches, validated through synthetic and large real-world datasets such as Twitter activity, taxi rides, and credit card transactions. | - Proposes a mixture model of multi-level marked point processes (MM-MPP) for clustering<br>- Each row modeled with multi-level log-Gaussian Cox processes (LGCPs)<br>- Introduces an efficient Expectation-Solution (ES) algorithm using functional PCA<br>- Demonstrates massive speed-ups and improved clustering stability compared to prior methods<br>- Handles repeated observations and multiple event types (analogous to SNPs affecting multiple traits)<br>- Multi-level decomposition: account-level, day-level, residual-level variability<br>- Potentially adaptable for SNP effect clustering with thoughtful modifications | [PDF](papers/NIPS/2021_clustering/likely/11369_row_clustering_of_a_point_proc.pdf) |
| On Margin-Based Cluster Recovery with Oracle Queries | This paper studies how to exactly recover clusters using a minimal number of oracle queries under various notions of margin separation. It introduces the convex hull margin (allowing efficient recovery using convex hull expansion) and one-versus-all margin (generalizing to pseudometric spaces). Two algorithms, CHEATREC and MREC, are proposed with strong theoretical guarantees, achieving exact cluster recovery with only O(log n) queries under margin conditions. The paper establishes deep connections between margin assumptions, active learning, and cluster recoverability. | - Introduces convex hull margin and convex hull expansion trick for cluster recovery.<br>- Proposes CHEATREC and MREC algorithms achieving O(log n) query complexity under margin conditions.<br>- Formalizes one-versus-all margin, connecting to clustering stability properties (e.g., perturbation resilience).<br>- Shows theoretical optimality of query bounds in terms of cluster margin and data dimension.<br>- Applies to general pseudometric spaces, not just Euclidean settings.<br>- Highlights connections between active learning, stability, and cluster recoverability.<br>- Potentially adaptable to effect-size-based SNP clustering to isolate meaningful biological pathways.<br>- Provides rigorous proofs of recovery guarantees and lower bounds. | [PDF](papers/NIPS/2021_clustering/likely/1965_on_margin_based_cluster_recove.pdf) |
| You Never Cluster Alone | This paper proposes Twin-Contrast Clustering (TCC), a new method for unsupervised clustering that combines instance-level and cluster-level contrastive learning. Instead of only learning how individual data points differ, TCC builds representations for entire clusters by aggregating instances with soft assignments. This method outperforms previous clustering techniques on multiple visual datasets and shows that explicitly modeling cluster context significantly improves clustering quality. TCC uses novel techniques like Gumbel-softmax reparameterization and a memory queue to maintain cluster-level consistency. | - Twin-Contrast Clustering (TCC) combining instance-level and cluster-level contrastive learning.<br>- Soft probabilistic cluster assignments via Gumbel-softmax reparameterization.<br>- Cluster context aggregation using deep sets.<br>- Use of memory queues for maintaining cluster representation consistency across batches.<br>- Extensive experiments showing strong performance over benchmarks like CIFAR-10 and ImageNet-10.<br>- Soft assignments help in dealing with noise and partial cluster memberships. | [PDF](papers/NIPS/2021_clustering/likely/218_you_never_cluster_alone.pdf) |
| Multi-view Contrastive Graph Clustering (MCGC) | MCGC proposes a new method for clustering multi-view graph data by learning a consensus graph through graph filtering, graph reconstruction, and graph-level contrastive learning. It filters node features to remove high-frequency noise, learns an optimized consensus graph from multiple views, and applies contrastive regularization to ensure clustering-friendliness. MCGC achieves state-of-the-art performance on multiple benchmark datasets, outperforming deep learning methods despite being a shallow model. Ablation studies confirm the key role of contrastive loss and graph smoothing. | - Proposes Multi-view Contrastive Graph Clustering (MCGC) for clustering noisy, multi-view graph data.<br>- Uses graph filtering to smooth node features and remove high-frequency noise.<br>- Learns a consensus graph by combining multiple views with an adaptive weighting mechanism.<br>- Applies contrastive learning at the graph-level to reinforce clustering structure.<br>- Outperforms both shallow and deep learning methods on ACM, DBLP, IMDB, Amazon datasets.<br>- Graph consensus learning and noise reduction strategies directly applicable to SNP clustering with association measures.<br>- Simple optimization strategy alternating between consensus graph updates and view weight updates.<br>- Public codebase available for reproducibility. | [PDF](papers/NIPS/2021_clustering/likely/2462_multi_view_contrastive_graph_c.pdf) |
| A Kernel-based Test of Independence for Cluster-correlated Data | This paper extends the Hilbert-Schmidt Independence Criterion (HSIC) to handle cluster-correlated data, proposing HSICcl. By adjusting for intra-cluster correlation, the authors derive the asymptotic distribution of the HSIC statistic under null and alternative hypotheses. They provide a practical testing procedure and show through simulation studies and real-world microbiome data that HSICcl effectively controls type I error and achieves higher power than competing methods. The method is kernel-based, nonparametric, and robust to complex, non-linear dependence patterns. | - Proposes HSICcl: a kernel-based independence test for cluster-correlated data.<br>- Extends empirical HSIC by deriving asymptotic null distribution under clustered settings.<br>- Efficient approximation method using estimated eigenvalues to perform testing.<br>- Simulation studies show better type I error control and higher power than naive methods.<br>- Real data application to microbiome-metabolite association identifies more significant pathways.<br>- Kernel-based approach allows modeling complex, nonlinear dependence without strong assumptions.<br>- Highlights importance of properly adjusting for correlated samples when assessing structure.<br>- Techniques could inspire robust similarity metrics for SNP clustering tasks. | [PDF](papers/NIPS/2021_clustering/likely/3015_a_kernel_based_test_of_indepen.pdf) |
| A Critique of Self-Expressive Deep Subspace Clustering | This paper critically analyzes Self-Expressive Deep Subspace Clustering (SEDSC) methods, which combine autoencoders with a self-expressive loss for unsupervised clustering. It shows that the standard formulations are often ill-posed and lead to trivial or degenerate embeddings rather than meaningful clusters. Empirical results suggest that previous successes of SEDSC can largely be attributed to ad-hoc post-processing steps, rather than the model itself. The paper concludes that significant theoretical and methodological improvements are needed to make these methods robust. | - Shows that the self-expressive loss function often leads to degenerate embeddings.<br>- Demonstrates that performance gains in previous deep clustering papers largely stem from post-processing.<br>- Provides theoretical proofs of ill-posedness in SEDSC objective formulations.<br>- Highlights the importance of normalization techniques (instance, dataset, or batch normalization) to stabilize embeddings.<br>- Offers important cautionary advice for embedding-based clustering designs. | [PDF](papers/ICLR/2021_clustering/not/1308_a_critique_of_self_expressive_.pdf) |
| Isotropy in the Contextual Embedding Space: Clusters and Manifolds | This paper studies the geometry of contextual word embedding spaces (e.g., BERT, GPT2) and challenges previous findings of strong anisotropy. It shows that while global embedding spaces appear anisotropic, local clusters of embeddings are highly isotropic after mean-centering. They use K-Means clustering, PCA, and Local Intrinsic Dimension (LID) analysis to reveal hidden structures. GPT/GPT2 embeddings form a Swiss Roll manifold related to word frequency. These findings help better understand how high-dimensional embeddings retain strong representation power. | - Introduced clustering (K-Means) to reveal local isotropy within otherwise anisotropic embedding spaces.<br>- Used PCA to analyze effective dimension and cluster separations.<br>- Discovered Swiss Roll manifold structure in GPT/GPT2 embeddings tied to word frequency.<br>- Applied Local Intrinsic Dimension (LID) to measure local geometric properties.<br>- Proposed cluster-wise center-shifting to improve isotropy measurements.<br>- Visualized and validated findings on multiple datasets (PTB, WikiText-2).<br>- Suggested that local manifold structure and clustering account for embedding effectiveness. | [PDF](papers/ICLR/2021_clustering/not/328_isotropy_in_the_contextual_emb.pdf) |
| Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction | This paper presents AV-HuBERT, a self-supervised learning framework for audio-visual speech representation. It uses masked cluster prediction across both audio and visual streams, refining clusters iteratively to improve the quality of learned features. The method significantly outperforms previous models for lip reading and speech recognition tasks, while being efficient in terms of labeled data requirements. Key innovations include multimodal cluster refinement, modality dropout for robustness, and a novel masking strategy using substitution rather than simple masking. | - Introduces AV-HuBERT for self-supervised multimodal speech representation learning<br>- Uses masked cluster prediction instead of traditional supervised loss<br>- Refines cluster assignments over iterations to improve feature quality<br>- Introduces modality dropout to handle imbalance between strong and weak signals<br>- Proposes substitution-based masking to make learning more robust<br>- Outperforms previous lip-reading and speech recognition models with far less labeled data<br>- Concepts like masking, dropout, and iterative clustering could inspire methods for SNP clustering | [PDF](papers/ICLR/2022_clustering/not/1757_learning_audio_visual_speech_r.pdf) |
| Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters | The paper proposes PrivacyFace, a framework that enhances federated learning for face recognition without compromising privacy. It introduces Differentially Private Local Clustering (DPLC) to create sanitized, privacy-agnostic clusters of local class centers, and a consensus-aware loss to align client models globally. These innovations significantly boost model performance (+9.63% and +10.26% TAR@FAR=1e-4 on IJB-B and IJB-C) while preserving differential privacy with minimal computational cost. | - Introduces Differentially Private Local Clustering (DPLC) with strong theoretical privacy guarantees.<br>- Develops a consensus-aware loss to harmonize client updates while avoiding privacy leakage.<br>- Demonstrates large performance improvements in federated face recognition with minimal overhead.<br>- Provides techniques for noise handling and robust clustering that could inform SNP clustering algorithms.<br>- Mathematical proofs for differential privacy and efficiency claims included. | [PDF](papers/ICLR/2022_clustering/not/1_improving_federated_learning_f.pdf) |
| DKM: Differentiable k-Means Clustering Layer for Neural Network Compression | This paper proposes DKM, a differentiable k-means clustering layer designed for compressing deep neural networks. Instead of hard cluster assignment, DKM uses an attention mechanism to softly assign weights to centroids and refines them through training to minimize task loss. DKM enables better compression-accuracy trade-offs than traditional clustering or quantization approaches, particularly in low-bit regimes, and can handle multi-dimensional clustering for enhanced model capacity. | - Introduces a differentiable k-means clustering layer (DKM) based on attention mechanisms<br>- Supports soft clustering during training with eventual hard assignment at inference<br>- Handles multi-dimensional clustering robustly<br>- Achieves superior compression results compared to DeepCompression, HAQ, and other methods<br>- Enables direct optimization of cluster assignments with respect to a downstream loss without interfering with the original model loss<br>- No additional parameters or loss terms needed<br>- Could inform strategies for gradual and uncertainty-aware SNP clustering | [PDF](papers/ICLR/2022_clustering/not/332_dkm_differentiable_k_means_clu.pdf) |
| Learning-Augmented k-Means Clustering | This paper proposes learning-augmented k-means algorithms that use possibly noisy predictors to improve clustering quality and runtime. The authors design robust center estimation procedures and efficient assignment steps using dimensionality reduction and approximate nearest neighbor search. The algorithms are theoretically proven to achieve (1+O(Œ±))-approximate clustering cost relative to optimal when the predictor's error rate is bounded by Œ±. Experiments on multiple real datasets demonstrate significant improvements over traditional clustering methods, even with imperfect predictors. | - Learning-augmented clustering framework with noisy predictors<br>- Robust center estimation using coordinate-wise filtering<br>- Optimized algorithm with near-linear runtime using dimension reduction and approximate nearest neighbor search<br>- Theoretical (1+O(Œ±)) approximation guarantees based on predictor error rate<br>- Handles adversarial as well as random noise in predictor labels<br>- Significant empirical improvements on internet graph, KDD, and CIFAR datasets<br>- Insight that predictors must be processed carefully, not blindly trusted | [PDF](papers/ICLR/2022_clustering/not/4103_learning_augmented_k_means_clu.pdf) |
| Discriminative Similarity for Data Clustering | This paper introduces Clustering by Discriminative Similarity (CDS), a novel clustering framework that learns discriminative similarities optimized for low generalization error. It models clustering as an unsupervised classification task and derives discriminative similarity via Rademacher complexity-based generalization bounds. The CDSK method, built on this framework, uses a kernel function with adaptive weights to generate a similarity matrix for clustering. Experiments on datasets like MNIST, CIFAR-10, and Yale faces show that CDSK significantly outperforms traditional clustering methods, especially on high-dimensional, complex data. | - Proposes Clustering by Discriminative Similarity (CDS) framework<br>- Introduces CDSK algorithm: kernel-based similarity learning with adaptive weights<br>- Optimizes clustering via minimizing generalization error bounds<br>- Uses Rademacher complexity for bounding classifier error<br>- Strong connection to kernel density classification theory<br>- Efficient coordinate descent optimization for large datasets<br>- Significantly outperforms k-means, spectral clustering, and random forest clustering methods<br>- Scalable and robust to high-dimensional, noisy data | [PDF](papers/ICLR/2022_clustering/not/4489_discriminative_similarity_for_.pdf) |
| Robust Fair Clustering: A Novel Fairness Attack and Defense Framework | This paper investigates whether existing fair clustering algorithms are vulnerable to adversarial attacks that degrade fairness without harming clustering accuracy. It introduces a novel black-box fairness attack by perturbing protected group memberships and shows that state-of-the-art fair clustering algorithms are highly vulnerable. To counter this, they propose Consensus Fair Clustering (CFC), a robust method that transforms consensus clustering into a fair graph partitioning problem, combining self-supervised contrastive learning and fairness loss to resist attacks. CFC outperforms traditional methods under adversarial conditions across multiple datasets. | - Introduced a novel black-box attack that perturbs protected group labels to degrade fairness in clustering.<br>- Proposed Consensus Fair Clustering (CFC), a two-stage defense using consensus clustering and graph-based learning.<br>- Demonstrated robustness of CFC across real-world datasets compared to existing fair clustering models.<br>- Utilized self-supervised contrastive loss and fairness regularization to build resilient cluster embeddings.<br>- Techniques could inspire SNP clustering designs robust to biological noise and unstable association signals. | [PDF](papers/ICLR/2023_clustering/not/1259_robust_fair_clustering_a_novel.pdf) |
| Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks | This paper introduces a new clustering approach based on an incompatibility property between data subsets: training on one subset should not improve performance on another if they are incompatible. Using a self-expansion measure, the authors iteratively trim and refine clusters through an Inverse Self-Paced Learning algorithm. A boosting procedure is then used to identify clean clusters. The method is evaluated on image datasets (CIFAR-10, GTSRB) for defending against various backdoor attacks, achieving below 1% attack success in most cases with minimal clean accuracy loss. | - Incompatibility clustering: partitions based on self-generalization failure across subsets.<br>- Inverse Self-Paced Learning (ISPL): iterative data refinement minimizing self-expansion error.<br>- Boosting method to identify clean subsets from clusters.<br>- Formal statistical guarantees: convergence and error decay.<br>- Empirical success against a wide range of backdoor poisoning attacks in deep neural networks.<br>- Source code publicly available for reproducibility and adaptation. | [PDF](papers/ICLR/2023_clustering/not/3055_incompatibility_clustering_as_.pdf) |
| Machine Unlearning of Federated Clusters | This paper introduces a framework for federated clustering (FC) that supports efficient and secure machine unlearning. They propose Secure Compressed Multiset Aggregation (SCMA) to aggregate client cluster information without leaking private data, and show that efficient clustering and unlearning can be achieved without full retraining. They use K-means++ initialization at clients, secure aggregation, and weighted clustering at the server, and prove strong theoretical performance guarantees. Their method achieves an 84√ó speed-up over retraining and handles imbalanced clusters well. Datasets include genetic and microbiome data, showcasing relevance to biological applications. | - Secure Compressed Multiset Aggregation (SCMA) for privacy-preserving data aggregation.<br>- K-means++ initialization at clients, full clustering at the server.<br>- Fast machine unlearning mechanism avoiding full retraining.<br>- Handles imbalanced clusters effectively.<br>- 84√ó average speed-up over retraining for unlearning requests.<br>- Application to real biological datasets (TCGA methylation, microbiome).<br>- Theoretical guarantees for clustering performance and unlearning complexity. | [PDF](papers/ICLR/2023_clustering/not/3491_machine_unlearning_of_federate.pdf) |
| The Hidden Uniform Cluster Prior in Self-Supervised Learning | This paper reveals that many popular self-supervised learning methods (e.g., SimCLR, VICReg, SwAV, MSN) unintentionally impose a uniform clustering prior on the learned features, favoring equal-sized clusters. While effective on class-balanced datasets like ImageNet, this uniformity bias severely degrades performance on real-world, class-imbalanced datasets that follow long-tailed distributions. The authors propose Prior Matching for Siamese Networks (PMSN), which allows arbitrary priors (e.g., power-law) instead of uniform. They show, through theoretical analysis, toy experiments, and large-scale evaluations (iNaturalist18, ImageNet), that matching the prior to the data distribution improves learned representations and downstream task performance. | - SSL methods (SimCLR, VICReg, SwAV, MSN) implicitly encourage uniform clusters.<br>- Uniform cluster prior harms performance on class-imbalanced datasets.<br>- Proposed Prior Matching for Siamese Networks (PMSN) allows flexible, non-uniform priors.<br>- Toy experiments show semantic features are suppressed by uniform priors but recovered by power-law priors.<br>- Real-world experiments (iNaturalist18) show substantial transfer learning gains by using non-uniform priors.<br>- Concept of controlling feature space structure via priors is introduced, applicable to any high-dimensional clustering task. | [PDF](papers/ICLR/2023_clustering/not/3748_the_hidden_uniform_cluster_pri.pdf) |
| Improved Learning-Augmented Algorithms for k-Means and k-Medians Clustering | This paper presents improved deterministic algorithms for k-means and k-medians clustering in the presence of noisy auxiliary labels. The authors propose trimming predicted clusters and constructing robust centers (means or medians) to achieve near-optimal clustering cost even when up to 50% of the points are mislabeled. They significantly improve the previous approximation guarantees and run in near-linear time. Experiments on CIFAR-10, MNIST, and PHY datasets show competitive or superior performance compared to prior methods. The work offers insights into robust clustering using imperfect predictor information, emphasizing deterministic, scalable, and noise-tolerant designs. | - Proposes deterministic learning-augmented algorithms for k-means and k-medians clustering.<br>- Handles label error rates up to 50%, significantly improving prior work (which only handled <14%).<br>- Uses robust subset selection and center construction to minimize the impact of noisy points.<br>- Proves tight theoretical bounds on clustering quality and computational efficiency.<br>- Experiments validate strong empirical performance across various real-world datasets.<br>- Insights on how auxiliary information can be corrected during clustering, relevant for SNP clustering with noisy beta/Z-scores. | [PDF](papers/ICLR/2023_clustering/not/4639_improved_learning_augmented_al.pdf) |
| Task-Customized Masked Autoencoder via Mixture of Cluster-Conditional Experts | This paper addresses the problem of negative transfer in self-supervised pretraining models like Masked Autoencoders (MAE) when applied to semantically different downstream tasks. It proposes Mixture of Cluster-Conditional Experts (MoCE), which first clusters the data based on semantic similarity and then trains different experts on these clusters. A customized routing mechanism ensures that each downstream task uses the most semantically relevant expert. Experiments show MoCE outperforms MAE by 2.45% on transfer tasks and achieves state-of-the-art results in object detection and segmentation benchmarks. It also offers training and deployment efficiencies by activating only relevant experts. | - Identifies negative transfer in standard Masked Autoencoders (MAE) for semantically different tasks.<br>- Proposes Mixture of Cluster-Conditional Experts (MoCE) for task-customized pretraining.<br>- Clusters pretraining data based on semantic similarity using learned embeddings.<br>- Trains different experts on different clusters to avoid negative transfer.<br>- Introduces efficient expert routing and sub-model deployment strategies.<br>- Achieves +2.45% improvement over MAE in transfer accuracy across 11 tasks.<br>- Outperforms baselines in object detection and segmentation (ADE20K, COCO).<br>- Conceptually adaptable to clustering heterogeneous SNPs for Mendelian randomization. | [PDF](papers/ICLR/2023_clustering/not/4851_task_customized_masked_autoenc.pdf) |
| KWIKBUCKS: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals | The paper introduces KwikBucks, a novel algorithm for budgeted correlation clustering when similarities are available from two sources: an expensive, accurate oracle and a cheaper, weaker oracle. KwikBucks uses the cheap signal to prioritize and minimize strong queries while extending the classical KwikCluster framework. It offers both theoretical guarantees and practical efficiency, demonstrating large improvements in clustering quality and query reduction across diverse datasets. The strategy of using weak signals to guide expensive queries can be adapted to noisy SNP data clustering. | - Two-oracle model: cheap-weak and expensive-strong signals.<br>- WeakFilterByRanking: ranks nodes based on weak similarity scores.<br>- Budgeted querying strategy using sampled pivots.<br>- Theoretical 3-approximation with controlled additive errors.<br>- Empirical validation across text and graph datasets.<br>- Practical optimizations like post-clustering merges based on weak signals.<br>- General framework easily adaptable to noisy genomic data. | [PDF](papers/ICLR/2023_clustering/not/5179_kwikbucks_correlation_clusteri.pdf) |
| Differentially-Private Clustering of Easy Instances | This paper introduces a framework for practical differentially-private clustering by focusing on 'easy' instances ‚Äî datasets where clusters are well-separated. It defines a new 'k-tuple clustering' problem and proposes two private algorithms (PrivatekAverages and PrivatekNoisyCenters) that first privately test for cluster separability, and then perform clustering with added noise to preserve privacy. They demonstrate that this approach can be used to privately approximate k-means clustering and learn mixtures of Gaussians under reasonable assumptions, and provide empirical results supporting its practicality. | - Defines the k-tuple clustering problem with well-separated clusters as a core building block.<br>- Develops two algorithms: PrivatekAverages (theoretically stronger, higher sample needs) and PrivatekNoisyCenters (more practical).<br>- Introduces differentially private tests for checking data 'niceness' (cluster separability).<br>- Applies framework to private k-means clustering and mixture of Gaussians learning.<br>- Highlights a modular approach: test for separation, then cluster ‚Äî relevant for SNP data.<br>- Empirical evaluation shows practical feasibility despite theoretical complexity. | [PDF](papers/ICML/2021_clustering/not/cohen21c.pdf) |
| Two-way Kernel Matrix Puncturing: towards resource-efficient PCA and spectral clustering | This paper introduces a method for reducing computational and storage costs in PCA and spectral clustering by randomly puncturing (sparsifying) both the data and kernel matrices. Theoretical analysis shows that clustering and PCA performance are largely preserved even with significant puncturing, and phase transitions identify when degradation occurs. The approach is tested on synthetic and real-world image datasets, showing significant computational savings with minimal loss in accuracy. | - Introduces two-way puncturing: random deletion of entries from both data and kernel matrices.<br>- Theoretical analysis shows eigenvector structure is preserved up to a sparsity threshold.<br>- Identifies phase transitions for when clustering/PCA performance deteriorates.<br>- Demonstrates that sparse matrices can still yield accurate clustering on synthetic and real datasets.<br>- Connects punctured matrix behavior to Marcenko-Pastur and Wigner laws from random matrix theory.<br>- Suggests that puncturing can help decorrelate real-world data, making clustering theoretically easier. | [PDF](papers/ICML/2021_clustering/not/couillet21a.pdf) |
| Heterogeneity for the Win: One-Shot Federated Clustering | The paper introduces k-FED, a one-shot federated clustering method that exploits heterogeneity in data distributions to reduce communication overhead and improve clustering performance. k-FED has each device run a local clustering algorithm and send cluster centers to a central server, which then aggregates them into global clusters in a single communication round. Theoretical analysis shows that heterogeneity allows for weaker separation assumptions compared to centralized clustering. Empirical results demonstrate high clustering accuracy and robustness on synthetic and real-world datasets. | - Introduces k-FED: a one-shot, communication-efficient federated clustering algorithm.<br>- Shows that statistical heterogeneity can help relax cluster separation requirements.<br>- Employs a two-stage clustering: local device clustering and global center aggregation.<br>- Demonstrates robustness to device failures and partial participation.<br>- Validated on FEMNIST, Shakespeare, and synthetic Gaussian mixtures datasets.<br>- Highlights new applications for federated personalized learning and client selection. | [PDF](papers/ICML/2021_clustering/not/dennis21a.pdf) |
| Efficient Online Learning for Dynamic k-Clustering | This paper studies clustering in dynamic environments where clients move unpredictably over time. It proposes Dynamic k-Clustering, an online learning problem where k centers are chosen at each time step to minimize connection cost. Key innovations include a fractional relaxation that allows efficient online no-regret learning and deterministic or randomized rounding techniques to map fractional solutions to discrete center placements. The algorithms achieve Œò(min(k, r)) regret, where r is the maximum number of clients at any time. Empirical results show practical low-regret behavior across dynamic distributions. | - Introduces Dynamic k-Clustering framed as an online learning problem.<br>- Proposes fractional placement of centers, solved efficiently via convex optimization and subgradient methods.<br>- Provides deterministic (Œò(k)) and randomized (Œò(r)) rounding schemes to obtain discrete center sets.<br>- Proves impossibility of constant regret under standard complexity assumptions.<br>- Demonstrates major empirical improvements in clustering dynamic client sets under various realistic scenarios.<br>- Builds a general framework for extending fractional relaxation to other combinatorial online learning problems. | [PDF](papers/ICML/2021_clustering/not/fotakis21a.pdf) |
| Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning | The paper proposes clustered sampling as a way to reduce variance and improve representativity in client selection for federated learning (FL). It introduces two strategies: clustering based on clients' sample sizes and clustering based on model similarity. Theoretical results prove that clustered sampling leads to lower variance and unbiased model updates, with convergence guarantees equivalent to standard FL sampling. Experiments on MNIST and CIFAR-10 datasets show that clustered sampling improves training stability and final accuracy, especially under non-iid data conditions. | - Introduces clustered sampling to improve client selection in federated learning.<br>- Provides theoretical proofs of lower sampling variance and unbiased aggregation.<br>- Develops two algorithms: one based on sample size clustering, one on model similarity clustering.<br>- Demonstrates superior performance in non-iid settings on MNIST and CIFAR-10 datasets.<br>- Maintains compatibility with existing privacy-preserving and communication-efficient FL methods.<br>- Highlights the first theoretical investigation of variance properties in client sampling. | [PDF](papers/ICML/2021_clustering/not/fraboni21a.pdf) |
| Approximate Group Fairness for Clustering | This paper introduces core fairness into the classical clustering problem, ensuring that no large coalition of points can significantly reduce their distance by deviating to a different center. They develop approximate fairness models, relaxing either the required coalition size or improvement, and design algorithms with provable guarantees for various types of metric spaces. Experiments show that their fair clustering methods improve group fairness while maintaining good clustering efficiency. Their ideas around fairness and coalition blocking could inform the design of more robust, noise-resistant clustering algorithms. | - Introduces core fairness for clustering, ensuring no large group can deviate for better distance costs.<br>- Proposes two relaxation dimensions: distance improvement (Œ≤) and coalition size (Œ±).<br>- Designs approximation algorithms for line, tree, and general metric spaces.<br>- Proves existence and efficiency bounds for approximate core clustering.<br>- Develops two-stage algorithms to refine clustering quality while maintaining fairness.<br>- Shows experimentally that fair clusterings outperform classic algorithms like k-means++ on group fairness.<br>- Fairness frameworks could be adapted to SNP clustering to detect and isolate noise clusters or unfair groupings. | [PDF](papers/ICML/2021_clustering/not/li21j.pdf) |
| Sharper Generalization Bounds for Clustering | This paper proposes a unified framework for clustering learning and derives state-of-the-art generalization bounds for clustering algorithms like k-means, kernel k-means, and spectral clustering. They introduce clustering Rademacher complexity and local clustering Rademacher complexity to study excess risk bounds, achieving faster convergence rates under mild covering number assumptions rather than strong margin conditions. Their work offers a theoretical foundation for building clustering algorithms that are statistically robust and efficient. | - Introduces a general framework for clustering based on pairwise functions and partitions.<br>- Develops clustering Rademacher complexity and local clustering Rademacher complexity.<br>- Obtains excess risk bounds of O(K¬≤/n) for general clustering and O(K/n) for hard clustering under mild assumptions.<br>- Avoids strong margin conditions required in previous work, making results more applicable to real-world data.<br>- Techniques applicable across k-means, kernel k-means, spectral clustering, and neural network-based clustering.<br>- Highlights that controlling model complexity via covering numbers is sufficient for strong generalization guarantees.<br>- Lays a statistical foundation for clustering methods that aim to balance accuracy and robustness to noise. | [PDF](papers/ICML/2021_clustering/not/li21k.pdf) |
| One Pass Late Fusion Multi-View Clustering | This paper introduces OP-LFMVC, an algorithm that unifies the learning of a consensus clustering matrix and the generation of cluster labels into a single optimization process. OP-LFMVC improves clustering quality and computational efficiency over previous multi-view clustering methods by tightly coupling partition learning and label generation. It is parameter-free, has provable convergence, and achieves strong generalization error bounds. Comprehensive experiments across benchmark datasets show it outperforms existing methods in clustering accuracy, robustness, and efficiency. | - Proposes OP-LFMVC, a unified one-pass clustering method combining consensus learning and label generation.<br>- Develops a four-step alternate optimization algorithm with theoretical convergence guarantees.<br>- Proves a generalization error bound using Rademacher complexity analysis.<br>- Demonstrates superior performance compared to state-of-the-art multi-view clustering algorithms across several datasets.<br>- Algorithm is parameter-free and scales linearly with data size, suitable for large-scale clustering tasks.<br>- Highlights the benefits of learning cluster labels directly rather than relying on post-processing (e.g., k-means).<br>- Concepts of robust clustering and negotiation between partition and labels could inspire SNP clustering adaptations. | [PDF](papers/ICML/2021_clustering/not/liu21l.pdf) |
| Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering | This paper explores how randomized dimensionality reduction can help speed up clustering for high-dimensional datasets without significant loss in quality. Focusing on facility location and single-linkage clustering, the authors show that projecting data into a number of dimensions proportional to its intrinsic (doubling) dimension can preserve clustering costs up to constant or (1+Œµ) factors. They provide strong theoretical proofs, lower bounds, and empirical results demonstrating that this method significantly accelerates clustering tasks while maintaining solution quality. | - Proposes using random projections to reduce data dimensionality based on the intrinsic doubling dimension.<br>- Proves that clustering quality (facility location cost, MST structure) is preserved after dimensionality reduction.<br>- Demonstrates significant runtime improvements (up to 80x) with small loss in solution quality.<br>- Establishes new theoretical guarantees and lower bounds for facility location and MST after projection.<br>- Experimental validation on MNIST and Faces datasets supports practical benefits.<br>- Highlights that in low doubling dimension settings, high-dimensional clustering can be significantly accelerated. | [PDF](papers/ICML/2021_clustering/not/narayanan21b.pdf) |
| Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels | This paper proposes an alternative to the anchor-point-based methods for estimating noise transition matrices in supervised learning with noisy labels. It introduces a clusterability condition based on 2-nearest neighbor (2-NN) relationships and uses high-order consensuses among noisy labels to estimate the transition matrix. The proposed High-Order Consensus (HOC) estimator achieves better sample efficiency and estimation accuracy than traditional anchor-based methods and shows strong empirical performance on noisy datasets like CIFAR-10, CIFAR-100, and Clothing1M. | - Replaces anchor-point reliance with a 2-nearest neighbor clusterability assumption.<br>- Introduces High-Order Consensus (HOC) method based on first-, second-, and third-order label agreements.<br>- Provides theoretical guarantees (uniqueness of solution, sample complexity benefits).<br>- Demonstrates effectiveness on synthetic and real noisy datasets (CIFAR-10/100, Clothing1M).<br>- Potentially extendable to local instance-dependent noise settings, suggesting flexibility in different noise scenarios.<br>- Open-source implementation available at https://github.com/UCSC-REAL/HOC. | [PDF](papers/ICML/2021_clustering/not/zhu21e.pdf) |
| Massively Parallel k-Means Clustering for Perturbation Resilient Instances | The paper presents scalable k-means clustering algorithms tailored for perturbation-resilient datasets in the Massively Parallel Computation (MPC) model. By leveraging Locality Sensitive Hashing (LSH) to create near neighbor graphs and hierarchical cluster trees, the authors design dynamic programming algorithms that find exact or (1 + Œµ)-approximate solutions using O(1) parallel rounds. They bypass known lower bounds for general clustering problems by assuming the dataset has a stable underlying structure. Experiments on synthetic and real-world datasets show that the approach yields strong clustering performance even when perturbation resilience is imperfect. | - Introduces massively parallel k-means clustering algorithms for perturbation-resilient instances.<br>- Uses Locality Sensitive Hashing (LSH) for fast neighborhood graph construction.<br>- Applies hierarchical clustering trees and dynamic programming for cluster extraction.<br>- Achieves optimal or near-optimal k-means clustering in O(1) parallel rounds.<br>- Demonstrates strong empirical performance even without perfect perturbation resilience.<br>- Techniques are highly adaptable for clustering SNPs based on beta/Z-score similarities.<br>- Focus on bypassing worst-case hardness aligns well with biological data clustering needs. | [PDF](papers/ICML/2022_clustering/not/cohen-addad22b.pdf) |
| A Random Matrix Analysis of Data Stream Clustering: Coping With Limited Memory Resources | This paper introduces an online kernel spectral clustering method for high-dimensional data streams under strict memory constraints. By modeling only a small subset of the Gram matrix via a Toeplitz mask, the authors leverage random matrix theory to derive when and how clustering remains effective despite heavy sparsification. They discover novel phase transition phenomena and propose an online clustering algorithm validated on real image datasets, achieving near-optimal performance with dramatically reduced memory usage. | - Introduces a new online spectral clustering method using a banded (Toeplitz-masked) kernel matrix.<br>- Applies random matrix theory to characterize phase transitions and noise effects in high-dimensional clustering.<br>- Demonstrates that only limited memory is needed to achieve near-optimal clustering performance.<br>- Develops a practical online clustering algorithm tested on image data (Fashion-MNIST, BigGAN).<br>- Shows how eigenvector behavior changes with increasing data sparsity, relevant for noise cluster isolation.<br>- Provides an open-source codebase for reproducing simulations. | [PDF](papers/ICML/2022_clustering/not/lebeau22a.pdf) |
| Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering | The paper proposes Orchestra, an unsupervised federated learning method that orchestrates a distributed clustering process to learn discriminative, privacy-preserving representations without labeled data. Clients perform local clustering on representations, send centroids to a server, which performs global clustering. This ensures globally consistent partitions and robust generalization, particularly under heterogeneity. Orchestra outperforms adaptations of centralized SSL methods (like SimCLR, BYOL) in cross-device federated settings, achieves communication efficiency, and scales well. | - Proposes a distributed clustering approach robust to heterogeneity and communication limits.<br>- Uses local clustering + global centroid aggregation for efficiency and privacy.<br>- Introduces theoretical bounds linking clusterability to generalization error.<br>- Demonstrates that increased heterogeneity can improve performance.<br>- Sinkhorn-Knopp based clustering to enforce equal-sized partitions.<br>- Demonstrates very low computational overhead suitable for resource-constrained environments. | [PDF](papers/ICML/2022_clustering/not/lubana22a.pdf) |
| Convergence and Recovery Guarantees of the K-Subspaces Method for Subspace Clustering | This paper analyzes the K-Subspaces (KSS) algorithm for clustering data lying in a union of overlapping subspaces. It proves that with a proper initialization (TIPS method based on thresholding inner products and spectral clustering), KSS converges superlinearly and achieves exact recovery in Œò(log log N) iterations. The paper presents strong theoretical results under a semi-random model and demonstrates through experiments that KSS is computationally efficient and competitive with state-of-the-art methods like sparse subspace clustering (SSC). | - Provides superlinear convergence guarantees for the K-Subspaces method under semi-random union of subspaces models.<br>- Introduces TIPS initialization based on thresholded inner-products and spectral clustering.<br>- Demonstrates robustness to subspace overlap and moderate affinity between clusters.<br>- Empirically competitive with sparse subspace clustering (SSC) and other methods.<br>- Algorithmic simplicity and linear computational cost per iteration make it scalable to large datasets.<br>- Could inspire robust, scalable SNP clustering methods with strong noise isolation properties. | [PDF](papers/ICML/2022_clustering/not/wang22r.pdf) |
| Parallel Online Clustering of Bandits via Hedonic Game | This paper introduces CLUB-HG, an algorithm that clusters users in contextual bandit settings by modeling clustering as a hedonic game. Each user/player independently chooses to stay or leave clusters based on their similarity to others, leading to a self-organized Nash equilibrium. The method supports parallel online learning, adapts to dynamic settings, and offers strong theoretical guarantees such as sublinear regret. Experiments show it outperforms state-of-the-art bandit clustering methods, being faster and more accurate in recovering true clusters even in noisy environments. | - Novel use of hedonic games for self-organizing clustering<br>- Parallel online updating, which may inspire SNP clustering pipelines with dynamic re-analysis<br>- Noise robustness through cost-based user incentives<br>- Theoretical guarantees on convergence and regret<br>- No need for predefining cluster numbers<br>- Empirical validation on large real-world datasets (Netflix, MovieLens) | [PDF](papers/ICML/2023_clustering/not/cheng23d.pdf) |
| Approximation Algorithms for Fair Range Clustering | The paper introduces a clustering framework called fair range clustering, where clusters must fairly represent different demographic groups within flexible ranges rather than fixed quotas. The authors develop constant-factor approximation algorithms for this problem across various ‚Ñìp-objectives (like k-means, k-median, k-center). They design efficient algorithms using LP-relaxation, sparsification, rounding via network flow, and polyhedral combinatorics. Their approach improves clustering quality over strict fairness models and offers theoretical guarantees of performance. | - Proposes fair range clustering with group representation intervals rather than strict quotas.<br>- Provides constant-factor approximation algorithms for k-means, k-median, and k-center under fairness constraints.<br>- Uses LP relaxation (FAIRRANGELP and STRUCTUREDLP) with rounding strategies based on network flow.<br>- Introduces efficient rounding from half-integral solutions exploiting total unimodularity.<br>- Demonstrates that relaxed fairness constraints significantly improve clustering quality.<br>- Focus is on fairness and group balance, not noise resistance or association score clustering. | [PDF](papers/ICML/2023_clustering/not/hotegni23a.pdf) |
| Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost | This paper introduces a series of algorithms that modify existing hierarchical clusterings to make them fair and balanced, with minimal increase in clustering cost (measured by Dasgupta‚Äôs cost function). It proposes simple tree operators like rebalancing, subtree insertion, level abstraction, and tree folding, and combines them to achieve deterministic and stochastic fairness under broad constraints. The methods substantially improve the previous approximation factors for fairness while maintaining explainable changes to clustering structure. Experiments on real-world datasets confirm the theoretical results. | - Introduces explainable tree operators for modifying hierarchical clusterings.<br>- Achieves near-exponential improvements in fairness-cost tradeoffs.<br>- Handles fairness for multiple protected classes and varying class proportions.<br>- Provides algorithms for both deterministic and stochastic fairness.<br>- Applies and validates algorithms on real-world datasets (Census and Bank).<br>- Focuses on hierarchical clustering cost minimization using Dasgupta‚Äôs cost. | [PDF](papers/ICML/2023_clustering/not/knittel23a.pdf) |
| Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs | This paper presents two nearly-linear time hierarchical clustering algorithms that produce trees with a cost within a constant factor (O(1)) of the optimal under Dasgupta‚Äôs cost, specifically for graphs with well-defined cluster structures. The methods combine spectral clustering with degree-based bucketing and efficient tree merging strategies. The first algorithm uses recursive sparsest cuts on contracted graphs, while the second constructs caterpillar-like trees under balanced degree conditions. Experimental results on synthetic and real-world datasets demonstrate that the proposed methods achieve better clustering quality at a fraction of the computational cost compared to prior work. | - Proposes SpecWRSC: a nearly-linear time spectral clustering + bucketing method.<br>- Achieves O(1)-approximate hierarchical clustering cost under Dasgupta‚Äôs metric.<br>- Introduces degree-based bucketing to simplify hierarchical tree construction.<br>- Avoids reliance on high inner-cluster conductance (a limitation of prior work).<br>- Extensive experiments on synthetic (SBM, HSBM) and real-world datasets.<br>- Algorithm is scalable: handles graphs with tens of thousands of nodes.<br>- Potentially useful for large-scale clustering of SNPs with well-separated signals. | [PDF](papers/ICML/2023_clustering/not/laenen23a.pdf) |
| Consistency of Multiple Kernel Clustering | This paper theoretically analyzes the consistency of kernel weights in multiple kernel clustering (MKC), introducing SimpleMKKM. The authors derive a non-asymptotic convergence bound of the kernel weights and establish an excess clustering risk bound. They also propose a Nystrom-based scalable MKC method with theoretical learning guarantees, allowing efficient clustering of large datasets. Extensive experiments on benchmark and large-scale datasets validate their theoretical results. | - First theoretical proof of kernel weight consistency in multiple kernel clustering (MKC).<br>- Non-asymptotic bound of OÃÉ(k/‚àön) for kernel weight convergence.<br>- Excess clustering risk bound for SimpleMKKM.<br>- Nystrom-based scalable MKC algorithm with learning guarantees.<br>- Validation on large datasets (up to 580,000 samples) showing scalability and effectiveness.<br>- Highlights the importance of eigenvalue gaps for clustering stability.<br>- Suggests that kernel-based methods can be extended to very large biological datasets if needed. | [PDF](papers/ICML/2023_clustering/not/liang23b.pdf) |
| CLUSTSEG: Clustering for Universal Segmentation | CLUSTSEG is a universal segmentation framework that formulates multiple types of segmentation tasks (superpixel, semantic, instance, panoptic) as clustering problems. Using transformer-based architectures, it introduces two key innovations: Dreamy-Start (task-specific initialization of cluster centers) and Recurrent Cross-Attention (iterative EM-style cluster assignment and updating). CLUSTSEG achieves superior performance across standard benchmarks, demonstrating the strength of clustering as a unifying principle for segmentation tasks. | - Unifies multiple segmentation tasks through clustering.<br>- Introduces 'Dreamy-Start': task-specific initialization of cluster centers.<br>- Uses Recurrent Cross-Attention: mimicking Expectation-Maximization (EM) clustering.<br>- Demonstrates state-of-the-art results on COCO Panoptic, ADE20K, and BSDS500 datasets.<br>- Framework handles heterogeneity between tasks (semantic vs instance vs superpixel).<br>- No extra parameters are needed for iterative clustering.<br>- Focuses on robust, informative cluster center selection to mitigate noise.<br>- Could inspire SNP clustering designs by adapting cluster initialization and EM-like iterative refinement. | [PDF](papers/ICML/2023_clustering/not/liang23h.pdf) |
| Spurious Valleys and Clustering Behavior of Neural Networks | This paper investigates the geometry of the loss landscapes of neural networks. It proves explicit size bounds for hidden layers to avoid spurious valleys, ensuring that gradient descent reaches global minima. Additionally, it introduces a novel method for assessing whether a neural network can represent a target function by studying the clustering behavior of critical error values across translated training datasets. These clusters reveal information about model expressiveness and the presence of spurious minima. | - Explicit bounds on hidden layer sizes to avoid spurious valleys (Theorems 3.7, 3.10)<br>- Clustering of singular error points as indicators of model representability (Theorem 4.4)<br>- Use of algebraic geometry techniques to study neural network loss landscapes<br>- Potentially inspiring ideas for defining and isolating 'noise clusters' in complex spaces<br>- Related work connections to landscape geometry, algebraic approaches, and robustness studies in deep learning | [PDF](papers/ICML/2023_clustering/not/pollaci23a.pdf) |
| Effective Neural Topic Modeling with Embedding Clustering Regularization | The paper introduces ECRTM (Embedding Clustering Regularization Topic Model) to solve the problem of topic collapsing in neural topic models. By applying a novel Embedding Clustering Regularization (ECR) based on optimal transport, the model forces topic embeddings to act as centers of separately aggregated clusters of word embeddings. This produces distinct, non-redundant topics. ECRTM outperforms state-of-the-art models on topic quality, diversity, and downstream classification tasks across multiple datasets. | - Problem: Topic collapsing (semantic redundancy) in neural topic models<br>- Proposed method: Embedding Clustering Regularization (ECR) using optimal transport<br>- New model: ECRTM (Embedding Clustering Regularization Topic Model)<br>- Avoids trivial solutions by presetting cluster size constraints<br>- Joint optimization with standard VAE-based topic modeling<br>- Extensive evaluation on benchmark datasets shows improved topic coherence, diversity, and document clustering<br>- Potentially adaptable to SNP clustering by treating SNPs' association statistics as embeddings | [PDF](papers/ICML/2023_clustering/not/wu23c.pdf) |
| Near-Optimal Quantum Coreset Construction Algorithms for Clustering | This paper develops a quantum algorithm for constructing coresets for k-clustering problems like k-median and k-means. The proposed method uses quantum approximate nearest neighbor search and quantum summation to reduce dataset size significantly while preserving clustering structure within a small approximation error. The approach achieves a near-quadratic speedup over classical methods, with matching lower bounds confirming near-optimality. A novel multidimensional quantum approximate summation subroutine is introduced, offering a potentially wide application in quantum machine learning. | - Quantum coreset construction reduces n-point dataset to size polynomial in k, d, and 1/epsilon<br>- Introduces a novel subroutine: multidimensional quantum approximate summation<br>- Achieves OÃÉ(‚àönkd^{3/2}) query complexity for clustering tasks<br>- Proves matching quantum lower bounds for clustering complexity<br>- Highlights quantum speedup potential for clustering without requiring strong assumptions like sparsity<br>- Coreset approach generalizable across k-median, k-means, and (k,z)-clustering problems | [PDF](papers/ICML/2023_clustering/not/xue23a.pdf) |
| CHAI: Clustered Head Attention for Efficient LLM Inference | The paper introduces CHAI (Clustered Head Attention), a method to improve efficiency during inference in large language models. CHAI clusters attention heads that produce similar outputs dynamically at inference time, reducing computation and memory use without fine-tuning. By analyzing head output correlations, CHAI prunes redundant operations and maintains model accuracy within 3.2% while reducing K,V cache memory by up to 21.4% and speeding up inference by 1.73√ó. Techniques include offline elbow plot analysis for cluster numbers and online lightweight dynamic clustering based on initial token outputs. | - Proposes dynamic clustering of attention heads based on high correlation in outputs<br>- Reduces inference memory (K,V cache) by up to 21.4% and speeds up inference by 1.73√ó<br>- Maintains high model accuracy with minimal loss (within 3.2%)<br>- Uses elbow plots for offline determination of number of clusters<br>- Online cluster membership determination after processing five tokens<br>- Ideas about redundancy, dynamic grouping, and lightweight clustering could inspire SNP clustering methods | [PDF](papers/ICML/2024_clustering/not/agarwal24a.pdf) |
| Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better | The paper addresses the Cluster Deletion problem, aiming to efficiently partition a graph into cliques by deleting the fewest number of edges. The authors develop simpler, faster combinatorial algorithms that improve previous approximation guarantees from 4 to 3. Key innovations include deterministic degree-based pivoting strategies and a combinatorial method for solving LP relaxations using minimum cut reductions. Their methods are highly scalable and robust against noise, with broad applications in graph clustering tasks, including biological networks. | - Focus on Cluster Deletion: partitioning graphs into disjoint cliques by minimal edge deletion<br>- Improved approximation guarantees: 3-approximation algorithms developed<br>- Deterministic, degree-based pivoting strategy for clustering<br>- Combinatorial solution replacing expensive LP solvers<br>- Efficient algorithms scaling to graphs with millions of nodes and billions of edges<br>- Robustness to noisy edges through careful edge deletion strategies<br>- Applicable to biological network clustering, including gene expression data | [PDF](papers/ICML/2024_clustering/not/balmaseda24a.pdf) |
| Dynamic Correlation Clustering in Sublinear Update Time | The paper proposes a dynamic algorithm for correlation clustering where nodes are added adversarially and deleted randomly. The algorithm maintains a clustering with an O(1)-approximation to the optimal correlation clustering cost, achieving sublinear (polylogarithmic) update time per node. It uses a dynamic, sparse representation of the graph based on anchor nodes and probabilistic agreement checks, supported by an efficient notification system to propagate graph changes. Experiments show that the method outperforms prior approaches on real-world data, especially for dense graphs. | - Proposes the Dynamic Agreement Algorithm for correlation clustering<br>- Uses sparse graph maintenance via anchor nodes and probabilistic agreement checking<br>- Achieves O(1)-approximation to optimal clustering cost<br>- Maintains sublinear polylog(n) update time per node addition/deletion<br>- Highly scalable, robust against noisy graph perturbations<br>- Experimentally validated on large real-world graph datasets<br>- Framework easily adaptable to SNP clustering with appropriate graph construction | [PDF](papers/ICML/2024_clustering/not/cohen-addad24d.pdf) |
| EDISON: Enhanced Dictionary-Induced Tensorized Incomplete Multi-View Clustering with Gaussian Error Rank Minimization | The paper introduces EDISON, a robust and scalable clustering algorithm for incomplete multi-view data. It integrates an enhanced dictionary representation to infer missing data, a novel Gaussian Error Rank (GER) approximation to better preserve important tensor information while suppressing noise, and a hyper-anchor graph Laplacian regularization to capture high-order and local geometric structures. Extensive experiments show that EDISON outperforms state-of-the-art methods on various large, incomplete datasets while maintaining strong computational efficiency. | - Enhanced Dictionary Representation (EDR) for missing data and subspace recovery<br>- Gaussian Error Rank (GER) for noise-robust tensor decomposition<br>- Hyper-anchor graph Laplacian Regularization (HLR) for preserving local manifold structure<br>- Optimization with guaranteed convergence (ADMM-based)<br>- Superior performance over 8 strong baselines on incomplete multi-view datasets<br>- Scalable to large datasets with low computational overhead<br>- Strong ablation studies and robustness analyses | [PDF](papers/ICML/2024_clustering/not/gu24b.pdf) |
| FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering | The paper introduces FedRC, a robust clustering framework for federated learning that addresses multiple simultaneous distribution shifts (label, feature, and concept shifts) across clients. FedRC uses a soft-clustering method based on a new bi-level optimization objective that emphasizes decision boundary consistency. The method consistently outperforms prior clustered FL approaches on several datasets, demonstrating better generalization to unseen distributions while maintaining high local performance. FedRC is robust across variations in cluster numbers, data imbalance, and noisy conditions. | - Introduces a novel clustering principle separating concept shifts from feature and label shifts<br>- Develops FedRC: a soft-clustering bi-level optimization framework<br>- Objective function encourages consistent decision boundaries<br>- Demonstrated robust performance across major datasets (FashionMNIST, CIFAR10, Tiny-ImageNet)<br>- Handles imbalanced, noisy, and shifting data distributions<br>- Potential applicability to SNP clustering where pleiotropy and pathway overlap exist<br>- Soft clustering assignments could mirror SNPs' partial membership to causal groups | [PDF](papers/ICML/2024_clustering/not/guo24f.pdf) |
| Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework | The paper introduces the first framework for adversarial attacks and defenses in deep multi-view clustering (DMVC). It presents a GAN-based attack model that disrupts both complementarity and consistency among views to degrade clustering performance. For defense, the authors propose an adversarially robust DMVC model (AR-DMVC) and an enhanced version (AR-DMVC-AM) using information-theoretic regularization to mitigate adversarial effects. Extensive experiments across four benchmark datasets demonstrate that AR-DMVC-AM successfully resists attacks and maintains clustering performance, offering a pioneering step in securing DMVC models against adversarial threats. | - First adversarial attack framework specifically for DMVC<br>- GAN-based attack targeting multi-view complementarity and consistency<br>- Proposed AR-DMVC and AR-DMVC-AM models for adversarially robust clustering<br>- Information-theoretic Attack Mitigator minimizes mutual information to boost robustness<br>- Comprehensive experiments on RegDB, NoisyFashion, NoisyMNIST, and PatchedMNIST datasets<br>- Highlights vulnerability of deep clustering models to subtle perturbations<br>- Potential inspiration for designing robust SNP clustering algorithms under noisy data conditions | [PDF](papers/ICML/2024_clustering/not/huang24ai.pdf) |
| Clustered Federated Learning via Gradient-based Partitioning | The paper proposes CFL-GP, a novel algorithm for Clustered Federated Learning (CFL) that clusters clients based on the similarity of their gradient updates. By accumulating and clustering gradient vectors using spectral clustering, CFL-GP achieves fast, robust, and theoretically guaranteed convergence to optimal clusters and models, even under noise and high-dimensional conditions. The method outperforms prior CFL approaches across synthetic, image, and industrial datasets, offering strong robustness, efficiency, and scalability for federated learning in heterogeneous environments. | - Proposes CFL-GP: Clustered Federated Learning via Gradient-based Partitioning<br>- Uses moving averages of gradients as clustering features<br>- Applies spectral clustering with dimensionality reduction for scalability<br>- Provides theoretical guarantees for convergence in clustering and model optimization<br>- Achieves rapid convergence and robustness to noise and heterogeneity<br>- Empirically validated on synthetic regression, rotated MNIST, CIFAR10, and industrial tasks<br>- Potentially adaptable to SNP clustering by redefining gradient features as SNP association profiles | [PDF](papers/ICML/2024_clustering/not/kim24p.pdf) |
| Cluster-Aware Similarity Diffusion for Instance Retrieval | This paper introduces Cluster-Aware Similarity (CAS) Diffusion, a re-ranking method for instance retrieval. Instead of propagating similarities globally, CAS restricts diffusion to local clusters formed by k-reciprocal nearest neighbors, minimizing the spread of noise from outliers. Two key techniques are used: Bidirectional Similarity Diffusion (BSD) for symmetric smoothing within local clusters, and Neighbor-guided Similarity Smooth (NSS) to enforce similarity consistency among neighbors. CAS achieves state-of-the-art results on benchmark instance retrieval and re-identification datasets, demonstrating robust performance and efficient re-ranking with noise suppression. | - Introduces Cluster-Aware Similarity Diffusion restricting propagation to local clusters.<br>- Proposes Bidirectional Similarity Diffusion (BSD) for smoother and symmetric similarity matrices.<br>- Introduces Neighbor-guided Similarity Smooth (NSS) to maintain local neighbor consistency.<br>- Employs Jensen-Shannon divergence to enhance distance measures post-diffusion.<br>- Demonstrates superior retrieval performance over state-of-the-art re-ranking methods.<br>- Highlights the importance of avoiding global noise contamination in graph-based similarity propagation. | [PDF](papers/ICML/2024_clustering/not/luo24i.pdf) |
| Expand-and-Cluster: Parameter Recovery of Neural Networks | This paper presents Expand-and-Cluster, a method for recovering the weights and structure of a neural network by training multiple overparameterized student networks to mimic a target (teacher) network. After training, neuron weight vectors are clustered to identify true underlying neurons, filtering out redundant or noise elements. They demonstrate that this method successfully recovers both shallow and deep networks across various activation functions, even under realistic noisy conditions. The method relies heavily on handling network symmetries and clustering based on neuron consistency. | - Introduction of Expand-and-Cluster method for neural network parameter recovery<br>- Key role of overparameterization to ease optimization landscapes<br>- Novel clustering approach to separate true neurons from noise<br>- Handling of activation function symmetries (even, odd, scaling effects)<br>- Application to both synthetic data (XOR-type problems) and real-world datasets (MNIST, CIFAR-10)<br>- Comparison to pruning methods; Expand-and-Cluster outperforms traditional pruning for parameter recovery<br>- Potential relevance to model reverse engineering and neuroscience connectivity inference<br>- Discussion of theoretical limitations and future directions, including scaling to larger networks | [PDF](papers/ICML/2024_clustering/not/martinelli24a.pdf) |
| Robust Online Correlation Clustering | This paper addresses the problem of correlation clustering in an online setting, where data points arrive sequentially and only noisy pairwise similarity/dissimilarity labels are available. The authors develop algorithms that maintain a trusted set of points and dynamically filter noise to achieve near-optimal clustering costs despite adversarial corruption. They provide strong theoretical guarantees and validate their methods through experiments. | - Robust online clustering under adversarial noise<br>- Dynamic coreset maintenance to trust certain points more<br>- Noise filtering techniques for similarity links<br>- Theoretical guarantees for clustering accuracy under noise<br>- Useful inspiration for isolating signal vs noise in SNP clustering<br>- Focus on pairwise similarity rather than continuous feature data | [PDF](papers/NIPS/2021_clustering/not/4647_robust_online_correlation_clus.pdf) |
| Clustering Effect of (Linearized) Adversarial Robust Models | This paper studies how adversarially robust deep neural networks exhibit a hierarchical clustering effect when examined through their linearized components. By removing non-linearities and analyzing weight correlation matrices, the authors find that robust models naturally form clusters aligned with semantic class hierarchies (e.g., animals vs. non-animals in CIFAR-10). They propose a clustering regularization strategy to enhance this effect, resulting in models with better robustness and improved performance on domain adaptation tasks. The insights suggest a novel perspective on robustness via structured feature representations. | - Linearization of neural networks by removing non-linear layers to analyze weight structure.<br>- Discovery of a hierarchical clustering effect aligned with class labels in robust models.<br>- Introduction of a clustering regularization penalty to enforce semantic groupings.<br>- Demonstrated improvements in both adversarial robustness and domain adaptation tasks.<br>- Use of weight correlation matrices to measure inter-class relationships.<br>- Findings suggest that better hierarchical representation leads to more robust and transferable models. | [PDF](papers/NIPS/2021_clustering/not/482_clustering_effect_of_adversari.pdf) |
| Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem | This paper introduces a novel framework combining sparse regression with clustering based on an unknown hidden attribute (e.g., bias). The authors propose an invex relaxation ‚Äî the first of its kind for a combinatorial problem ‚Äî making the optimization globally tractable. Their method jointly recovers the regression coefficients and the hidden group memberships with theoretical guarantees. Experiments on synthetic and real-world data validate their theoretical claims, showing accurate bias detection and support recovery with a sample complexity scaling logarithmically with the number of features. | - First invex relaxation for a combinatorial clustering and regression problem<br>- Simultaneous recovery of sparse regression vector and hidden binary clustering<br>- Logarithmic sample complexity with feature size<br>- Fairness constraints incorporated without access to sensitive attributes<br>- Strong theoretical guarantees and primal-dual construction proofs<br>- Empirical validation on synthetic and real-world datasets<br>- Framework could inspire SNP clustering models that handle hidden biological subgrouping | [PDF](papers/NIPS/2021_clustering/not/7805_fair_sparse_regression_with_cl.pdf) |
| Parallel and Efficient Hierarchical k-Median Clustering | This paper introduces the first distributed algorithm for hierarchical Euclidean k-median clustering with provable theoretical guarantees. It uses a quadtree-based 2-restricted Hierarchically Separated Tree embedding of the data and a greedy algorithm to build a clustering hierarchy efficiently in parallel under the Massive Parallel Computation (MPC) model. The method achieves an expected approximation factor within O(min{d, log n} log Œî) of optimal and works for all k values simultaneously. It shows strong empirical improvements in runtime and competitiveness in solution quality compared to prior distributed methods. | - First distributed algorithm for hierarchical Euclidean k-median clustering with guarantees<br>- Uses randomized quadtree embedding into 2-RHST for tree-based approximation<br>- Greedy, quasi-linear time algorithm for clustering on the tree structure<br>- Handles all k-values simultaneously (nested clusters)<br>- Near-linear total work and logarithmic parallel rounds<br>- Empirical validation shows substantial speed gains over previous methods<br>- Highly scalable to datasets with billions of points<br>- Adaptations needed for SNP noise detection and non-Euclidean association structures | [PDF](papers/NIPS/2021_clustering/not/8652_parallel_and_efficient_hierarc.pdf) |
| Coresets for Time Series Clustering | This paper develops an algorithm (CRGMM) to construct small coresets for clustering large time series datasets generated from Gaussian Mixture Models with autocorrelations. The algorithm reduces the clustering problem to a k-means formulation using average observations and applies two-stage importance sampling to select representative entity-time pairs. The coresets achieve theoretical approximation guarantees and demonstrate large computational and storage savings (14x-171x speedup) without significant loss in clustering quality compared to full datasets. | - First coreset method for clustering time series with Gaussian mixtures and autocorrelations<br>- Reduces full clustering problem to a k-means problem on averaged entity observations<br>- Two-stage importance sampling to select small, representative subsets<br>- Provides theoretical approximation guarantees on clustering quality<br>- Coreset size independent of number of entities or time points, only depends on k, d, and 1/Œµ<br>- Empirically validated: 14x-171x faster than full data clustering<br>- Highly promising for scaling up SNP clustering tasks with necessary adaptations | [PDF](papers/NIPS/2021_clustering/not/9329_coresets_for_time_series_clust.pdf) |
| Better Algorithms for Individually Fair k-Clustering | This paper introduces LP-based algorithms for individually fair k-clustering, optimizing standard clustering objectives (like k-means and k-median) while ensuring that each point has a nearby center within a fairness radius. The Fair-Round algorithm uses LP rounding and a filtering technique to balance cost and fairness, achieving bi-criteria approximation guarantees (8,8 for k-median, 8,4 for k-means). A sparsification technique is also proposed to drastically improve runtime on large datasets. Empirical results show fairness violations are minimal and clustering costs stay close to the LP lower bound. | - Formulates fair k-clustering as a linear program and rounds solutions efficiently<br>- Develops Fair-Round algorithm achieving strong fairness and cost guarantees<br>- Sparsification method greatly improves LP solving speed for large datasets<br>- Empirical results show minimal fairness violations and near-optimal clustering cost<br>- Fine-grained fairness analysis with histograms of individual fairness violations<br>- Framework is adaptable to SNP clustering where local fairness (signal strength consistency) is critical<br>- Scalable to very large datasets via LP sparsification and filtering techniques | [PDF](papers/NIPS/2021_clustering/not/9679_better_algorithms_for_individu.pdf) |
| A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space | This paper proposes a cluster-based method to improve the isotropy (uniformity) of contextual embeddings from models like BERT and GPT-2. It first clusters embeddings using K-means, then applies PCA within each cluster to remove dominant directions. The method improves the spread of embeddings and sometimes leads to better performance on semantic tasks. Although focused on NLP, the methodology could inspire clustering strategies for SNP data. | - Introduces a cluster-based method using K-means and PCA to improve isotropy.<br>- Shows local isotropy enhancement often outperforms global methods for embeddings.<br>- Deals explicitly with noise and structured redundancy in data representations.<br>- Reproducibility challenges indicate care is needed with hyperparameters and clustering size.<br>- Open-source code available at https://github.com/Sara-Rajaee/clusterbased_isotropy_enhancement.<br>- Findings could inspire strategies for clustering numeric feature vectors like SNP effects. | [PDF](papers/NIPS/2022_clustering/not/A Cluster-based Approach for Improving Isotropy in.pdf) |